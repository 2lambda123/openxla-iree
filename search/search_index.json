{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"IREE","text":"<p>IREE (Intermediate Representation Execution Environment<sup>1</sup>) is an MLIR-based end-to-end compiler and runtime that lowers Machine Learning (ML) models to a unified IR that scales up to meet the needs of the datacenter and down to satisfy the constraints and special considerations of mobile and edge deployments.</p>"},{"location":"#key-features","title":"Key features","text":"<ul> <li> Ahead-of-time compilation of scheduling and execution logic together</li> <li> Support for dynamic shapes, flow control, streaming, and other advanced       model features</li> <li> Optimized for many CPU and GPU architectures</li> <li> Low overhead, pipelined execution for efficient power and resource usage</li> <li> Binary size as low as 30KB on embedded systems</li> <li> Debugging and profiling support</li> </ul>"},{"location":"#support-matrix","title":"Support matrix","text":"<p>IREE supports importing from a variety of ML frameworks:</p> <ul> <li> TensorFlow</li> <li> TensorFlow Lite</li> <li> JAX</li> <li> PyTorch</li> <li> ONNX (hoped for)</li> </ul> <p>The IREE compiler tools run on  Linux,  Windows, and  macOS and can generate efficient code for a variety of runtime platforms:</p> <ul> <li> Linux</li> <li> Windows</li> <li> Android</li> <li> macOS</li> <li> iOS</li> <li> Bare metal</li> <li> WebAssembly (planned)</li> </ul> <p>and architectures:</p> <ul> <li> ARM</li> <li> x86</li> <li> RISC-V</li> </ul> <p>Support for hardware accelerators and APIs is also included:</p> <ul> <li> Vulkan</li> <li> CUDA</li> <li> Metal (for Apple silicon devices)</li> <li> WebGPU (planned)</li> </ul>"},{"location":"#project-architecture","title":"Project architecture","text":"<p>IREE adopts a holistic approach towards ML model compilation: the IR produced contains both the scheduling logic, required to communicate data dependencies to low-level parallel pipelined hardware/API like Vulkan, and the execution logic, encoding dense computation on the hardware in the form of hardware/API-specific binaries like SPIR-V.</p> <p> </p>"},{"location":"#workflow-overview","title":"Workflow overview","text":"<p>Using IREE involves the following general steps:</p> <ol> <li> <p>Import your model</p> <p>Develop your program using one of the supported frameworks, then import into IREE</p> </li> <li> <p>Select your deployment configuration</p> <p>Identify your target platform, accelerator(s), and other constraints</p> </li> <li> <p>Compile your model</p> <p>Compile through IREE, picking settings based on your deployment configuration</p> </li> <li> <p>Run your model</p> <p>Use IREE's runtime components to execute your compiled model</p> </li> </ol>"},{"location":"#importing-models-from-ml-frameworks","title":"Importing models from ML frameworks","text":"<p>IREE supports importing models from a growing list of ML frameworks and model formats:</p> <ul> <li>TensorFlow and   TensorFlow Lite</li> <li>JAX</li> <li>PyTorch</li> </ul>"},{"location":"#selecting-deployment-configurations","title":"Selecting deployment configurations","text":"<p>IREE provides a flexible set of tools for various deployment scenarios. Fully featured environments can use IREE for dynamic model deployments taking advantage of multi-threaded hardware, while embedded systems can bypass IREE's runtime entirely or interface with custom accelerators.</p> <ul> <li>What platforms are you targeting? Desktop? Mobile? An embedded system?</li> <li>What hardware should the bulk of your model run on? CPU? GPU?</li> <li>How fixed is your model itself? Can the weights be changed? Do you want   to support loading different model architectures dynamically?</li> </ul> <p>IREE supports the full set of these configurations using the same underlying technology.</p>"},{"location":"#compiling-models","title":"Compiling models","text":"<p>Model compilation is performed ahead-of-time on a host machine for any combination of targets. The compilation process converts from layers and operators used by high level frameworks down into optimized native code and associated scheduling logic.</p> <p>For example, compiling for GPU execution using Vulkan generates SPIR-V kernels and Vulkan API calls. For CPU execution, native code with static or dynamic linkage and the associated function calls are generated.</p>"},{"location":"#running-models","title":"Running models","text":"<p>IREE offers a low level C API, as well as several sets of API bindings for compiling and running programs using various languages.</p>"},{"location":"#communication-channels","title":"Communication channels","text":"<ul> <li> GitHub issues: Feature requests,   bugs, and other work tracking</li> <li> IREE Discord server: Daily development   discussions with the core team and collaborators</li> <li> iree-discuss email list:   Announcements, general and low-priority discussion</li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":"<p>IREE is in the early stages of development and is not yet ready for broad adoption. We use both GitHub Projects and GitHub Milestones to track progress.</p> <ol> <li> <p>Pronounced \"eerie\" and often styled with the   emoji\u00a0\u21a9</p> </li> </ol>"},{"location":"building-from-source/","title":"Building from source","text":"<p>While IREE does offer binary distributions for its compiler tools and Python bindings, building from source is still useful when using IREE's runtime or when making changes to the compiler or import tools themselves.</p>"},{"location":"building-from-source/#reference-pages","title":"Reference pages","text":"<ul> <li>Getting started</li> <li>Android cross-compilation</li> <li>iOS cross-compilation</li> <li>RISC-V cross-compilation</li> </ul>"},{"location":"building-from-source/android/","title":"Android cross-compilation","text":"<p>Running on a platform like Android involves cross-compiling from a host platform (e.g. Linux) to a target platform (a specific Android version and system architecture):</p> <ul> <li>IREE's compiler is built on the host and is used there to generate modules   for the target</li> <li>IREE's runtime is built on the host for the target. The runtime is then   either pushed to the target to run natively or is bundled into an Android   APK</li> </ul>","tags":["Android"]},{"location":"building-from-source/android/#prerequisites","title":"Prerequisites","text":"","tags":["Android"]},{"location":"building-from-source/android/#host-environment-setup","title":"Host environment setup","text":"<p>You should already be able to build IREE from source on your host platform. Please make sure you have followed the getting started steps.</p>","tags":["Android"]},{"location":"building-from-source/android/#install-android-ndk-and-adb","title":"Install Android NDK and ADB","text":"<p>The Android Native Developer Kit (NDK) is needed to use native C/C++ code on Android. You can download it here, or, if you have installed Android Studio, you can follow this guide instead.</p> <p>Note</p> <p>Make sure the <code>ANDROID_NDK</code> environment variable is set after installing the NDK.</p> <p>ADB (the Android Debug Bridge) is also needed to communicate with Android devices from the command line. Install it following the official user guide.</p>","tags":["Android"]},{"location":"building-from-source/android/#configure-and-build","title":"Configure and build","text":"","tags":["Android"]},{"location":"building-from-source/android/#host-configuration","title":"Host configuration","text":"<p>Build and install on your host machine:</p> <pre><code>cmake -GNinja -B ../iree-build/ \\\n-DCMAKE_INSTALL_PREFIX=../iree-build/install \\\n-DCMAKE_BUILD_TYPE=RelWithDebInfo \\\n.\ncmake --build ../iree-build/ --target install\n</code></pre>","tags":["Android"]},{"location":"building-from-source/android/#target-configuration","title":"Target configuration","text":"<p>Build the runtime using the Android NDK toolchain:</p> LinuxmacOSWindows <pre><code>cmake -GNinja -B ../iree-build-android/ \\\n-DCMAKE_TOOLCHAIN_FILE=\"${ANDROID_NDK?}/build/cmake/android.toolchain.cmake\" \\\n-DIREE_HOST_BIN_DIR=\"$PWD/../iree-build/install/bin\" \\\n-DANDROID_ABI=\"arm64-v8a\" \\\n-DANDROID_PLATFORM=\"android-29\" \\\n-DIREE_BUILD_COMPILER=OFF \\\n.\ncmake --build ../iree-build-android/\n</code></pre> <pre><code>cmake -GNinja -B ../iree-build-android/ \\\n-DCMAKE_TOOLCHAIN_FILE=\"${ANDROID_NDK?}/build/cmake/android.toolchain.cmake\" \\\n-DIREE_HOST_BIN_DIR=\"$PWD/../iree-build/install/bin\" \\\n-DANDROID_ABI=\"arm64-v8a\" \\\n-DANDROID_PLATFORM=\"android-29\" \\\n-DIREE_BUILD_COMPILER=OFF \\\n.\ncmake --build ../iree-build-android/\n</code></pre> <pre><code>cmake -GNinja -B ../iree-build-android/ \\\n-DCMAKE_TOOLCHAIN_FILE=\"%ANDROID_NDK%/build/cmake/android.toolchain.cmake\" \\\n-DIREE_HOST_BIN_DIR=\"%CD%/../iree-build/install/bin\" \\\n-DANDROID_ABI=\"arm64-v8a\" \\\n-DANDROID_PLATFORM=\"android-29\" \\\n-DIREE_BUILD_COMPILER=OFF \\\n.\ncmake --build ../iree-build-android/\n</code></pre> <p>Note</p> <p>See the Android NDK CMake guide and Android Studio CMake guide for details on configuring CMake for Android.</p> <p>The specific <code>ANDROID_ABI</code> and <code>ANDROID_PLATFORM</code> used should match your target device.</p>","tags":["Android"]},{"location":"building-from-source/android/#running-android-tests","title":"Running Android tests","text":"<p>Make sure you enable developer options and USB debugging on your Android device and can see your it when you run <code>adb devices</code>, then run all tests through ctest:</p> <pre><code># Build test dependencies\ncmake --build ../iree-build-android/ --target iree-test-deps\n\n# Ensure that your Android device is visible\nadb devices\n\n# Run tests\nctest --test-dir ../iree-build-android/ --output-on-failure\n</code></pre> <p>This will automatically upload build artifacts to the connected Android device, run the tests, then report the status back to your host machine.</p>","tags":["Android"]},{"location":"building-from-source/android/#running-tools-directly","title":"Running tools directly","text":"<p>Invoke the host compiler tools to produce a bytecode module FlatBuffer:</p> <pre><code>../iree-build/install/bin/iree-compile \\\n--iree-hal-target-backends=vmvx \\\nsamples/models/simple_abs.mlir \\\n-o /tmp/simple_abs_vmvx.vmfb\n</code></pre> <p>Push the Android runtime tools to the device, along with any FlatBuffer files:</p> <pre><code>adb push ../iree-build-android/tools/iree-run-module /data/local/tmp/\nadb shell chmod +x /data/local/tmp/iree-run-module\nadb push /tmp/simple_abs_vmvx.vmfb /data/local/tmp/\n</code></pre> <p>Run the tool:</p> <pre><code>adb shell /data/local/tmp/iree-run-module --device=local-task \\\n--module=/data/local/tmp/simple_abs_vmvx.vmfb \\\n--function=abs \\\n--input=\"f32=-5\"\n</code></pre>","tags":["Android"]},{"location":"building-from-source/getting-started/","title":"Getting started","text":""},{"location":"building-from-source/getting-started/#prerequisites","title":"Prerequisites","text":"<p>IREE can be built from source using CMake. We also recommend the Ninja CMake generator and the clang or MSVC C/C++ compilers.</p> Note - Other CMake generators and compilers <p>IREE developers and CIs primarily use Ninja, clang, and MSVC. Other configurations (including the Makefile generator and gcc) are \"best effort\". Patches to improve support are always welcome.</p> LinuxmacOSWindows <ol> <li> <p>Install a compiler/linker (typically \"clang\" and \"lld\" package)</p> </li> <li> <p>Install CMake (typically \"cmake\" package)</p> </li> <li> <p>Install Ninja (typically \"ninja-build\"    package)</p> </li> </ol> <p>On Debian/Ubuntu:</p> <pre><code>sudo apt install cmake ninja-build clang lld\n</code></pre> <ol> <li> <p>Install CMake</p> </li> <li> <p>Install Ninja</p> </li> </ol> <p>If using Homebrew:</p> <pre><code>brew install cmake ninja\n</code></pre> <ol> <li> <p>Install MSVC from Visual Studio or \"Tools for Visual Studio\" on the    official downloads page</p> </li> <li> <p>Install CMake from the    official downloads page</p> </li> <li> <p>Install Ninja from the official site</p> </li> </ol> <p>Note</p> <p>Initialize MSVC by running <code>vcvarsall.bat</code> to build on the command line. See the official documentation for details.</p>"},{"location":"building-from-source/getting-started/#quickstart-clone-and-build","title":"Quickstart: clone and build","text":"<p>Use Git to clone the IREE repository and initialize its submodules:</p> <pre><code>git clone https://github.com/openxla/iree.git\ncd iree\ngit submodule update --init\n</code></pre> <p>The most basic CMake workflow is:</p> <pre><code># Configure\ncmake -G Ninja -B ../iree-build/ .\n\n# Build\ncmake --build ../iree-build/\n</code></pre> <p>Caution - slow builds</p> <p>The compiler build is complex. You will want a powerful machine and to tune the settings following the next section. In 2023, we've seen builds take around 5-10 minutes on 64-core Linux machines.</p> <p>Use case permitting, disabling the compiler build with <code>-DIREE_BUILD_COMPILER=OFF</code> will drastically simplify the build.</p>"},{"location":"building-from-source/getting-started/#configuration-settings","title":"Configuration settings","text":"<p>The configure step should be customized for your build environment. These settings can improve compile and link times substantially.</p> LinuxmacOSWindows <pre><code># Recommended development options using clang and lld:\ncmake -G Ninja -B ../iree-build/ -S . \\\n-DCMAKE_BUILD_TYPE=RelWithDebInfo \\\n-DIREE_ENABLE_ASSERTIONS=ON \\\n-DIREE_ENABLE_SPLIT_DWARF=ON \\\n-DIREE_ENABLE_THIN_ARCHIVES=ON \\\n-DCMAKE_C_COMPILER=clang \\\n-DCMAKE_CXX_COMPILER=clang++ \\\n-DIREE_ENABLE_LLD=ON\n</code></pre> <pre><code># Recommended development options using clang and lld:\ncmake -G Ninja -B ../iree-build/ -S . \\\n-DCMAKE_BUILD_TYPE=RelWithDebInfo \\\n-DIREE_ENABLE_ASSERTIONS=ON \\\n-DIREE_ENABLE_SPLIT_DWARF=ON \\\n-DCMAKE_C_COMPILER=clang \\\n-DCMAKE_CXX_COMPILER=clang++ \\\n-DIREE_ENABLE_LLD=ON\n</code></pre> <p>It is also possible to add <code>-DIREE_ENABLE_THIN_ARCHIVES=ON</code> if the <code>CMAKE_AR</code> variable is defined and points to the path of either the GNU binutils or LLVM <code>ar</code> program, overriding the default Apple <code>ar</code>.</p> <pre><code># Recommended development options:\ncmake -G Ninja -B ../iree-build/ -S . \\\n-DCMAKE_BUILD_TYPE=RelWithDebInfo \\\n-DIREE_ENABLE_ASSERTIONS=ON\n</code></pre> Tip - CMAKE_BUILD_TYPE values <p>We recommend using the <code>RelWithDebInfo</code> build type by default for a good balance of debug info and performance. The <code>Debug</code>, <code>Release</code>, and <code>MinSizeRel</code> build types are useful in more specific cases. Note that several useful LLVM debugging features are only available in <code>Debug</code> builds. See the official CMake documentation for general details.</p> Tip - Faster recompilation with ccache <p>We recommend using <code>ccache</code> with CMake, especially when rebuilding the compiler. To use it, configure CMake with:</p> <pre><code>-DCMAKE_C_COMPILER_LAUNCHER=ccache -DCMAKE_CXX_COMPILER_LAUNCHER=ccache\n</code></pre> <p>See also our developer documentation for ccache.</p>"},{"location":"building-from-source/getting-started/#optional-components","title":"Optional components","text":"<p>By default, the CMake build includes:</p> <ul> <li>All compiler targets (<code>llvm-cpu</code>, <code>cuda</code>, <code>vulkan-spirv</code>, etc.)</li> <li>All runtime HAL drivers (<code>local-task</code>, <code>cuda</code>, <code>vulkan</code>, etc.)</li> <li>All compiler input formats (StableHLO, TOSA, etc.)</li> <li>All compiler output formats (VM bytecode, C)</li> </ul> <p>The default build does not include:</p> <ul> <li>Compiler or runtime bindings (Python, TFLite, etc.)</li> <li>Advanced features like AddressSanitizer or tracing instrumentation</li> <li>Experimental components</li> </ul> <p>These can be changed via the <code>IREE_</code> CMake options listed in the root <code>CMakeLists.txt</code>.</p>"},{"location":"building-from-source/getting-started/#extensions-and-integrations","title":"Extensions and integrations","text":"<p>When using IREE within other projects, you can register compiler plugins and runtime HAL drivers. You can also bring your own copy of LLVM and some other tools. See the root <code>CMakeLists.txt</code> for details.</p>"},{"location":"building-from-source/getting-started/#tests-and-samples","title":"Tests and samples","text":""},{"location":"building-from-source/getting-started/#running-tests","title":"Running tests","text":"<p>Tests are run via ctest. To build and run the core project tests:</p> <pre><code># Build default targets\ncmake --build ../iree-build/\n\n# Run tests\nctest --test-dir ../iree-build/\n</code></pre> <p>Caution</p> <p>This has two limitations:</p> <ol> <li>Large tests are excluded from the build by default</li> <li>Some tests require hardware like a GPU and will fail on unsupported systems</li> </ol> <p>To build and then run all tests:</p> <pre><code># 1. Build default targets\ncmake --build ../iree-build/\n\n# 2. Build test dependencies\ncmake --build ../iree-build/ --target iree-test-deps\n\n# 3. Run tests\nctest --test-dir ../iree-build/\n\n\n# Or combine all steps using a utility target\ncmake --build ../iree-build --target iree-run-tests\n</code></pre> <p>To run only certain tests, we have a helper script that converts environment variables into ctest filters:</p> <pre><code># Run default tests\n./build_tools/cmake/ctest_all.sh ../iree-build\n\n# Run tests, turning CUDA on and Vulkan off\nexport IREE_CUDA_DISABLE=0\nexport IREE_VULKAN_DISABLE=1\n./build_tools/cmake/ctest_all.sh ../iree-build\n</code></pre>"},{"location":"building-from-source/getting-started/#running-samples","title":"Running samples","text":"<pre><code># Build\ncmake --build ../iree-build/\n\n# Run a standalone sample application\n../iree-build/runtime/src/iree/runtime/demo/hello_world_embedded\n# 4xf32=1 1.1 1.2 1.3\n#  *\n# 4xf32=10 100 1000 10000\n#  =\n# 4xf32=10 110 1200 13000\n\n# Try out the developer tools\nls ../iree-build/tools/\n../iree-build/tools/iree-compile --help\n../iree-build/tools/iree-run-module --help\n</code></pre>"},{"location":"building-from-source/getting-started/#python-bindings","title":"Python bindings","text":"<p>Python packages can either be built from source or installed from our releases. See the Python bindings page for details about the bindings themselves.</p>"},{"location":"building-from-source/getting-started/#dependencies","title":"Dependencies","text":"<p>You will need a recent Python installation &gt;=3.9 (we aim to support non-eol Python versions).</p> Tip - Managing Python versions <p>Make sure your 'python' is what you expect:</p> LinuxmacOSWindows <p>Note that on multi-python systems, this may have a version suffix, and on many Linuxes where python2 and python3 can co-exist, you may also want to use <code>python3</code>.</p> <pre><code>which python\npython --version\n</code></pre> <p>Note that on multi-python systems, this may have a version suffix, and on macOS where python2 and python3 can co-exist, you may also want to use <code>python3</code>.</p> <pre><code>which python\npython --version\n</code></pre> <p>The Python launcher for Windows (<code>py</code>) can help manage versions.</p> <pre><code>which python\npython --version\npy --list-paths\n</code></pre> Tip - Virtual environments <p>We recommend using virtual environments to manage python packages, such as through <code>venv</code> (about, tutorial):</p> LinuxmacOSWindows <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre> <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre> <pre><code>python -m venv .venv\n.venv\\Scripts\\activate.bat\n</code></pre> <p>When done, run <code>deactivate</code>.</p> <pre><code># Upgrade PIP before installing other requirements\npython -m pip install --upgrade pip\n\n# Install IREE build requirements\npython -m pip install -r runtime/bindings/python/iree/runtime/build_requirements.txt\n</code></pre>"},{"location":"building-from-source/getting-started/#building-with-cmake","title":"Building with CMake","text":"<p>To build the Python bindings, configure CMake with the <code>IREE_BUILD_PYTHON_BINDINGS</code> option. We also recommend explicitly setting which Python executable to use with <code>Python3_EXECUTABLE</code>:</p> <pre><code># Configure (including other options as discussed above)\ncmake -G Ninja -B ../iree-build/ \\\n-DIREE_BUILD_PYTHON_BINDINGS=ON  \\\n-DPython3_EXECUTABLE=\"$(which python)\" \\\n.\n\n# Build\ncmake --build ../iree-build/\n</code></pre>"},{"location":"building-from-source/getting-started/#using-the-python-bindings","title":"Using the Python bindings","text":"<p>Extend your <code>PYTHONPATH</code> with IREE's <code>bindings/python</code> paths and try importing:</p> LinuxmacOSWindows <pre><code>source ../iree-build/.env &amp;&amp; export PYTHONPATH\npython -c \"import iree.compiler\"\npython -c \"import iree.runtime\"\n</code></pre> <pre><code>source ../iree-build/.env &amp;&amp; export PYTHONPATH\npython -c \"import iree.compiler\"\npython -c \"import iree.runtime\"\n</code></pre> <pre><code>../iree-build/.env.bat\npython -c \"import iree.compiler\"\npython -c \"import iree.runtime\"\n</code></pre> <p>Using IREE's ML framework importers requires a few extra steps:</p> <pre><code># Install test requirements\npython -m pip install -r integrations/tensorflow/test/requirements.txt\n\n# Install pure Python packages (no build required)\npython -m pip install integrations/tensorflow/python_projects/iree_tf\npython -m pip install integrations/tensorflow/python_projects/iree_tflite\n\n# Then test the tools:\niree-import-tf --help\niree-import-tflite --help\n</code></pre>"},{"location":"building-from-source/ios/","title":"iOS cross-compilation","text":"<p>Cross-compilation for iOS consists of the two steps below.</p> <ul> <li>On the macOS host, build the IREE compiler.  We can run it to create   IREE modules.</li> <li>Build the IREE runtime on the macOS host for iOS devices and the   simulator.  We can then run the IREE module on the simulator.</li> </ul>","tags":["iOS"]},{"location":"building-from-source/ios/#prerequisites","title":"Prerequisites","text":"","tags":["iOS"]},{"location":"building-from-source/ios/#install-xcode-and-ios-sdk","title":"Install Xcode and iOS SDK","text":"<p>For cross-compilation, you need Xcode. It comes with the SDKs for iOS devices and the simulator, as well as the <code>simctl</code> tool for controlling the simulator from the command line.</p>","tags":["iOS"]},{"location":"building-from-source/ios/#host-environment-setup","title":"Host environment setup","text":"<p>On your host platform, you should already be able to build IREE from source.  Please make sure you've gone through the steps in getting started.</p>","tags":["iOS"]},{"location":"building-from-source/ios/#configure-and-build","title":"Configure and Build","text":"","tags":["iOS"]},{"location":"building-from-source/ios/#build-the-iree-compiler-for-the-host","title":"Build the IREE Compiler for the Host","text":"<p>Build and install on your macOS host:</p> <pre><code>cmake -S . -B ../iree-build/ -GNinja \\\n-DCMAKE_BUILD_TYPE=RelWithDebInfo \\\n-DCMAKE_INSTALL_PREFIX=../iree-build/install\n\ncmake --build ../iree-build/ --target install\n</code></pre>","tags":["iOS"]},{"location":"building-from-source/ios/#cross-compile-the-iree-runtime-for-ios","title":"Cross-compile the IREE Runtime for iOS","text":"<p>Build the runtime for the iOS Simulator.</p> <pre><code>cmake -S . -B ../build-ios-sim -GNinja \\\n-DCMAKE_SYSTEM_NAME=iOS \\\n-DCMAKE_OSX_SYSROOT=$(xcodebuild -version -sdk iphonesimulator Path) \\\n-DCMAKE_OSX_ARCHITECTURES=arm64 \\\n-DCMAKE_SYSTEM_PROCESSOR=arm64 \\\n-DCMAKE_OSX_DEPLOYMENT_TARGET=11.0 \\\n-DCMAKE_IOS_INSTALL_COMBINED=YES \\\n-DIREE_HOST_BIN_DIR=\"$PWD/../iree-build/install/bin\" \\\n-DCMAKE_INSTALL_PREFIX=../build-ios-sim/install \\\n-DIREE_BUILD_COMPILER=OFF\n\ncmake --build ../build-ios-sim --config Release --target install\n</code></pre> <p>Or, we can build the runtime for iOS devices it by changing the value of the <code>-DCMAKE OSX SYSROOT</code> option to:</p> <pre><code>  -DCMAKE_OSX_SYSROOT=$(xcodebuild -version -sdk iphoneos Path)\n</code></pre>","tags":["iOS"]},{"location":"building-from-source/ios/#running-iree-modules-on-the-ios-simulator","title":"Running IREE Modules on the iOS Simulator","text":"<p>Run the IREE compiler on the host to generate a module.</p> <pre><code>../iree-build/install/bin/iree-compile \\\n--iree-hal-target-backends=vmvx \\\nsamples/models/simple_abs.mlir \\\n-o /tmp/simple_abs_vmvx.vmfb\n</code></pre> <p>We could test the generated module by running the macOS version of <code>iree-run-module</code> on the host.</p> <pre><code>../iree-build/install/bin/iree-run-module \\\n--module=/tmp/simple_abs_vmvx.vmfb \\\n--device=local-task \\\n--function=abs \\\n--input=\"f32=-5\"\n</code></pre> <p>To run it on the iOS simulator, we need to copy the vmfb file into the <code>iree-run-module</code> iOS app bundle.</p> <pre><code>cp /tmp/simple_abs_vmvx.vmfb \\\n../build-ios-sim/install/bin/iree-run-module.app/\n</code></pre> <p>Open the iOS Simulator Manager on the host.</p> <pre><code>open -a Simulator\n</code></pre> <p>After creating and booting a simulator in this app, you can list it from the command-line.</p> <pre><code>xcrun simctl list devices | grep Booted\n</code></pre> <p>This is what should come out of the command:</p> <pre><code>    iPhone 14 Pro (12341234-ABCD-ABCD-ABCD-123412341234) (Booted)\n</code></pre> <p>where <code>iPhone 14 Pro</code> is the device being simulated and <code>12341234-ABCD-ABCD-ABCD-123412341234</code> is the simulator's unique device ID (UDID).</p> <p>Install the app <code>iree-run-module</code> on the simulator, given its UDID.</p> <pre><code>xcrun simctl install &lt;UDID&gt; ../build-ios-sim/install/bin/iree-run-module.app\n</code></pre> <p>Check the path to the installed bundle, where the <code>simple_abs_vmvx.vmfb</code> module should be found.</p> <pre><code>ls $(xcrun simctl get_app_container &lt;UDID&gt; dev.iree.iree-run-module)\n</code></pre> <p>The string <code>dev.iree.iree-run-module</code> is the bundle identifier of the iOS app.  The CMake building process generates it and saves it in the property list (plist) file <code>../build-ios-sim/install/bin/iree-run-module.app/Info.plist</code>.</p> <p>Launch the <code>iree-run-module</code> app on the simulator to run the IREE module <code>simple_abs_vmvx.vmfb</code>.</p> <pre><code>xcrun simctl launch --console \\\n&lt;UDID&gt; \\\ndev.iree.runmodule \\\n--device=local-task \\\n--function=abs \\\n--input=\"f32=-5\" \\\n--module=$(xcrun simctl get_app_container &lt;UDID&gt; dev.iree.iree-run-module)/simple_abs_vmvx.vmfb\n</code></pre>","tags":["iOS"]},{"location":"building-from-source/riscv/","title":"RISC-V cross-compilation","text":"<p>Running on a platform like RISC-V involves cross-compiling from a host platform (e.g. Linux) to a target platform (a specific RISC-V CPU architecture and operating system):</p> <ul> <li>IREE's compiler is built on the host and is used there to generate modules   for the target</li> <li>IREE's runtime is built on the host for the target. The runtime is then   pushed to the target to run natively.</li> </ul>","tags":["CPU"]},{"location":"building-from-source/riscv/#prerequisites","title":"Prerequisites","text":"","tags":["CPU"]},{"location":"building-from-source/riscv/#host-environment-setup","title":"Host environment setup","text":"<p>You should already be able to build IREE from source on your host platform. Please make sure you have followed the getting started steps.</p>","tags":["CPU"]},{"location":"building-from-source/riscv/#install-risc-v-cross-compile-toolchain-and-emulator","title":"Install RISC-V cross-compile toolchain and emulator","text":"<p>You'll need a RISC-V LLVM compilation toolchain and a RISC-V enabled QEMU emulator.</p> <p>See instructions in the following links</p> <ul> <li>Clang getting started</li> <li>RISC-V GNU toolchain</li> <li>QEMU</li> <li>RISC-V Linux QEMU</li> </ul> <p>Note</p> <p>The <code>RISCV_TOOLCHAIN_ROOT</code> environment variable needs to be set to the root directory of the installed GNU toolchain when building the RISC-V compiler target and the runtime library.</p>","tags":["CPU"]},{"location":"building-from-source/riscv/#install-prebuilt-risc-v-tools-risc-v-64-bit-linux-toolchain","title":"Install prebuilt RISC-V tools (RISC-V 64-bit Linux toolchain)","text":"<p>Execute the following script to download the prebuilt RISC-V toolchain and QEMU from the IREE root directory:</p> <pre><code>./build_tools/riscv/riscv_bootstrap.sh\n</code></pre> <p>Note</p> <p>The prebuilt toolchain is built with Ubuntu 20.04 LTS. It requires glibc &gt;=2.31 for your host machine.</p>","tags":["CPU"]},{"location":"building-from-source/riscv/#support-vector-extension","title":"Support vector extension","text":"<p>For RISC-V vector extensions support, see additional instructions</p>","tags":["CPU"]},{"location":"building-from-source/riscv/#configure-and-build","title":"Configure and build","text":"","tags":["CPU"]},{"location":"building-from-source/riscv/#host-configuration","title":"Host configuration","text":"<p>Build and install on your host machine:</p> <pre><code>cmake -GNinja -B ../iree-build/ \\\n-DCMAKE_C_COMPILER=clang \\\n-DCMAKE_CXX_COMPILER=clang++ \\\n-DCMAKE_INSTALL_PREFIX=../iree-build/install \\\n-DCMAKE_BUILD_TYPE=RelWithDebInfo \\\n.\ncmake --build ../iree-build/ --target install\n</code></pre>","tags":["CPU"]},{"location":"building-from-source/riscv/#target-configuration","title":"Target configuration","text":"<p>The following instruction shows how to build for a RISC-V 64-bit Linux machine. For other RISC-V targets, please refer to riscv.toolchain.cmake as a reference of how to set up the cmake configuration.</p>","tags":["CPU"]},{"location":"building-from-source/riscv/#risc-v-64-bit-linux-target","title":"RISC-V 64-bit Linux target","text":"<pre><code>cmake -GNinja -B ../iree-build-riscv/ \\\n-DCMAKE_TOOLCHAIN_FILE=\"./build_tools/cmake/riscv.toolchain.cmake\" \\\n-DIREE_HOST_BIN_DIR=$(realpath ../iree-build/install/bin) \\\n-DRISCV_CPU=linux-riscv_64 \\\n-DIREE_BUILD_COMPILER=OFF \\\n-DRISCV_TOOLCHAIN_ROOT=${RISCV_TOOLCHAIN_ROOT} \\\n-DIREE_ENABLE_CPUINFO=OFF \\\n.\ncmake --build ../iree-build-riscv/\n</code></pre>","tags":["CPU"]},{"location":"building-from-source/riscv/#running-iree-bytecode-modules-on-the-risc-v-system","title":"Running IREE bytecode modules on the RISC-V system","text":"<p>Note</p> <p>The following instructions are meant for the RISC-V 64-bit Linux target. For the bare-metal target, please refer to simple_embedding to see how to build a ML workload for a bare-metal machine.</p> <p>Set the path to qemu-riscv64 Linux emulator binary in the <code>QEMU_BIN</code> environment variable. If it is installed with <code>riscv_bootstrap.sh</code>, the path is default at ${HOME}/riscv/qemu/linux/RISCV/bin/qemu-riscv64.</p> <pre><code>export QEMU_BIN=&lt;path to qemu-riscv64 binary&gt;\n</code></pre> <p>Invoke the host compiler tools to produce a bytecode module FlatBuffer:</p> <pre><code>../iree-build/install/bin/iree-compile \\\n--iree-hal-target-backends=vmvx \\\nsamples/models/simple_abs.mlir \\\n-o /tmp/simple_abs_vmvx.vmfb\n</code></pre> <p>Run the RISC-V emulation:</p> <pre><code>${QEMU_BIN} \\\n-cpu rv64 \\\n-L ${RISCV_TOOLCHAIN_ROOT}/sysroot/ \\\n../iree-build-riscv/tools/iree-run-module \\\n--device=local-task \\\n--module=/tmp/simple_abs_vmvx.vmfb \\\n--function=abs \\\n--input=f32=-5\n</code></pre>","tags":["CPU"]},{"location":"building-from-source/riscv/#optional-configuration","title":"Optional configuration","text":"<p>RISC-V Vector extensions allows SIMD  code to run more efficiently. To enable the vector extension for the compiler  toolchain and the emulator, build the tools from the following sources:</p> <ul> <li>RISC-V toolchain is built from https://github.com/llvm/llvm-project (main branch).<ul> <li>Currently, the LLVM compiler is built on GNU toolchain, including libgcc,   GNU linker, and C libraries. You need to build GNU toolchain first.</li> <li>Clone GNU toolchain from:   https://github.com/riscv/riscv-gnu-toolchain   (master branch). Switch the \"riscv-binutils\" submodule to   <code>git://sourceware.org/git/binutils-gdb.git</code> (master branch) manually.</li> </ul> </li> <li>RISC-V QEMU is built from https://github.com/sifive/qemu/tree/v5.2.0-rvv-rvb-zfh.</li> </ul> <p>The SIMD code can be generated following the IREE CPU flow with the additional command-line flags</p> <pre><code>tools/iree-compile \\\n--iree-hal-target-backends=llvm-cpu \\\n--iree-llvmcpu-target-triple=riscv64 \\\n--iree-llvmcpu-target-cpu=generic-rv64 \\\n--iree-llvmcpu-target-abi=lp64d \\\n--iree-llvmcpu-target-cpu-features=\"+m,+a,+f,+d,+zvl512b,+v\" \\\n--riscv-v-fixed-length-vector-lmul-max=8 \\\niree_input.mlir -o mobilenet_cpu.vmfb\n</code></pre> <p>Then run on the RISC-V QEMU:</p> <pre><code>${QEMU_BIN} \\\n-cpu rv64,x-v=true,x-k=true,vlen=512,elen=64,vext_spec=v1.0 \\\n-L ${RISCV_TOOLCHAIN_ROOT}/sysroot/ \\\n../iree-build-riscv/tools/iree-run-module \\\n--device=local-task \\\n--module=mobilenet_cpu.vmfb \\\n--function=predict \\\n--input=\"1x224x224x3xf32=0\"\n</code></pre>","tags":["CPU"]},{"location":"community/","title":"Community projects","text":"<p>Projects built by community members:</p> <ul> <li> <p>The SHARK project from   nod.ai uses a forked version of IREE   (SHARK-Runtime), offering   highly tuned performance on a large corpus of machine learning programs.</p> </li> <li> <p>The IREE Bare-Metal Arm Sample   shows how to build IREE with the   Arm GNU Toolchain   for bare-metal Arm targets using the open-source firmware libraries   CMSIS and   libopencm3.</p> </li> <li> <p>The IREE C++ Template   shows one way to integrate IREE's runtime into a project with CMake.</p> </li> </ul> <p>Official repositories:</p> <ul> <li> <p>iree-jax is home to   IREE's support for JAX programs.</p> </li> <li> <p>iree-torch contains   IREE's PyTorch frontend, leveraging the   torch-mlir project.</p> </li> <li> <p>iree-samples   includes various samples and prototypes built with IREE.</p> </li> <li> <p>iree-llvm-sandbox   contains experimental work by the IREE team closely related to LLVM and   MLIR, usually with the aim of contributing back to those upstream projects.</p> </li> </ul>"},{"location":"community/tags/","title":"Tags","text":"<p>Website pages sorted by tag:</p>"},{"location":"community/tags/#android","title":"Android","text":"<ul> <li>Android cross-compilation</li> </ul>"},{"location":"community/tags/#cpu","title":"CPU","text":"<ul> <li>RISC-V cross-compilation</li> <li>Matrix Multiplication with MMT4D</li> <li>CPU - Bare-Metal</li> <li>CPU</li> </ul>"},{"location":"community/tags/#cuda","title":"CUDA","text":"<ul> <li>CUDA backend</li> <li>GPU - CUDA/ROCm</li> </ul>"},{"location":"community/tags/#gpu","title":"GPU","text":"<ul> <li>CUDA backend</li> <li>GPU - CUDA/ROCm</li> <li>GPU - Metal</li> <li>GPU - Vulkan</li> </ul>"},{"location":"community/tags/#jax","title":"JAX","text":"<ul> <li>JAX</li> <li>Extensions</li> <li>Glossary</li> </ul>"},{"location":"community/tags/#pytorch","title":"PyTorch","text":"<ul> <li>PyTorch</li> <li>Extensions</li> <li>Glossary</li> </ul>"},{"location":"community/tags/#python","title":"Python","text":"<ul> <li>JAX</li> <li>PyTorch</li> <li>TensorFlow</li> <li>TensorFlow Lite</li> <li>Python</li> </ul>"},{"location":"community/tags/#tensorflow","title":"TensorFlow","text":"<ul> <li>TFLite support via TOSA</li> <li>TensorFlow</li> <li>TensorFlow Lite</li> <li>Extensions</li> <li>Glossary</li> </ul>"},{"location":"community/tags/#vulkan","title":"Vulkan","text":"<ul> <li>GPU - Vulkan</li> </ul>"},{"location":"community/tags/#ios","title":"iOS","text":"<ul> <li>iOS cross-compilation</li> <li>GPU - Metal</li> </ul>"},{"location":"community/blog/","title":"Blog","text":"<p>Updates from the IREE team</p>"},{"location":"community/blog/2021-10-15-cuda-backend/","title":"CUDA backend","text":"<p>IREE is being designed with re-targetability as a core goal: it should be possible to use IREE to target a broad spectrum of power regimes, from embedded systems to distributed clusters; and it should be possible to extend IREE to target new back-ends without having to reinvent the wheel each time.</p> <p>To explore this, we recently branched out from our initial focus on low-latency mobile deployments with a goal of using IREE to target data center workloads on Nvidia CUDA. This post describes how we quickly brought up a CUDA back-end for IREE and used it to train BERT, then shares some metrics and next steps.</p>","tags":["GPU","CUDA"]},{"location":"community/blog/2021-10-15-cuda-backend/#bring-up","title":"Bring up","text":"","tags":["GPU","CUDA"]},{"location":"community/blog/2021-10-15-cuda-backend/#hal-support","title":"HAL support","text":"<p>IREE has a HAL API that abstract all the targets behind a common interface. The first step to supporting a CUDA target was to map the HAL API onto CUDA. We use the CUDA driver API to reduce dependencies and be closer to the hardware. The HAL API is based on other GPU APIs like Vulkan and Metal, so it was a natural fit for CUDA. The HAL API exposes memory allocations, basic fill and memset commands, kernel dispatch, and general command buffer handling. The original implementation uses the CUDA graph API as a graph maps naturally to command buffers. There is also an implementation using CUDA streams for comparison.</p> <p>HAL exposes an API that can be tested independently, even if we are not able to create CUDA kernels yet we can test a large portion of the CUDA driver using CTS tests. Those can be run to make sure a system has the required CUDA support.</p> <p></p>","tags":["GPU","CUDA"]},{"location":"community/blog/2021-10-15-cuda-backend/#compiler-support","title":"Compiler support","text":"<p>CUDA has an open source backend in LLVM generating PTX that we are leveraging. Therefore IREE can create NVVM (CUDA LLVM variant) and use LLVM's backend to generate PTX. The CUDA driver will do the \"last mile compilation\" at runtime to convert PTX into the GPU's native ISA.</p> <p>IREE compiler pipeline starts from linalg with tensor operands. A large part of the compiler is independent of the target.</p> <p>The linalg on tensor representation of the graph is broken up into dispatch regions that are processed by NVVM Codegen. A simple implementation of the compiler is to run bufferization and convert linalg to standard followed by conversion to NVVM/LLVM. Most of those transformation can re-use upstream MLIR transformations and share it with any other backend targeting LLVM IR. Leveraging MLIR conversion to LLVM will allow us to quickly go from a simple \"hello world\" to supporting full models.</p> <p>IREE code generation is based on MLIR infrastructure so each step can easily be tested independently using the MLIR lit framework.</p>","tags":["GPU","CUDA"]},{"location":"community/blog/2021-10-15-cuda-backend/#flatbuffer-definition","title":"FlatBuffer definition","text":"<p>Kernels are encoded in a FlatBuffer containing the PTX code as well as the workgroup size to use for the dispatch. This allows serialization of the kernels in the IR, it is then de-serialized by the HAL layer.</p> <pre><code>table CUDAExecutableDef {\n  // A map of entry point ordinals to string names as used in the shader\n  // library.\n  entry_points:[string];\n\n  // Block sizes for each entry point.\n  block_sizes:[CUDABlockSizeDef];\n\n  // PTX string of the module.\n  ptx_image:string;\n}\n</code></pre>","tags":["GPU","CUDA"]},{"location":"community/blog/2021-10-15-cuda-backend/#hello-world","title":"Hello world","text":"<p>Together those 3 steps are enough to provide most of the functionality and we can now successfully compile full models.</p> <p></p> <p>The steps to reproduce running a simple op end to end through CUDA backend are described here.</p>","tags":["GPU","CUDA"]},{"location":"community/blog/2021-10-15-cuda-backend/#performance","title":"Performance","text":"<p>Now that we have enabled functionality we need to look at the performance. Once again we can leverage existing MLIR transformations to speed up the developement work.</p>","tags":["GPU","CUDA"]},{"location":"community/blog/2021-10-15-cuda-backend/#tiling-and-distribution","title":"Tiling and distribution","text":"<p>The first obvious step to get efficient code on CUDA is to make sure we distribute the work on enough blocks and threads to fill up the GPU. At the time of bring up not all ops were being tiled and distributed in the common IREE layer. During dispatch region creation we apply tile and fuse which will distribute the work into a set of workgroups that are mapped to CUDA blocks.</p> <p>At the beginning of the code generation we look at the dispatch region and decide on the tile size for a workgroup. For CUDA we also decide the number of threads per block. We will then have a pass tiling the ops in the dispatch region a second time to distribute the work onto threads within the block.</p> <p>At this stage the IR looks like the following:</p> <pre><code>    %8 = \"gpu.thread_id\"() {dimension = \"x\"} : () -&gt; index\n    %9 = affine.apply affine_map&lt;()[s0] -&gt; (s0 * 4)&gt;()[%8]\n    %10 = memref.subview %in0[%9] [4] [1] : memref&lt;128xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt; to memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;\n    %11 = memref.subview %in1[%9] [4] [1] : memref&lt;128xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt; to memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;\n    %12 = memref.subview %out[%9] [4] [1] : memref&lt;128xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt; to memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;\n    linalg.generic {\n        indexing_maps = [affine_map&lt;(d0) -&gt; (d0)&gt;,\n                         affine_map&lt;(d0) -&gt; (d0)&gt;,\n                         affine_map&lt;(d0) -&gt; (d0)&gt;],\n        iterator_types = [\"parallel\"]}\n      ins(%10, %11 :\n          memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;,\n          memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;)\n      outs(%12 : memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;) {\n    ^bb0(%arg1: f32, %arg2: f32, %arg3: f32):  // no predecessors\n      %13 = addf %arg1, %arg2 : f32\n      linalg.yield %13 : f32\n    }\n</code></pre>","tags":["GPU","CUDA"]},{"location":"community/blog/2021-10-15-cuda-backend/#vectorization","title":"Vectorization","text":"<p>Even though GPUs execute most operations as scalar, memory operations are optimized to access 128 bits of data per thread. Therefore it is critical to vectorize load/store operations. After tiling to a size we vectorize the IR to get vector read/write mapping to load4/store4. This significantly improves the memory access pattern of the code generated.</p> <p>This convert the previous IR to:</p> <pre><code>    %8 = \"gpu.thread_id\"() {dimension = \"x\"} : () -&gt; index\n    %9 = affine.apply affine_map&lt;()[s0] -&gt; (s0 * 4)&gt;()[%8]\n    %10 = memref.subview %in0[%9] [4] [1] : memref&lt;128xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt; to memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;\n    %11 = memref.subview %in1[%9] [4] [1] : memref&lt;128xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt; to memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;\n    %12 = memref.subview %out[%9] [4] [1] : memref&lt;128xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt; to memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;\n    %13 = vector.transfer_read %10[%c0], %cst {in_bounds = [true]} : memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;, vector&lt;4xf32&gt;\n    %14 = vector.transfer_read %11[%c0], %cst {in_bounds = [true]} : memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;, vector&lt;4xf32&gt;\n    %15 = addf %13, %14 : vector&lt;4xf32&gt;\n    vector.transfer_write %15, %12[%c0] {in_bounds = [true]} : vector&lt;4xf32&gt;, memref&lt;4xf32, affine_map&lt;(d0)[s0] -&gt; (d0 + s0)&gt;&gt;\n</code></pre>","tags":["GPU","CUDA"]},{"location":"community/blog/2021-10-15-cuda-backend/#shared-memory-optimization","title":"Shared memory optimization","text":"<p>Nvidia GPUs have a fast shared memory that needs to be leveraged to optimize cases where we may be memory bound and have the potential to re-use memory reads.</p> <p>For operations like GEMM using shared memory gives us a significant speed up. We leverage memory promotion, vector distribution and software pipelining transformations from MLIR to generate efficient copies from global to shared memory that can be interleaved with the compute work.</p>","tags":["GPU","CUDA"]},{"location":"community/blog/2021-10-15-cuda-backend/#optimization-pipeline","title":"Optimization pipeline","text":"<p>Those different transformations compose to this flow:</p> <p></p> <p>The full dump step by step of a linalg.matmul operation can be found here.</p>","tags":["GPU","CUDA"]},{"location":"community/blog/2021-10-15-cuda-backend/#results-and-next-steps","title":"Results and next steps","text":"","tags":["GPU","CUDA"]},{"location":"community/blog/2021-10-15-cuda-backend/#gemm","title":"GEMM","text":"<p>We compare the performance of a single GEMM operation to highly optimized library cuBLAS using mmperf framework.</p> <p></p> <p>The graph can be re-produced based on instructions on mmperf</p>","tags":["GPU","CUDA"]},{"location":"community/blog/2021-10-15-cuda-backend/#future-work","title":"Future work","text":"<p>Nod.ai has contributed an experimental HAL module for ROCM that allows us to re-use the compiler parts to support ROCM, more support is going to be added in the future.</p> <p>Several performance improvements are still under progress, including optimizing the runtime allocator to reduce the host-side overhead and tuning tile sizes based profiling.</p> <p>Several models are running and we will publish more detailed benchmark results in the near future.</p>","tags":["GPU","CUDA"]},{"location":"community/blog/2021-10-13-matrix-multiplication-with-mmt4d/","title":"Matrix Multiplication with MMT4D","text":"","tags":["CPU"]},{"location":"community/blog/2021-10-13-matrix-multiplication-with-mmt4d/#introduction","title":"Introduction","text":"<p>Matrix multiplication (matmul) is an important operation in ML workloads that poses specific challenges to code generation. For example, matmul makes repeated accesses to the same data, which makes locality of reference a top concern.</p> <p>Moreover, modern CPUs instruction set architectures (ISAs) offer specialized SIMD instructions that the matmul implementation needs to use to achieve optimal performance, and these instructions expect data to be in a particular layout.</p> <p>This article is about an in-development MLIR operation, <code>linalg.mmt4d</code>, offering a compilation path for <code>linalg.matmul</code> that is designed from the ground up for these efficiency considerations.</p> <p>We are still in the early implementation phase of this <code>linalg.mmt4d</code> plan, but we feel confident that we know where we are going because what we are really doing here is importing into the compiler what we have learned working on optimized matrix multiplication libraries, particularly Ruy. We know what loop schedule and kernel we want the compiler to generate \u2014 essentially the same as we wrote in Ruy, give or take additional optimizations such as fusions and constant folding that become possible now that we are doing this within a compiler. This allows us to focus on how we get the compiler to generate that schedule and kernel with purely algebraic transformations that compose and enable further compiler optimizations.</p> <p>At the basis of this work is the extensible op system of the Linalg dialect in the MLIR compiler toolkit. In this case, a general purpose, mixed precision mmt4d op is defined via a high level description directly in the compiler and is then available to both users of the compiler (as a <code>linalg.mmt4d</code> op) or for direct emission via Python based IR construction (i.e. for direct integration into high level frameworks without rebuilding the compiler). The ability to define such new special forms cheaply, and without any systemic framework level cost, is part of the extensibility and composition story that we expect will become increasingly important in development and deployment scenarios in the future, and in this case, it let us spring board off of high quality code generation which was already well integrated and composed well with other features of the compiler.</p>","tags":["CPU"]},{"location":"community/blog/2021-10-13-matrix-multiplication-with-mmt4d/#existing-matrix-multplication-code-generation","title":"Existing Matrix Multplication Code Generation","text":"<p>Let us start by discussing IREE\u2019s existing matmul code generation and highlight the issues that <code>mmt4d</code> aims to overcome.</p> <p>The existing approach operates in-place on the source matrices. When we discuss \"tiling\" in this paragraph, we refer exclusively to the traversal \u2014 how these source matrices are traversed by the matmul loop. There is no \"tiled layout\" here, which will be the key difference with <code>mmt4d</code> below.</p> <p>The destination matrix is tiled into workgroups (CPU threads) tiles, then each workgroup tile is tiled to fit some level of CPU cache, and finally each tile is further tiled to fit target architecture registers (e.g. 8x8).</p> <p>That multi-level tiling means that the code works like the following loop nest:</p> <pre><code>def tiled_matmul(A, B, C, tile_m, tile_n, tile_k, tile_m_v, tile_n_v, tile_k_v):\n m = A.shape[0]\n k = A.shape[1]\n n = B.shape[1]\n for m1 in range(0, m, tile_m):\n   for n1 in range(0, n, tile_n):\n     for k1 in range(0, k, tile_k):\n       # First level of tiling views...\n       lhs_tile = A[m1:m1+tile_m, k1:k1+tile_k]\n       rhs_tile = B[k1:k1+tile_k, n1:n1+tile_n]\n       dst_tile = C[m1:m1+tile_m, n1:n1+tile_n]\n       for mv in range(0, tile_m, tile_m_v):\n         for nv in range(0, tile_n, tile_n_v):\n           for kv in range(0, tile_k, tile_k_v):\n             # Register tiling views...\n             lhs_tile_v = lhs_tile[mv:mv+tile_m_v, kv:kv+tile_k_v]\n             rhs_tile_v = rhs_tile[kv:kv+tile_k_v, nv:nv+tile_n_v]\n             # kernel.\n             dst_tile[mv:mv+tile_m_v, nv:nv+tile_n_v] += np.matmul(lhs_tile_v, rhs_tile_v)\n return C\n</code></pre> <p>The two main problems with this approach are:</p> <ul> <li> <p>Overhead to meet SIMD ISA layout requirements: In practice, the kernel     needs to use specific SIMD     instructions to perform the arithmetic. They expect small tiles of the     matrices to be loaded in registers, in a specific layout. If the matrix data     wasn't already stored in memory in such a tiled layout, then the kernel has     to perform such a data rearrangement on the fly, incurring substantial     overhead. For NxN matrix multiplication, the kernel performs     O(N<sup>3</sup>) work on O(N<sup>2</sup>) data, so doing that rearrangement     there means O(N<sup>3</sup>) overhead where O(N<sup>2</sup>) should have     sufficed, as this could have been done as a pre-processing step on     O(N<sup>2</sup>) data.</p> </li> <li> <p>Inefficent memory traversal: For efficiency reasons, we always need     <code>tile_m_v&gt;1</code> and <code>tile_n_v&gt;1</code>. That is because the higher these values, the     fewer memory-load instructions are needed overall; and this is also dictated     by the SIMD instructions that we want to use. But that means that the kernel     is accessing simultaneously multiple rows or columns of the left-hand and     right-hand side matrices. And in this existing approach, they are stored in     linear layout, not in a tiled layout, so these accesses are not contiguous     in memory. This is detrimental to memory access performance, meaning the     CPU caches, in multiple ways. One     is that these multiple non-contiguous accesses may alias each other in the     L1 cache because of low     associativity.</p> </li> </ul>","tags":["CPU"]},{"location":"community/blog/2021-10-13-matrix-multiplication-with-mmt4d/#matrix-multiplication-operation-with-4d-tiled-operands","title":"Matrix Multiplication Operation With 4D Tiled Operands","text":"<p>For the reasons above, an efficient matmul implementation must reorder data into a tiled layout matching the target SIMD ISA and making the memory access patterns as contiguous as possible.</p> <p>IREE/MLIR defaults to bufferizing all tensors into a \"row-major\" order, meaning that the last-enumerated dimension is the one that is contiguous in memory. As we prefer not to write custom bufferization code, we can't specify an alternative layout for a tensor. Fortunately, it is possible to represent a 2D tiled layout as a 4D layout. For example, <code>tensor&lt;2x2x2x2xf32&gt;</code> can represent a 4x4 matrix made of 2x2 tiles, each of which is 2x2. The row-major layout on <code>tensor&lt;2x2x2x2xf32&gt;</code> makes each 2x2 tile contiguous and row-major, and arranges the 2x2 tiles themselves into a row-major 2x2 layout in the overall 4x4 matrix.</p> <p>Such a row-major-tiled layout is exactly what we need for the left-hand side of a matrix multiplication, because matrix multiplication traverses the left-hand side matrix row by row. But for the right-hand side matrix, we want a column-major-tiled layout. To solve this problem, we decide to implement not matrix multiplication, but matrix-multiplication-by-transposed-right-hand-side which is where the <code>t</code> in the <code>linalg.mmt4d</code> came from. Now such an op is happy with both the left and right-hand sides being row-major-tiled.</p> <p>The following example illustrates that. In these diagrams, each matrix element is rendered its memory offset.</p> <p></p> <p>To compute the 2x2 block in the destination matrix, we will have to load two yellow blocks from LHS, RHS matrices respectively compute their matmul results (i.e. call the kernel), then the two blue blocks, and so on. As we can see, each tile loads data that is not contiguous. It would be better if we rearranged the elements in the following layout:</p> <p></p> <p>Now tiles are stored contiguously in memory and the kernel can simply load them from memory into the registers that will be directly consumed by the SIMD instructions performing the multiplications. Moreover, the kernel is now loading from just two contiguous data streams, a simple memory access pattern which is sure to be efficient (regarding caches, etc) on any reasonable target hardware.</p> <p>We introduce a <code>linalg.mmt4d</code> operation that performs such a matrix multiplication on matrices in a tiled layout represented as 4D tensors. That leaves the question of how to represent, within the linalg dialect, the conversions between ordinary matrices represented as 2D tensors, and these tiled matrices represented as 4D tensors. Moreover, these conversions should be tileable and decompose well. Thankfully, the transformation from 2D to 4D can be written as a reshape followed by a transpose as in the following digram:</p> <p></p> <p>So we can think of the outermost two dimensions of the 4D representations as the tile position in the overall matrix, and the innermost two as the element position within one tile. Hopefully the following Python pseudocode makes it more concrete:</p> <pre><code>def pack_2d_4d(operand, parallel_size, reduction_size):\n i1 = operand.shape[0] // parallel_size # M1\n i2 = parallel_size    # M0\n j1 = operand.shape[1] // reduction_size # K1\n j2 = reduction_size   # K0\n operand_4d = np.reshape(operand, [i1, i2, j1, j2])\n return np.transpose(operand_4d, [0, 2, 1, 3]) # [M1, K1, M0, K0]\n</code></pre> <p>Now the mmt4d operation will follow a structure as the multi level tiling, for simplicity we considered the case here where no L1 tiling is required only first level of distribution to workgroups:</p> <pre><code>def mmt4d(A, B, C, M0, N0, K0):\n M = A.shape[0]\n N = B.shape[1]\n Bt = np.transpose(B, [1, 0])\n A4d = pack_2d_4d(A, M0, K0)\n Bt4d = pack_2d_4d(Bt, N0, K0)\n M1 = A4d.shape[0]\n N1 = Bt4d.shape[0]\n K1 = A4d.shape[1]\n for m1 in range(0, M1):\n   for n1 in range(0, N1):\n     for k1 in range(0, K1):\n       # Tile views that are contiguous in memory.\n       lhs_tile = np.reshape(A4d[m1, k1, :, :], [M0, K0])\n       rhs_tile = np.reshape(Bt4d[n1, k1, :, :], [N0, K0])\n       # Inner kernel.\n       C[m1, n1, :, :] += np.matmul(lhs_tile, np.transpose(rhs_tile, [1, 0]))\n # 4d -&gt; 2D\n C2d = unpack_4d_2d(C)\n return C2d\n</code></pre> <p>The resulting 4D tiled matrix still needs be rearranged back to the original layout as 2D tensor:</p> <pre><code>def unpack_4d_2d(operand):\n i1 = operand.shape[0] # M1\n j1 = operand.shape[1] # N1\n i2 = operand.shape[2] # M0\n j2 = operand.shape[3] # N0\n operand_transposed = operand.transpose([0, 2, 1, 3]) # [M1, M0, N1, N0]\n return operand_transposed.reshape([i1 * i2, j1 * j2]) # [M, N]\n</code></pre>","tags":["CPU"]},{"location":"community/blog/2021-10-13-matrix-multiplication-with-mmt4d/#performance-results","title":"Performance Results","text":"<p>We benchmarked various float32 matmul problems of different sizes and the result showed that mmt4d is faster than the existing matmul implementation for bigger matrices as we can see the in the following chart:</p> <p></p> <p>The SIMD instruction being used here is the simplest kind, a <code>vector*scalar</code> multiplication, and the storage orders of the matrices allow the existing implementation to directly load the vectors from the source matrices without any rearrangement overhead. So this case is particularly friendly to the existing code, which is why the mmt4d code is only faster for bigger matrices. To understand why mmt4d is faster in that case, we collected statistics of L1 cache misses:</p> <p></p> <p>This shows that in this case, the better cache-friendliness of mmt4d, thanks to its simple contiguous memory access pattern, accounts for its higher performance.</p> <p>As we proceed with increasingly sophisticated SIMD targets, starting with the dot-product instructions found in current mobile devices for the int8 case and going to become generalized to all data types all the way to float32 over the next few years with upcoming ARM SIMD instructions, the advantage of mmt4d will widen for all sizes, not just the larger ones.</p> <p>Part of why we feel confident about the eventual performance that our approach will achieve is that, as mentioned in the introduction, we are rebuilding within the compiler an existing library's schedule and kernel, and we have benchmark results about it.</p>","tags":["CPU"]},{"location":"community/blog/2021-10-13-matrix-multiplication-with-mmt4d/#conclusion","title":"Conclusion","text":"<p>We introduced a 4d tiled representation for 2d matrix-matrix multiplication with a decomposable algebric transformations that requires only reshape and transpose of input operands, we discussed and empirically showed how that solves major drawbacks in row-major linear matmul by providing a flexible way to match different ISA layout along with better cache locality achieving near peak performance.</p> <p>As was mentioned in the introduction, this work in under active development and the next immediate steps are to prove the rest of the hypothesis by:</p> <ul> <li> <p>Handling dynamic sizes and padding to the next multiple of the target tile   size.</p> </li> <li> <p>Implementing the integer case (<code>int32 += int8 * int8</code>).</p> </li> <li> <p>Implementing the dispatch to different SIMD ISA variants at runtime.</p> </li> <li> <p>Implementing cache-friendly traversal for larger matmuls and multi-threading   by interfacing with IREE's runtime dispatch.</p> </li> <li> <p>Improving the generated code by fusing the 4d tiled layout with the   producers and consumers of the <code>linalg.mmt4d</code>.</p> </li> </ul>","tags":["CPU"]},{"location":"community/blog/2021-07-19-tflite-support-via-tosa/","title":"TFLite support via TOSA","text":"<p>IREE can now execute TensorFlow Lite (TFLite) models through the use of TOSA, an open standard of common tensor operations, and a part of MLIR core. TOSA\u2019s high-level representation of tensor operations provides a common front-end for ingesting models from different frameworks. In this case we ingest a TFLite FlatBuffer and compile it to TOSA IR, which IREE takes as an input format to compile to its various backends.</p> <p></p> <p>Using TFLite as a frontend for IREE provides an alternative ingestion method for already existing models that could benefit from IREE\u2019s design. This enables models already designed for on-device inference to have an alternative path for execution without requiring any additional porting, while benefiting from IREE\u2019s improvements in buffer management, work dispatch system, and compact binary format. With continued improvements to IREE/MLIR\u2019s compilation performance, more optimized versions can be compiled and distributed to target devices without an update to the clientside environment.</p> <p>Today, we have validated floating point support for a variety of models, including mobilenet (v1, v2, and v3) and mobilebert. More work is in progress to support fully quantized models, and TFLite\u2019s hybrid quantization, along with dynamic shape support.</p>","tags":["TensorFlow"]},{"location":"community/blog/2021-07-19-tflite-support-via-tosa/#examples","title":"Examples","text":"<p>TFLite with IREE is available in Python and Java.  We have a colab notebook that shows how to use IREE\u2019s python bindings and TFLite compiler tools to compile a pre-trained TFLite model from a FlatBuffer and run using IREE.  We also have an Android Java app that was forked from an existing TFLite demo app, swapping out the TFLite library for our own AAR.  More information on IREE\u2019s TFLite frontend is available here.</p>","tags":["TensorFlow"]},{"location":"guides/","title":"Guides","text":""},{"location":"guides/#ml-frameworks","title":"ML frameworks","text":"<p>Start here: ML frameworks overview</p> <p>Guides for specific frameworks:</p> <ul> <li> TensorFlow and    TensorFlow Lite</li> <li> JAX</li> <li> PyTorch</li> </ul>"},{"location":"guides/#deployment-configurations","title":"Deployment configurations","text":"<p>Start here: Deplyment configurations overview</p> <p>Guides for specific configurations:</p> <ul> <li> CPU for general   purpose CPU deployment</li> <li> CPU - Bare-Metal   with minimal platform dependencies</li> <li> GPU - Vulkan   for cross-platform usage and interop with graphics applications</li> <li> GPU - CUDA/ROCm   for NVIDIA/AMD-specific solutions</li> <li> GPU - Metal   for running on Apple hardware</li> </ul>"},{"location":"guides/#other-topics","title":"Other topics","text":"<ul> <li> Developer tips and tricks</li> </ul>"},{"location":"guides/developer-tips/","title":"IREE developer tips and tricks","text":"<p>The IREE compiler is built using MLIR, so it naturally supports the common MLIR debugging workflows. For areas where IREE differentiates itself, this page lists other helpful tips and tricks.</p>"},{"location":"guides/developer-tips/#setting-compiler-options","title":"Setting compiler options","text":"<p>Tools such as <code>iree-compile</code> take options via command-line flags. Pass <code>--help</code> to see the full list:</p> <pre><code>$ iree-compile --help\n\nOVERVIEW: IREE compilation driver\n\nUSAGE: iree-compile [options] &lt;input file or '-' for stdin&gt;\n\nOPTIONS:\n  ...\n</code></pre> <p>Tip - Options and the Python bindings</p> <p>If you are using the Python bindings, options can be passed via the <code>extra_args=[\"--flag\"]</code> argument:</p> <pre><code>import iree.compiler as ireec\n\ninput_mlir = \"\"\"\nfunc.func @abs(%input : tensor&lt;f32&gt;) -&gt; (tensor&lt;f32&gt;) {\n%result = math.absf %input : tensor&lt;f32&gt;\n  return %result : tensor&lt;f32&gt;\n}\"\"\"\n\ncompiled_module = ireec.tools.compile_str(\n    input_mlir,\n    target_backends=[\"llvm-cpu\"],\nextra_args=[\"--mlir-timing\"])\n</code></pre>"},{"location":"guides/developer-tips/#inspecting-vmfb-files","title":"Inspecting <code>.vmfb</code> files","text":"<p>The IREE compiler generates FlatBuffer files using the <code>.vmfb</code> file extension, short for \"Virtual Machine FlatBuffer\", which can then be loaded and executed using IREE's runtime.</p> Info - other output formats <p>The IREE compiler can output different formats with the <code>`--output-format=</code> flag:</p> Flag value Output <code>--output-format=vm-bytecode</code> (default) VM Bytecode (<code>.vmfb</code>) files <code>--output-format=vm-c</code> C source modules <p>VM Bytecode files are usable across a range of deployment scenarios, while C source modules provide low level connection points for constrained environments like bare metal platforms.</p> <p>By default, <code>.vmfb</code> files can be opened as zip files: (1)</p> <ol> <li>Setting <code>--iree-vm-emit-polyglot-zip=false</code> will disable this feature and    decrease file size slightly</li> </ol> <pre><code>$ unzip -d simple_abs_cpu ./simple_abs_cpu.vmfb\n\nArchive:  ./simple_abs_cpu.vmfb\n  extracting: simple_abs_cpu/module.fb\n  extracting: simple_abs_cpu/abs_dispatch_0_system_elf_x86_64.so\n</code></pre> <p>The embedded binary (here an ELF shared object with CPU code) can be parsed by standard tools:</p> <pre><code>$ readelf -Ws ./simple_abs_cpu/abs_dispatch_0_system_elf_x86_64.so\n\nSymbol table '.dynsym' contains 2 entries:\n  Num:    Value          Size Type    Bind   Vis      Ndx Name\n    0: 0000000000000000     0 NOTYPE  LOCAL  DEFAULT  UND\n    1: 0000000000001760    17 FUNC    GLOBAL DEFAULT    7 iree_hal_executable_library_query\n\nSymbol table '.symtab' contains 42 entries:\n  Num:    Value          Size Type    Bind   Vis      Ndx Name\n    0: 0000000000000000     0 NOTYPE  LOCAL  DEFAULT  UND\n    1: 0000000000000000     0 FILE    LOCAL  DEFAULT  ABS abs_dispatch_0\n    2: 0000000000001730    34 FUNC    LOCAL  DEFAULT    7 abs_dispatch_0_generic\n    3: 00000000000034c0    80 OBJECT  LOCAL  DEFAULT    8 iree_hal_executable_library_query_v0\n    4: 0000000000001780   111 FUNC    LOCAL  DEFAULT    7 iree_h2f_ieee\n    5: 00000000000017f0   207 FUNC    LOCAL  DEFAULT    7 iree_f2h_ieee\n    ...\n</code></pre> <p>The <code>iree-dump-module</code> tool can also be used to see information about a given <code>.vmfb</code> file:</p> <pre><code>$ iree-dump-module simple_abs.vmfb\n\n//===---------------------------------------------------------------------===//\n// @module : version 0\n//===---------------------------------------------------------------------===//\n\nRequired Types:\n  [  0] i32\n  [  1] i64\n  [  2] !hal.allocator\n  [  3] !hal.buffer\n  ...\n\nModule Dependencies:\n  hal, version &gt;= 0, required\n\nImported Functions:\n  [  0] hal.ex.shared_device() -&gt; (!vm.ref&lt;?&gt;)\n  [  1] hal.allocator.allocate(!vm.ref&lt;?&gt;, i32, i32, i64) -&gt; (!vm.ref&lt;?&gt;)\n  ...\n\nExported Functions:\n  [  0] abs(!vm.ref&lt;?&gt;) -&gt; (!vm.ref&lt;?&gt;)\n  [  1] __init() -&gt; ()\n\n...\n</code></pre>"},{"location":"guides/developer-tips/#dumping-executable-files","title":"Dumping executable files","text":"<p>The <code>--iree-hal-dump-executable-*</code> flags instruct the compiler to save files related to \"executable translation\" (code generation for a specific hardware target) into a directory of your choosing. If you are interested in seeing which operations in your input program were fused into a compute kernel or what device code was generated for a given program structure, these flags are a great starting point.</p> Flag Files dumped <code>iree-hal-dump-executable-files-to</code> All files (meta-flag) <code>iree-hal-dump-executable-sources-to</code> Source <code>.mlir</code> files prior to HAL compilation <code>iree-hal-dump-executable-intermediates-to</code> Intermediate files (e.g. <code>.o</code> files, <code>.mlir</code> stages) <code>iree-hal-dump-executable-binaries-to</code> Binary files (e.g. <code>.so</code>, <code>.spv</code>, <code>.ptx</code>), as used in the <code>.vmfb</code> <code>iree-hal-dump-executable-benchmarks-to</code> Standalone benchmark files for <code>iree-benchmark-module</code> CPUGPU - VulkanGPU - CUDA <pre><code>$ mkdir -p /tmp/iree/simple_abs/\n\n$ iree-compile simple_abs.mlir \\\n--iree-hal-target-backends=llvm-cpu \\\n--iree-llvmcpu-link-embedded=false \\\n--iree-hal-dump-executable-files-to=/tmp/iree/simple_abs \\\n-o /tmp/iree/simple_abs/simple_abs_cpu.vmfb\n\n$ ls /tmp/iree/simple_abs\n\nmodule_abs_dispatch_0.mlir\nmodule_abs_dispatch_0_system_elf_x86_64_benchmark.mlir\nmodule_abs_dispatch_0_system_elf_x86_64.codegen.bc\nmodule_abs_dispatch_0_system_elf_x86_64.linked.bc\nmodule_abs_dispatch_0_system_elf_x86_64.optimized.bc\nmodule_abs_dispatch_0_system_elf_x86_64.s\nmodule_abs_dispatch_0_system_elf_x86_64.so\nsimple_abs_cpu.vmfb\n</code></pre> <p>Tip - Embedded and system linking</p> <p>The default value of <code>--iree-llvmcpu-link-embedded=true</code> generates embedded ELF files. By disabling that flag, the compiler will produce platform-standard <code>.so</code> files for Linux, <code>.dll</code> files for Windows, etc. While embedded ELF files can be smaller and more portable, inspection of artifacts is easier with platform-standard shared object files.</p> Tip - Disassembling <code>.bc</code> files with <code>llvm-dis</code> <p>The <code>.bc</code> intermediate files use the LLVM BitCode format, which can be disassembled using <code>llvm-dis</code>:</p> <pre><code>// Build `llvm-dis` from source as needed:\n$ cmake --build iree-build/ --target llvm-dis\n$ iree-build/llvm-project/bin/llvm-dis --help\n\n$ cd /tmp/iree/simple_abs/\n$ llvm-dis module_abs_dispatch_0_system_elf_x86_64.codegen.bc\n$ cat module_abs_dispatch_0_system_elf_x86_64.codegen.ll\n\n; ModuleID = 'module_abs_dispatch_0_system_elf_x86_64.codegen.bc'\nsource_filename = \"abs_dispatch_0\"\ntarget triple = \"x86_64-linux-gnu\"\n\n%iree_hal_executable_library_header_t = type { i32, ptr, i32, i32 }\n%iree_hal_executable_dispatch_attrs_v0_t = type { i16, i16 }\n\n...\n\ndefine internal i32 @abs_dispatch_0_generic(\n    ptr noalias nonnull align 16 %0,\n    ptr noalias nonnull align 16 %1,\n    ptr noalias nonnull align 16 %2) #0 {\n  %4 = load %iree_hal_executable_dispatch_state_v0_t, ptr %1, align 8,\n  %5 = extractvalue %iree_hal_executable_dispatch_state_v0_t %4, 10,\n  %6 = load ptr, ptr %5, align 8,\n  %7 = ptrtoint ptr %6 to i64,\n  %8 = and i64 %7, 63,\n  %9 = icmp eq i64 %8, 0,\n  call void @llvm.assume(i1 %9),\n  %10 = load %iree_hal_executable_dispatch_state_v0_t, ptr %1, align 8,\n  %11 = extractvalue %iree_hal_executable_dispatch_state_v0_t %10, 10,\n  %12 = getelementptr ptr, ptr %11, i32 1,\n  %13 = load ptr, ptr %12, align 8,\n  %14 = ptrtoint ptr %13 to i64,\n  %15 = and i64 %14, 63,\n  %16 = icmp eq i64 %15, 0,\n  call void @llvm.assume(i1 %16),\n  %17 = load float, ptr %6, align 4,\n  %18 = call float @llvm.fabs.f32(float %17),\n  store float %18, ptr %13, align 4,\n  ret i32 0,\n}\n\n...\n</code></pre> <pre><code>$ mkdir -p /tmp/iree/simple_abs/\n\n$ iree-compile simple_abs.mlir \\\n--iree-hal-target-backends=vulkan-spirv \\\n--iree-hal-dump-executable-files-to=/tmp/iree/simple_abs \\\n-o /tmp/iree/simple_abs/simple_abs_vulkan.vmfb\n\n$ ls /tmp/iree/simple_abs\n\nmodule_abs_dispatch_0.mlir\nmodule_abs_dispatch_0_vulkan_spirv_fb_benchmark.mlir\nmodule_abs_dispatch_0_vulkan_spirv_fb.mlir\nmodule_abs_dispatch_0_vulkan_spirv_fb.spv\nsimple_abs_vulkan.vmfb\n</code></pre> Tip - Disassembling <code>.spv</code> files with <code>spirv-dis</code> <p>The <code>.spv</code> files use the SPIR-V binary format, which can be disassembled using <code>spirv-dis</code> from SPIR-V Tools:</p> <pre><code>$ cd /tmp/iree/simple_abs/\n$ spirv-dis module_abs_dispatch_0_vulkan_spirv_fb.spv\n\n; SPIR-V\n; Version: 1.0\n; Generator: Khronos; 22\n; Bound: 20\n; Schema: 0\n              OpCapability Shader\n              OpExtension \"SPV_KHR_storage_buffer_storage_class\"\n        %18 = OpExtInstImport \"GLSL.std.450\"\n              OpMemoryModel Logical GLSL450\n              OpEntryPoint GLCompute %abs_dispatch_0_generic \"abs_dispatch_0_generic\"\n              OpExecutionMode %abs_dispatch_0_generic LocalSize 1 1 1\n              OpName %__resource_var_0_0_ \"__resource_var_0_0_\"\n              OpName %__resource_var_0_1_ \"__resource_var_0_1_\"\n              OpName %abs_dispatch_0_generic \"abs_dispatch_0_generic\"\n              OpDecorate %_arr_float_uint_1 ArrayStride 4\n              OpMemberDecorate %_struct_2 0 Offset 0\n              OpDecorate %_struct_2 Block\n              OpDecorate %__resource_var_0_0_ Binding 0\n              OpDecorate %__resource_var_0_0_ DescriptorSet 0\n              OpDecorate %__resource_var_0_1_ Binding 1\n              OpDecorate %__resource_var_0_1_ DescriptorSet 0\n      %float = OpTypeFloat 32\n      %uint = OpTypeInt 32 0\n    %uint_1 = OpConstant %uint 1\n%_arr_float_uint_1 = OpTypeArray %float %uint_1\n  %_struct_2 = OpTypeStruct %_arr_float_uint_1\n%_ptr_StorageBuffer__struct_2 = OpTypePointer StorageBuffer %_struct_2\n%__resource_var_0_0_ = OpVariable %_ptr_StorageBuffer__struct_2 StorageBuffer\n%__resource_var_0_1_ = OpVariable %_ptr_StorageBuffer__struct_2 StorageBuffer\n      %void = OpTypeVoid\n          %9 = OpTypeFunction %void\n    %uint_0 = OpConstant %uint 0\n%_ptr_StorageBuffer_float = OpTypePointer StorageBuffer %float\n%abs_dispatch_0_generic = OpFunction %void None %9\n        %12 = OpLabel\n        %15 = OpAccessChain %_ptr_StorageBuffer_float %__resource_var_0_0_ %uint_0 %uint_0\n        %16 = OpLoad %float %15\n        %17 = OpExtInst %float %18 FAbs %16\n        %19 = OpAccessChain %_ptr_StorageBuffer_float %__resource_var_0_1_ %uint_0 %uint_0\n              OpStore %19 %17\n              OpReturn\n              OpFunctionEnd\n</code></pre> <pre><code>$ mkdir -p /tmp/iree/simple_abs/\n\n$ iree-compile simple_abs.mlir \\\n--iree-hal-target-backends=cuda \\\n--iree-hal-dump-executable-files-to=/tmp/iree/simple_abs \\\n-o /tmp/iree/simple_abs/simple_abs_cuda.vmfb\n\n$ ls /tmp/iree/simple_abs\n\nmodule_abs_dispatch_0_cuda_nvptx_fb_benchmark.mlir\nmodule_abs_dispatch_0_cuda_nvptx_fb.codegen.bc\nmodule_abs_dispatch_0_cuda_nvptx_fb.linked.bc\nmodule_abs_dispatch_0_cuda_nvptx_fb.optimized.bc\nmodule_abs_dispatch_0_cuda_nvptx_fb.ptx\nmodule_abs_dispatch_0.mlir\nsimple_abs_cuda.vmfb\n</code></pre> Tip - Disassembling <code>.bc</code> files with <code>llvm-dis</code> <p>The <code>.bc</code> intermediate files use the LLVM BitCode format, which can be disassembled using <code>llvm-dis</code>:</p> <pre><code>// Build `llvm-dis` from source as needed:\n$ cmake --build iree-build/ --target llvm-dis\n$ iree-build/llvm-project/bin/llvm-dis --help\n\n$ cd /tmp/iree/simple_abs/\n$ llvm-dis module_abs_dispatch_0_cuda_nvptx_fb.codegen.bc\n$ cat module_abs_dispatch_0_cuda_nvptx_fb.codegen.ll\n\n; ModuleID = 'module_abs_dispatch_0_cuda_nvptx_fb.codegen.bc'\nsource_filename = \"abs_dispatch_0\"\n\ndeclare ptr @malloc(i64)\n\ndeclare void @free(ptr)\n\ndeclare float @__nv_fabsf(float)\n\ndefine void @abs_dispatch_0_generic(ptr noalias readonly align 16 %0, ptr noalias align 16 %1) {\n  %3 = ptrtoint ptr %0 to i64\n  %4 = and i64 %3, 63\n  %5 = icmp eq i64 %4, 0\n  call void @llvm.assume(i1 %5)\n  %6 = ptrtoint ptr %1 to i64\n  %7 = and i64 %6, 63\n  %8 = icmp eq i64 %7, 0\n  call void @llvm.assume(i1 %8)\n  %9 = load float, ptr %0, align 4\n  %10 = call float @__nv_fabsf(float %9)\n  store float %10, ptr %1, align 4\n  ret void\n}\n\n!nvvm.annotations = !{!0, !1, !2, !3}\n\n!0 = !{ptr @abs_dispatch_0_generic, !\"kernel\", i32 1}\n!1 = !{ptr @abs_dispatch_0_generic, !\"maxntidx\", i32 1}\n!2 = !{ptr @abs_dispatch_0_generic, !\"maxntidy\", i32 1}\n!3 = !{ptr @abs_dispatch_0_generic, !\"maxntidz\", i32 1}\n</code></pre>"},{"location":"guides/developer-tips/#compiling-phase-by-phase","title":"Compiling phase by phase","text":"<p>IREE compiles programs through a series of broad phases:</p> <pre><code>graph LR\n  accTitle: Compilation phases overview\n  accDescr: Input to ABI to Flow to Stream to HAL to VM\n\n  A([Input])\n  A --&gt; B([ABI])\n  B --&gt; C([Flow])\n  C --&gt; D([Stream])\n  D --&gt; E([HAL])\n  E --&gt; F([VM])</code></pre> Tip - available phases <p>These are the phase names available for use with the <code>--compile-to</code> and <code>--compile-from</code> flags described below:</p> Phase name Description <code>input</code> Performs input processing and lowering into core IREE input dialects (linalg/etc) <code>abi</code> Adjusts the program ABI for the specified execution environment <code>preprocessing</code> Applies customizable <code>preprocessing</code> prior to FLow/Stream/HAL/VM <code>flow</code> Models execution data flow and partitioning using the <code>flow</code> dialect <code>stream</code> Models execution partitioning and scheduling using the <code>stream</code> dialect <code>executable-sources</code> Prepares <code>hal</code> dialect executables for translation, prior to codegen <code>executable-targets</code> Runs code generation for <code>hal</code> dialect executables <code>hal</code> Finishes <code>hal</code> dialect processing <code>vm</code> Lowers to IREE's abstract virtual machine using the <code>vm</code> dialect <code>end</code> Completes the full compilation pipeline <p>For an accurate list of phases, see the source code or check the help output with a command such as:</p> <pre><code>iree-compile --help | sed -n '/--compile-to/,/--/p' | head -n -1\n</code></pre> <p>You can output a program snapshot at intermediate phases with the <code>--compile-to=&lt;phase name&gt;</code> flag:</p> <pre><code>$ cat simple_abs.mlir\n\nfunc.func @abs(%input : tensor&lt;f32&gt;) -&gt; (tensor&lt;f32&gt;) {\n  %result = math.absf %input : tensor&lt;f32&gt;\n  return %result : tensor&lt;f32&gt;\n}\n\n$ iree-compile simple_abs.mlir --compile-to=abi\n\nmodule {\n  func.func @abs(%arg0: !hal.buffer_view) -&gt; !hal.buffer_view attributes {iree.abi.stub} {\n    %0 = hal.tensor.import %arg0 \"input 0\" : !hal.buffer_view -&gt; tensor&lt;f32&gt;\n    %1 = math.absf %0 : tensor&lt;f32&gt;\n    %2 = hal.tensor.export %1 \"output 0\" : tensor&lt;f32&gt; -&gt; !hal.buffer_view\n    return %2 : !hal.buffer_view\n  }\n}\n</code></pre> <p>This is similar to the <code>--mlir-print-ir-after=</code> flag, but at clearly defined pipeline phases.</p> <p>Compilation can be continued from any intermediate phase. This allows for interative workflows - compile to a phase, make edits to the <code>.mlir</code> file, then resume compilation and continue through the pipeline:</p> <pre><code>$ iree-compile simple_abs.mlir --compile-to=abi -o simple_abs_abi.mlir\n\n$ sed \\\n-e 's/math.absf/math.exp/' \\\n-e 's/@abs/@exp/' \\\nsimple_abs_abi.mlir &gt; simple_exp_abi.mlir\n\n$ iree-compile simple_exp_abi.mlir \\\n--iree-hal-target-backends=llvm-cpu \\\n-o simple_exp_cpu.vmfb\n</code></pre> <p>or explicitly resume from an intermediate phase with <code>--compile-from=&lt;phase name&gt;</code>:</p> <pre><code>$ iree-compile simple_exp_abi.mlir \\\n--iree-hal-target-backends=llvm-cpu \\\n--compile-from=abi \\\n-o simple_exp_cpu.vmfb\n</code></pre>"},{"location":"guides/deployment-configurations/","title":"Deployment configurations","text":"<p>IREE provides a flexible set of tools for various deployment scenarios. Fully featured environments can use IREE to load programs on demand and to take advantage of multi-threaded hardware, while embedded systems can bypass IREE's runtime entirely or interface with custom accelerators.</p>"},{"location":"guides/deployment-configurations/#stable-configurations","title":"Stable configurations","text":"<ul> <li> CPU for general   purpose CPU deployment</li> <li> CPU - Bare-Metal   with minimal platform dependencies</li> <li> GPU - Vulkan   for cross-platform usage and interop with graphics applications</li> <li> GPU - CUDA/ROCm   for NVIDIA/AMD-specific solutions</li> <li> GPU - Metal   for running on Apple hardware</li> </ul> <p>These are just the most stable configurations IREE supports. Feel free to reach out on any of IREE's communication channels if you have questions about a specific platform, hardware accelerator, or set of system features.</p>"},{"location":"guides/deployment-configurations/#compiler-target-backends","title":"Compiler target backends","text":"<p>Compiler target backends are used to generate executable code for hardware APIs and device architectures. Compiler targets may implement special optimizations or generate distinct code for certain device/architecture/performance profiles.</p> <p>When compiling programs, a list of target backends must be specified via</p> <ul> <li><code>--iree-hal-target-backends=</code> (command-line)</li> <li><code>target_backends=[...]</code> (Python)</li> </ul> Target backend Description Compatible HAL devices <code>llvm-cpu</code> Code generation for CPU-like devices supported by LLVM <code>local-sync</code>, <code>local-task</code> <code>vmvx</code> Portable interpreter powered by a microkernel library <code>local-sync</code>, <code>local-task</code> <code>vulkan</code> or<code>vulkan-spirv</code> Portable GPU support via SPIR-V for Vulkan <code>vulkan</code> <code>cuda</code> NVIDIA GPU support via PTX for CUDA <code>cuda</code> <code>metal</code> or<code>metal-spirv</code> GPU support on Apple platforms via MSL for Metal <code>metal</code> <code>rocm</code> Experimental  AMD GPU support via HSACO for ROCm <code>rocm</code> <code>webgpu-wgsl</code> Experimental  GPU support on the Web via WGSL for WebGPU <code>webgpu</code> <p>Tip - listing available backends</p> <p>The list of compiler target backends can be queried:</p> Command-linePython bindings <pre><code>$ iree-compile --iree-hal-list-target-backends\n\nRegistered target backends:\n    cuda\n    llvm-cpu\n    metal\n    metal-spirv\n    vmvx\n    vmvx-inline\n    vulkan\n    vulkan-spirv\n</code></pre> <pre><code>iree.compiler.query_available_targets()\n\n['cuda',\n 'llvm-cpu',\n 'metal',\n 'metal-spirv',\n 'vmvx',\n 'vmvx-inline',\n 'vulkan',\n 'vulkan-spirv']\n</code></pre>"},{"location":"guides/deployment-configurations/#runtime-hal-driversdevices","title":"Runtime HAL drivers/devices","text":"<p>Runtime HAL devices call into hardware APIs to load and run executable code. Devices may use multithreading or other system resources, depending on their focus and the build configuration.</p> HAL device Description <code>local-sync</code> Synchronous local CPU device with inline execution <code>local-task</code> Multithreaded local CPU device using a 'task' executor <code>vulkan</code> Portable GPU execution using the Vulkan API <code>cuda</code> NVIDIA GPU execution using CUDA <code>metal</code> GPU execution on Apple platforms using Metal <code>rocm</code> Experimental  AMD GPU execution using ROCm <code>webgpu</code> Experimental  GPU execution on the web using WebGPU <p>Additional HAL drivers can also be defined external to the core project via <code>IREE_EXTERNAL_HAL_DRIVERS</code>.</p>"},{"location":"guides/deployment-configurations/bare-metal/","title":"Running on a bare-metal platform","text":"<p>IREE supports model execution via CPU on bare-metal platforms. Bare metal platforms have no operating system support, and executables are built using machine-specific linker scripts and/or board support packages (BSPs).</p> <p>Bare-metal deployment typically uses IREE's LLVM compiler target backend much like the CPU configuration, but using a limited subset of IREE's CPU HAL driver code at runtime to load and execute compiled programs.</p>","tags":["CPU"]},{"location":"guides/deployment-configurations/bare-metal/#prerequisites","title":"Prerequisites","text":"<p>Out-of-tree bare-metal platform tools and source code for the system should be ready, such as</p> <ul> <li>Compilation toolchain</li> <li>Platform linker script</li> <li>Firmware libraries</li> </ul> <p>Please follow the instructions to retrieve the IREE compiler.</p>","tags":["CPU"]},{"location":"guides/deployment-configurations/bare-metal/#compile-the-model-for-bare-metal","title":"Compile the model for bare-metal","text":"<p>The model can be compiled with the following command:</p> <pre><code>iree-compile \\\n--iree-stream-partitioning-favor=min-peak-memory \\\n--iree-hal-target-backends=llvm-cpu \\\n--iree-llvmcpu-target-triple=x86_64-pc-linux-elf \\\n--iree-llvmcpu-debug-symbols=false \\\nsamples/models/simple_abs.mlir \\\n-o /tmp/simple_abs_cpu.vmfb\n</code></pre> <p>In which</p> <ul> <li><code>--iree-stream-partitioning-favor=min-peak-memory</code>: Optimize for minimum peak     memory usage at the cost of concurrency - include when targeting     single-threaded execution to reduce memory consumption.</li> <li><code>--iree-hal-target-backends=llvm-cpu</code>: Compile using the LLVM CPU target</li> <li><code>--iree-llvmcpu-target-triple</code>: Use the <code>&lt;arch&gt;-pc-linux-elf</code> LLVM target triple     so the artifact has a fixed ABI to be rendered by the     elf_module library</li> <li><code>--iree-llvmcpu-debug-symbols=false</code>: To reduce the artifact size</li> </ul> <p>See generate.sh for example command-line instructions of some common architectures.</p> <p>You can replace the MLIR file with the other MLIR model files, following the instructions.</p>","tags":["CPU"]},{"location":"guides/deployment-configurations/bare-metal/#compiling-the-bare-metal-model-for-static-library-support","title":"Compiling the bare-metal model for static-library support","text":"<p>See the static_library demo sample for an example and instructions on running a model with IREE's <code>static_library_loader</code>.</p> <p>By default, the demo targets the host machine when compiling. To produce a bare-metal compatible model, run <code>iree-compile</code> as in the previous example and add the additional <code>-iree-llvmcpu-static-library-output-path=</code> flag to specify the static library destination. This will produce a <code>.h\\.o</code> file to link directly into the target application.</p>","tags":["CPU"]},{"location":"guides/deployment-configurations/bare-metal/#build-bare-metal-runtime-from-source","title":"Build bare-metal runtime from source","text":"<p>A few CMake options and macros should be set to build a subset of IREE runtime libraries compatible with the bare-metal platform. We assume there's no multi-thread control nor system library support in the bare-metal system. The model execution is in a single-thread synchronous fashion.</p>","tags":["CPU"]},{"location":"guides/deployment-configurations/bare-metal/#set-cmake-options","title":"Set CMake options","text":"<pre><code># Build the IREE runtime only\nset(IREE_BUILD_COMPILER OFF)\n\n# Tell CMake to skip targeting a specific operating system\nset(CMAKE_SYSTEM_NAME Generic)\n\n# Disable multi-thread library support\nset(IREE_ENABLE_THREADING OFF)\n\n# Only enable the local synchronous HAL driver\nset(IREE_HAL_DRIVER_DEFAULTS OFF)\nset(IREE_HAL_DRIVER_LOCAL_SYNC ON)\n\n# Only enable some executable loaders\nset(IREE_HAL_EXECUTABLE_LOADER_DEFAULTS OFF)\nset(IREE_HAL_EXECUTABLE_LOADER_EMBEDDED_ELF ON)\nset(IREE_HAL_EXECUTABLE_LOADER_VMVX_MODULE ON)\n\n# Only enable the embedded ELF executable plugin\nset(IREE_HAL_EXECUTABLE_PLUGIN_DEFAULTS OFF)\nset(IREE_HAL_EXECUTABLE_PLUGIN_EMBEDDED_ELF ON)\n\n# Disable tests until IREE supports running them on bare-metal platforms\nset(IREE_BUILD_TESTS OFF)\n\n# Build samples\nset(IREE_BUILD_SAMPLES ON)\n</code></pre> <p>Todo</p> <p>Clean the list up after #6353 is fixed.</p> <p>Also, set the toolchain-specific cmake file to match the tool path, target architecture, target abi, linker script, system library path, etc.</p>","tags":["CPU"]},{"location":"guides/deployment-configurations/bare-metal/#define-iree-macros","title":"Define IREE macros","text":"<p>These macros should be defined, either in C/C++ or via CMake options like</p> <pre><code>set(MY_FLAGS \"-DIREE_PLATFORM_GENERIC=1\")\nset(CMAKE_C_FLAGS ${MY_FLAGS} ${CMAKE_C_FLAGS})\nset(CMAKE_CXX_FLAGS ${MY_FLAGS} ${CMAKE_CXX_FLAGS})\n</code></pre> Macro Description <code>IREE_PLATFORM_GENERIC</code> Let IREE build the runtime library without targeting a specific platform. <code>IREE_SYNCHRONIZATION_DISABLE_UNSAFE=1</code> Disable thread synchronization support.Must only be used if there's a single thread. <code>IREE_FILE_IO_ENABLE=0</code> Disable file I/O. <code>IREE_TIME_NOW_FN</code> A function to return the system time. For the bare-metal systems, it can be set as <code>IREE_TIME_NOW_FN=\\\"\\{ return 0;\\}\\\"</code> as there's no asynchronous wait handling. <code>IREE_WAIT_UNTIL_FN</code> A function to wait until the given time in nanoseconds. Must match the signature <code>bool(uint64_t nanos)</code> and return false if the wait failed. <p>Examples of how to setup the CMakeLists.txt and .cmake file:</p> <ul> <li>IREE RISC-V toolchain cmake</li> <li>IREE Bare-Metal Arm Sample</li> <li>IREE Bare-Metal RV32 Sample</li> </ul>","tags":["CPU"]},{"location":"guides/deployment-configurations/bare-metal/#bare-metal-execution-example","title":"Bare-metal execution example","text":"<p>See simple_embedding for generic platform to see how to use the IREE runtime library to build/run the IREE model for the bare-metal target.</p>","tags":["CPU"]},{"location":"guides/deployment-configurations/cpu/","title":"CPU deployment","text":"<p>IREE supports efficient program execution on CPU devices by using LLVM to compile all dense computations in each program into highly optimized CPU native instruction streams, which are embedded in one of IREE's deployable formats.</p> <p>To compile a program for CPU execution, pick one of IREE's supported executable formats:</p> Executable Format Description embedded ELF portable, high performance dynamic library system library platform-specific dynamic library (.so, .dll, etc.) VMVX reference target <p>At runtime, CPU executables can be loaded using one of IREE's CPU HAL drivers:</p> <ul> <li><code>local-task</code>: asynchronous, multithreaded driver built on IREE's \"task\"    system</li> <li><code>local-sync</code>: synchronous, single-threaded driver that executes work inline</li> </ul> <p>Todo</p> <p>Add IREE's CPU support matrix: what architectures are supported; what architectures are well optimized; etc.</p>","tags":["CPU"]},{"location":"guides/deployment-configurations/cpu/#prerequisites","title":"Prerequisites","text":"","tags":["CPU"]},{"location":"guides/deployment-configurations/cpu/#get-the-iree-compiler","title":"Get the IREE compiler","text":"","tags":["CPU"]},{"location":"guides/deployment-configurations/cpu/#download-the-compiler-from-a-release","title":"Download the compiler from a release","text":"<p>Python packages are regularly published to PyPI. See the Python Bindings page for more details. The core <code>iree-compiler</code> package includes the LLVM-based CPU compiler:</p> Stable releases Nightly releases <p>Stable release packages are published to PyPI.</p> <pre><code>python -m pip install iree-compiler\n</code></pre> <p>Nightly releases are published on GitHub releases.</p> <pre><code>python -m pip install \\\n--find-links https://openxla.github.io/iree/pip-release-links.html \\\n--upgrade iree-compiler\n</code></pre> <p>Tip</p> <p><code>iree-compile</code> is installed to your python module installation path. If you pip install with the user mode, it is under <code>${HOME}/.local/bin</code>, or <code>%APPDATA%Python</code> on Windows. You may want to include the path in your system's <code>PATH</code> environment variable:</p> <pre><code>export PATH=${HOME}/.local/bin:${PATH}\n</code></pre>","tags":["CPU"]},{"location":"guides/deployment-configurations/cpu/#build-the-compiler-from-source","title":"Build the compiler from source","text":"<p>Please make sure you have followed the Getting started page to build IREE for your host platform and the Android cross-compilation or iOS cross-compilation page if you are cross compiling for a mobile device. The <code>llvm-cpu</code> compiler backend is compiled in by default on all platforms.</p> <p>Ensure that the <code>IREE_TARGET_BACKEND_LLVM_CPU</code> CMake option is <code>ON</code> when configuring for the host.</p> <p>Tip</p> <p><code>iree-compile</code> will be built under the <code>iree-build/tools/</code> directory. You may want to include this path in your system's <code>PATH</code> environment variable.</p>","tags":["CPU"]},{"location":"guides/deployment-configurations/cpu/#get-the-iree-runtime","title":"Get the IREE runtime","text":"<p>You will need to get an IREE runtime that supports the local CPU HAL driver, along with the appropriate executable loaders for your application.</p> <p>You can check for CPU support by looking for the <code>local-sync</code> and <code>local-task</code> drivers:</p> <pre><code>$ iree-run-module --list_drivers\n\n        cuda: CUDA (dynamic)\n  local-sync: Local execution using a lightweight inline synchronous queue\n  local-task: Local execution using the IREE multithreading task system\n      vulkan: Vulkan 1.x (dynamic)\n</code></pre>","tags":["CPU"]},{"location":"guides/deployment-configurations/cpu/#build-the-runtime-from-source","title":"Build the runtime from source","text":"<p>Please make sure you have followed the Getting started page to build IREE for your host platform and the Android cross-compilation page if you are cross compiling for Android. The local CPU HAL drivers are compiled in by default on all platforms.</p> <p>Ensure that the <code>IREE_HAL_DRIVER_LOCAL_TASK</code> and <code>IREE_HAL_EXECUTABLE_LOADER_EMBEDDED_ELF</code> (or other executable loader) CMake options are <code>ON</code> when configuring for the target.</p>","tags":["CPU"]},{"location":"guides/deployment-configurations/cpu/#compile-and-run-a-program","title":"Compile and run a program","text":"<p>With the requirements out of the way, we can now compile a model and run it.</p>","tags":["CPU"]},{"location":"guides/deployment-configurations/cpu/#compile-a-program","title":"Compile a program","text":"<p>The IREE compiler transforms a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR) expected by the IREE compiler first.</p> <p>Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer. Then run the following command to compile with the <code>llvm-cpu</code> target:</p> <pre><code>iree-compile \\\n--iree-hal-target-backends=llvm-cpu \\\nmobilenet_iree_input.mlir -o mobilenet_cpu.vmfb\n</code></pre> <p>Tip - CPU targets</p> <p>The <code>--iree-llvmcpu-target-triple</code> flag tells the compiler to generate code for a specific type of CPU. You can see the list of supported targets with <code>iree-compile --iree-llvmcpu-list-targets</code>, or pass \"host\" to let LLVM infer the triple from your host machine (e.g. <code>x86_64-linux-gnu</code>).</p> <pre><code>$ iree-compile --iree-llvmcpu-list-targets\n\n  Registered Targets:\n    aarch64    - AArch64 (little endian)\n    aarch64_32 - AArch64 (little endian ILP32)\n    aarch64_be - AArch64 (big endian)\n    arm        - ARM\n    arm64      - ARM64 (little endian)\n    arm64_32   - ARM64 (little endian ILP32)\n    armeb      - ARM (big endian)\n    riscv32    - 32-bit RISC-V\n    riscv64    - 64-bit RISC-V\n    wasm32     - WebAssembly 32-bit\n    wasm64     - WebAssembly 64-bit\n    x86        - 32-bit X86: Pentium-Pro and above\n    x86-64     - 64-bit X86: EM64T and AMD64\n</code></pre> <p>Tip - CPU features</p> <p>The <code>--iree-llvmcpu-target-cpu-features</code> flag tells the compiler to generate code using certain CPU \"features\", like SIMD instruction sets. Like the target triple, you can pass \"host\" to this flag to let LLVM infer the features supported by your host machine.</p>","tags":["CPU"]},{"location":"guides/deployment-configurations/cpu/#run-a-compiled-program","title":"Run a compiled program","text":"<p>In the build directory, run the following command:</p> <pre><code>tools/iree-run-module \\\n--device=local-task \\\n--module=mobilenet_cpu.vmfb \\\n--function=predict \\\n--input=\"1x224x224x3xf32=0\"\n</code></pre> <p>The above assumes the exported function in the model is named as <code>predict</code> and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see <code>iree-run-module --help</code> for the format to specify concrete values.</p>","tags":["CPU"]},{"location":"guides/deployment-configurations/gpu-cuda-rocm/","title":"GPU deployment using CUDA and ROCm","text":"<p>IREE can accelerate model execution on Nvidia GPUs using CUDA and on AMD GPUs using ROCm. Due to the similarity of CUDA and ROCm APIs and infrastructure, the CUDA and ROCm backends share much of their implementation in IREE:</p> <ul> <li>The IREE compiler uses a similar GPU code generation pipeline for each, but   generates PTX for CUDA and hsaco for ROCm</li> <li>The IREE runtime HAL driver for ROCm mirrors the one for CUDA, except for   command buffers implementations - where CUDA has \"direct\", \"stream\", and   \"graph\" command buffers, and ROCm has only \"direct\" command buffers</li> </ul>","tags":["GPU","CUDA"]},{"location":"guides/deployment-configurations/gpu-cuda-rocm/#prerequisites","title":"Prerequisites","text":"<p>In order to use CUDA or ROCm to drive the GPU, you need to have a functional CUDA or ROCm environment. It can be verified by the following steps:</p> Nvidia/CUDAAMD/ROCm <p>Run the following command in a shell:</p> <pre><code>nvidia-smi | grep CUDA\n</code></pre> <p>If <code>nvidia-smi</code> does not exist, you will need to install the latest CUDA Toolkit SDK.</p> <p>Run the following command in a shell:</p> <pre><code>rocm-smi | grep rocm\n</code></pre> <p>If <code>rocm-smi</code> does not exist, you will need to install the latest ROCm Toolkit SDK).</p>","tags":["GPU","CUDA"]},{"location":"guides/deployment-configurations/gpu-cuda-rocm/#get-the-iree-compiler","title":"Get the IREE compiler","text":"","tags":["GPU","CUDA"]},{"location":"guides/deployment-configurations/gpu-cuda-rocm/#download-the-compiler-from-a-release","title":"Download the compiler from a release","text":"Nvidia/CUDAAMD/ROCm <p>Python packages are regularly published to PyPI. See the Python Bindings page for more details. The core <code>iree-compiler</code> package includes the CUDA compiler:</p> Stable releases Nightly releases <p>Stable release packages are published to PyPI.</p> <pre><code>python -m pip install iree-compiler\n</code></pre> <p>Nightly releases are published on GitHub releases.</p> <pre><code>python -m pip install \\\n--find-links https://openxla.github.io/iree/pip-release-links.html \\\n--upgrade iree-compiler\n</code></pre> <p>Tip</p> <p><code>iree-compile</code> is installed to your python module installation path. If you pip install with the user mode, it is under <code>${HOME}/.local/bin</code>, or <code>%APPDATA%Python</code> on Windows. You may want to include the path in your system's <code>PATH</code> environment variable:</p> <pre><code>export PATH=${HOME}/.local/bin:${PATH}\n</code></pre> <p>Currently ROCm is NOT supported for the Python interface.</p>","tags":["GPU","CUDA"]},{"location":"guides/deployment-configurations/gpu-cuda-rocm/#build-the-compiler-from-source","title":"Build the compiler from source","text":"<p>Please make sure you have followed the Getting started page to build the IREE compiler, then enable the CUDA compiler target with the <code>IREE_TARGET_BACKEND_CUDA</code> option or the ROCm compiler target with the <code>IREE_TARGET_BACKEND_ROCM</code> option.</p> <p>Tip</p> <p><code>iree-compile</code> will be built under the <code>iree-build/tools/</code> directory. You may want to include this path in your system's <code>PATH</code> environment variable.</p>","tags":["GPU","CUDA"]},{"location":"guides/deployment-configurations/gpu-cuda-rocm/#get-the-iree-runtime","title":"Get the IREE runtime","text":"<p>Next you will need to get an IREE runtime that includes the CUDA (for Nvidia hardware) or ROCm (for AMD hardware) HAL driver.</p>","tags":["GPU","CUDA"]},{"location":"guides/deployment-configurations/gpu-cuda-rocm/#build-the-runtime-from-source","title":"Build the runtime from source","text":"<p>Please make sure you have followed the Getting started page to build IREE from source, then enable the CUDA HAL driver with the <code>IREE_HAL_DRIVER_CUDA</code> option or the experimental ROCm HAL driver with the <code>IREE_EXTERNAL_HAL_DRIVERS=rocm</code> option.</p>","tags":["GPU","CUDA"]},{"location":"guides/deployment-configurations/gpu-cuda-rocm/#compile-and-run-a-program-model","title":"Compile and run a program model","text":"<p>With the compiler and runtime ready, we can now compile programs and run them on GPUs.</p>","tags":["GPU","CUDA"]},{"location":"guides/deployment-configurations/gpu-cuda-rocm/#compile-a-program","title":"Compile a program","text":"<p>The IREE compiler transforms a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR) expected by the IREE compiler first.</p> <p>Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer. Then run one of the following commands to compile:</p> Nvidia/CUDAAMD/ROCm <pre><code>iree-compile \\\n--iree-hal-target-backends=cuda \\\n--iree-hal-cuda-llvm-target-arch=&lt;...&gt; \\\nmobilenet_iree_input.mlir -o mobilenet_cuda.vmfb\n</code></pre> <p>Note that a cuda target architecture(<code>iree-hal-cuda-llvm-target-arch</code>) of the form <code>sm_&lt;arch_number&gt;</code> is needed to compile towards each GPU architecture. If no architecture is specified then we will default to <code>sm_35</code>.</p> <p>Here is a table of commonly used architectures:</p> CUDA GPU Target Architecture Nvidia K80 <code>sm_35</code> Nvidia P100 <code>sm_60</code> Nvidia V100 <code>sm_70</code> Nvidia A100 <code>sm_80</code> <pre><code>iree-compile \\\n--iree-hal-target-backends=rocm \\\n--iree-rocm-target-chip=&lt;...&gt; \\\n--iree-rocm-link-bc=true \\\n--iree-rocm-bc-dir=&lt;...&gt; \\\nmobilenet_iree_input.mlir -o mobilenet_rocm.vmfb\n</code></pre> <p>Note ROCm Bitcode Dir(<code>iree-rocm-bc-dir</code>) path is required. If the system you are compiling IREE in has ROCm installed, then the default value of <code>/opt/rocm/amdgcn/bitcode</code> will usually suffice. If you intend on building ROCm compiler in a non-ROCm capable system, please set <code>iree-rocm-bc-dir</code> to the absolute path where you might have saved the amdgcn bitcode.</p> <p>Note that a ROCm target chip(<code>iree-rocm-target-chip</code>) of the form <code>gfx&lt;arch_number&gt;</code> is needed to compile towards each GPU architecture. If no architecture is specified then we will default to <code>gfx908</code>.</p> <p>Here is a table of commonly used architectures:</p> AMD GPU Target Chip AMD MI25 <code>gfx900</code> AMD MI50 <code>gfx906</code> AMD MI60 <code>gfx906</code> AMD MI100 <code>gfx908</code>","tags":["GPU","CUDA"]},{"location":"guides/deployment-configurations/gpu-cuda-rocm/#run-a-compiled-program","title":"Run a compiled program","text":"<p>Run the following command:</p> Nvidia/CUDAAMD/ROCm <pre><code>iree-run-module \\\n--device=cuda \\\n--module=mobilenet_cuda.vmfb \\\n--function=predict \\\n--input=\"1x224x224x3xf32=0\"\n</code></pre> <pre><code>iree-run-module \\\n--device=rocm \\\n--module=mobilenet_rocm.vmfb \\\n--function=predict \\\n--input=\"1x224x224x3xf32=0\"\n</code></pre> <p>The above assumes the exported function in the model is named as <code>predict</code> and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see <code>iree-run-module --help</code> for the format to specify concrete values.</p>","tags":["GPU","CUDA"]},{"location":"guides/deployment-configurations/gpu-metal/","title":"GPU deployment using Metal","text":"<p>Documentation coming soon!</p>","tags":["GPU","iOS"]},{"location":"guides/deployment-configurations/gpu-vulkan/","title":"GPU deployment using Vulkan","text":"<p>IREE can accelerate model execution on GPUs via Vulkan, a low-overhead graphics and compute API. Vulkan is cross-platform: it is available on many operating systems, including Android, Linux, and Windows. Vulkan is also cross-vendor: it is supported by most GPU vendors, including AMD, ARM, Intel, NVIDIA, and Qualcomm.</p>","tags":["GPU","Vulkan"]},{"location":"guides/deployment-configurations/gpu-vulkan/#support-matrix","title":"Support matrix","text":"<p>As IREE and the compiler ecosystem it operates within matures, more target specific optimizations will be implemented. At this stage, expect reasonable performance across all GPUs and for improvements to be made over time for specific vendors and architectures.</p> GPU Vendor Category Performance Focus Architecture ARM Mali GPU Mobile Good Valhall Qualcomm Adreno GPU Mobile Reasonable 640+ AMD GPU Desktop/server Reasonable - NVIDIA GPU Desktop/server Good -","tags":["GPU","Vulkan"]},{"location":"guides/deployment-configurations/gpu-vulkan/#prerequisites","title":"Prerequisites","text":"<p>In order to use Vulkan to drive the GPU, you need to have a functional Vulkan environment. IREE requires Vulkan 1.1 on Android and 1.2 elsewhere. It can be verified by the following steps:</p> AndroidLinuxWindows <p>Android mandates Vulkan 1.1 support since Android 10. You just need to make sure the device's Android version is 10 or higher.</p> <p>Run the following command in a shell:</p> <pre><code>vulkaninfo | grep apiVersion\n</code></pre> <p>If <code>vulkaninfo</code> does not exist, you will need to install the latest Vulkan SDK. Installing via LunarG's package repository is recommended, as it places Vulkan libraries and tools under system paths so it's easy to discover.</p> <p>If the listed version is lower than Vulkan 1.2, you will need to update the driver for your GPU.</p> <p>Run the following command in a shell:</p> <pre><code>vulkaninfo | grep apiVersion\n</code></pre> <p>If <code>vulkaninfo</code> does not exist, you will need to install the latest Vulkan SDK.</p> <p>If the listed version is lower than Vulkan 1.2, you will need to update the driver for your GPU.</p>","tags":["GPU","Vulkan"]},{"location":"guides/deployment-configurations/gpu-vulkan/#get-the-iree-compiler","title":"Get the IREE compiler","text":"<p>Vulkan expects the program running on GPU to be expressed by the SPIR-V binary exchange format, which the model must be compiled into.</p>","tags":["GPU","Vulkan"]},{"location":"guides/deployment-configurations/gpu-vulkan/#download-the-compiler-from-a-release","title":"Download the compiler from a release","text":"<p>Python packages are regularly published to PyPI. See the Python Bindings page for more details. The core <code>iree-compiler</code> package includes the SPIR-V compiler:</p> Stable releases Nightly releases <p>Stable release packages are published to PyPI.</p> <pre><code>python -m pip install iree-compiler\n</code></pre> <p>Nightly releases are published on GitHub releases.</p> <pre><code>python -m pip install \\\n--find-links https://openxla.github.io/iree/pip-release-links.html \\\n--upgrade iree-compiler\n</code></pre> <p>Tip</p> <p><code>iree-compile</code> is installed to your python module installation path. If you pip install with the user mode, it is under <code>${HOME}/.local/bin</code>, or <code>%APPDATA%Python</code> on Windows. You may want to include the path in your system's <code>PATH</code> environment variable:</p> <pre><code>export PATH=${HOME}/.local/bin:${PATH}\n</code></pre>","tags":["GPU","Vulkan"]},{"location":"guides/deployment-configurations/gpu-vulkan/#build-the-compiler-from-source","title":"Build the compiler from source","text":"<p>Please make sure you have followed the Getting started page to build IREE for your host platform and the Android cross-compilation page if you are cross compiling for Android. The SPIR-V compiler backend is compiled in by default on all platforms.</p> <p>Ensure that the <code>IREE_TARGET_BACKEND_VULKAN_SPIRV</code> CMake option is <code>ON</code> when configuring for the host.</p> <p>Tip</p> <p><code>iree-compile</code> will be built under the <code>iree-build/tools/</code> directory. You may want to include this path in your system's <code>PATH</code> environment variable.</p>","tags":["GPU","Vulkan"]},{"location":"guides/deployment-configurations/gpu-vulkan/#get-the-iree-runtime","title":"Get the IREE runtime","text":"<p>Next you will need to get an IREE runtime that supports the Vulkan HAL driver.</p> <p>You can check for Vulkan support by looking for a matching driver and device:</p> <pre><code>$ iree-run-module --list_drivers\n\n        cuda: CUDA (dynamic)\n  local-sync: Local execution using a lightweight inline synchronous queue\n  local-task: Local execution using the IREE multithreading task system\n      vulkan: Vulkan 1.x (dynamic)\n</code></pre> <pre><code>$ iree-run-module --list_devices\n\n  cuda://GPU-00000000-1111-2222-3333-444444444444\n  local-sync://\n  local-task://\n  vulkan://00000000-1111-2222-3333-444444444444\n</code></pre>","tags":["GPU","Vulkan"]},{"location":"guides/deployment-configurations/gpu-vulkan/#build-the-runtime-from-source","title":"Build the runtime from source","text":"<p>Please make sure you have followed the Getting started page to build IREE for Linux/Windows and the Android cross-compilation page for Android. The Vulkan HAL driver is compiled in by default on non-Apple platforms.</p> <p>Ensure that the <code>IREE_HAL_DRIVER_VULKAN</code> CMake option is <code>ON</code> when configuring for the target.</p>","tags":["GPU","Vulkan"]},{"location":"guides/deployment-configurations/gpu-vulkan/#compile-and-run-a-program","title":"Compile and run a program","text":"<p>With the SPIR-V compiler and Vulkan runtime, we can now compile programs and run them on GPUs.</p>","tags":["GPU","Vulkan"]},{"location":"guides/deployment-configurations/gpu-vulkan/#compile-a-program","title":"Compile a program","text":"<p>The IREE compiler transforms a model into its final deployable format in many sequential steps. A model authored with Python in an ML framework should use the corresponding framework's import tool to convert into a format (i.e., MLIR) expected by the IREE compiler first.</p> <p>Using MobileNet v2 as an example, you can download the SavedModel with trained weights from TensorFlow Hub and convert it using IREE's TensorFlow importer. Then run the following command to compile with the <code>vulkan-spirv</code> target:</p> <pre><code>iree-compile \\\n--iree-hal-target-backends=vulkan-spirv \\\n--iree-vulkan-target-triple=&lt;...&gt; \\\nmobilenet_iree_input.mlir -o mobilenet_vulkan.vmfb\n</code></pre> <p>Note</p> <p>A target triple of the form <code>&lt;vendor/arch&gt;-&lt;product&gt;-&lt;os&gt;</code> is needed to compile towards each GPU architecture. If no triple is specified then a safe but more limited default will be used. We don't support the full spectrum here<sup>1</sup>; the following table summarizes the currently recognized ones:</p> GPU Vendor Target Triple ARM Mali GPU e.g., <code>valhall-g78-android30</code> Qualcomm Adreno GPU e.g., <code>adreno-unknown-android30</code> AMD GPU e.g., <code>rdna1-5700xt-linux</code> NVIDIA GPU e..g, <code>ampere-rtx3080-windows</code> SwiftShader CPU <code>cpu-swiftshader-unknown</code>","tags":["GPU","Vulkan"]},{"location":"guides/deployment-configurations/gpu-vulkan/#run-a-compiled-program","title":"Run a compiled program","text":"<p>In the build directory, run the following command:</p> <pre><code>tools/iree-run-module \\\n--device=vulkan \\\n--module=mobilenet_vulkan.vmfb \\\n--function=predict \\\n--input=\"1x224x224x3xf32=0\"\n</code></pre> <p>The above assumes the exported function in the model is named as <code>predict</code> and it expects one 224x224 RGB image. We are feeding in an image with all 0 values here for brevity, see <code>iree-run-module --help</code> for the format to specify concrete values.</p> <ol> <li> <p>It's also impossible to capture all details of a Vulkan implementation with a target triple, given the allowed variances on extensions, properties, limits, etc. So the target triple is just an approximation for usage.\u00a0\u21a9</p> </li> </ol>","tags":["GPU","Vulkan"]},{"location":"guides/ml-frameworks/","title":"ML frameworks","text":"<p>IREE supports popular machine learning frameworks using the same underlying technology.</p> <pre><code>graph LR\n  accTitle: ML framework to runtime deployment workflow overview\n  accDescr {\n    Programs start in some ML framework.\n    Programs are imported into MLIR.\n    The IREE compiler uses the imported MLIR.\n    Compiled programs are used by the runtime.\n  }\n\n  A[ML frameworks]\n  B[Imported MLIR]\n  C[IREE compiler]\n  D[Runtime deployment]\n\n  A --&gt; B\n  B --&gt; C\n  C --&gt; D</code></pre>"},{"location":"guides/ml-frameworks/#supported-frameworks","title":"Supported frameworks","text":"<p>See end-to-end examples of how to use each framework with IREE:</p> <ul> <li> TensorFlow and    TensorFlow Lite</li> <li> JAX</li> <li> PyTorch</li> </ul> <p>Importing from other frameworks is planned - stay tuned!</p>"},{"location":"guides/ml-frameworks/#samples","title":"Samples","text":"<p>Check out the samples in IREE's <code>samples/</code> directory, as well as the iree-samples repository.</p>"},{"location":"guides/ml-frameworks/#exportimport","title":"Export/Import","text":"<p>Each machine learning framework has some \"export\" mechanism that snapshots the structure and data in your program. These exported programs can then be \"imported\" into IREE's compiler by using either a stable import format or one of IREE's importer tools.</p> <p>This export/import process is specific to each frontend and typically involves a number of stages:</p> <ol> <li>Capture/trace/freeze the ML model into a graph</li> <li>Write that graph to an interchange format (e.g. SavedModel, TorchScript)</li> <li>Load the saved program into an import tool and convert to MLIR</li> <li>Legalize the graph's operations so only IREE-compatible operations remain</li> <li>Write the imported MLIR to a file</li> </ol> <p>This fully imported form can then be compiled indepedently of the source language and framework.</p>"},{"location":"guides/ml-frameworks/#compilation","title":"Compilation","text":"<p>IREE compiles MLIR files for specified sets of backends (CPU, GPU, etc). Each backend generates optimized native code custom to the input program and intended target platform. Once compiled, modules can be executed using IREE's runtime.</p> <p>See the deployment configuration guides for details on selecting a compiler backend and tuning options for your choice of target platform(s) or device(s).</p>"},{"location":"guides/ml-frameworks/#execution","title":"Execution","text":"<p>Compiled modules can be executed by selecting what compute devices to use, loading the module, and then executing it with the intended inputs. IREE provides several language bindings for its runtime API.</p>"},{"location":"guides/ml-frameworks/jax/","title":"JAX integration","text":"<p>Note</p> <p>IREE's JAX support is under active development. This page is still under construction.</p> <p>IREE offers two ways to interface with JAX programs:</p> <ul> <li>An API for extracting and compiling full models ahead of time (AOT) for   execution apart from JAX. This API is being developed in the   iree-org/iree-jax repository.</li> <li>A PJRT plugin that adapts IREE as a native JAX backend for online / just in   time (JIT) use. This plugin is being developed in the   openxla/openxla-pjrt-plugin repository.</li> </ul>","tags":["Python","JAX"]},{"location":"guides/ml-frameworks/pytorch/","title":"PyTorch integration","text":"<p>IREE supports compiling and running PyTorch programs represented as <code>nn.Module</code> classes as well as models defined using <code>functorch</code>.</p> <pre><code>graph LR\n  accTitle: PyTorch to runtime deployment workflow overview\n  accDescr {\n    Programs start as either PyTorch nn.Module or functorch programs.\n    Programs are imported into MLIR as either StableHLO, TOSA, or Linalg.\n    The IREE compiler uses the imported MLIR.\n    Compiled programs are used by the runtime.\n  }\n\n  subgraph A[PyTorch]\n    direction TB\n    A1[nn.Module]\n    A2[functorch]\n\n    A1 --- A2\n  end\n\n  subgraph B[MLIR]\n    direction TB\n    B1[StableHLO]\n    B2[TOSA]\n    B3[Linalg]\n\n    B1 --- B2\n    B2 --- B3\n  end\n\n  C[IREE compiler]\n  D[Runtime deployment]\n\n  A -- torch_mlir --&gt; B\n  B --&gt; C\n  C --&gt; D</code></pre>","tags":["Python","PyTorch"]},{"location":"guides/ml-frameworks/pytorch/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Install IREE packages, either by     building from source     or from pip:</p> Stable releases Nightly releases <p>Stable release packages are published to PyPI.</p> <pre><code>python -m pip install \\\niree-compiler \\\niree-runtime\n</code></pre> <p>Nightly releases are published on GitHub releases.</p> <pre><code>python -m pip install \\\n--find-links https://openxla.github.io/iree/pip-release-links.html \\\n--upgrade \\\niree-compiler \\\niree-runtime\n</code></pre> </li> <li> <p>Install <code>torch-mlir</code>, necessary for     compiling PyTorch models to a format IREE is able to execute:</p> <pre><code>pip install --pre torch-mlir \\\n-f https://llvm.github.io/torch-mlir/package-index/\n  --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n</code></pre> <p>The command will also install the right version of <code>torch</code> that has been tested with the version of <code>torch-mlir</code> being installed.</p> </li> </ol>","tags":["Python","PyTorch"]},{"location":"guides/ml-frameworks/pytorch/#running-a-model","title":"Running a model","text":"<p>Going from a loaded PyTorch model to one that's executing on IREE happens in four steps:</p> <ol> <li>Compile the model to MLIR</li> <li>Compile the MLIR to IREE VM flatbuffer</li> <li>Load the VM flatbuffer into IREE</li> <li>Execute the model via IREE</li> </ol> <p>Note</p> <p>In the following steps, we'll be borrowing the model from this BERT colab and assuming it is available as <code>model</code>.</p>","tags":["Python","PyTorch"]},{"location":"guides/ml-frameworks/pytorch/#compile-the-model-to-mlir","title":"Compile the model to MLIR","text":"<p>First, we need to trace and compile our model to MLIR:</p> <pre><code>model = # ... the model we're compiling\nexample_input = # ... an input to the model with the expected shape and dtype\nmlir = torch_mlir.compile(\n    model,\n    example_input,\n    output_type=\"linalg-on-tensors\",\n    use_tracing=True)\n</code></pre> <p>The full list of available output types can be found in the source code, and includes <code>linalg-on-tensors</code>, <code>stablehlo</code>, and <code>tosa</code>.</p>","tags":["Python","PyTorch"]},{"location":"guides/ml-frameworks/pytorch/#compile-the-mlir-to-an-iree-vm-flatbuffer","title":"Compile the MLIR to an IREE VM flatbuffer","text":"<p>Next, we compile the resulting MLIR to IREE's deployable file format:</p> <pre><code>iree_backend = \"llvm-cpu\"\niree_input_type = \"tm_tensor\"\nbytecode_stream = io.BytesIO()\nmlir.operation.write_bytecode(bytecode_stream)\niree_vmfb = ireec.compile_str(bytecode_stream.getvalue(),\n                              target_backends=[iree_backend],\n                              input_type=iree_input_type)\n</code></pre> <p>Here we have a choice of backend we want to target. See the deployment configuration guides for more details about specific targets and configurations.</p> <p>The generated flatbuffer can now be serialized and stored for another time or loaded and executed immediately.</p> <p>Note</p> <p>The input type <code>tm_tensor</code> corresponds to the <code>linalg-on-tensors</code> output type of <code>torch-mlir</code>.</p> <p>Note</p> <p>The conversion to bytecode before passing the module to IREE is needed to cross the border from the Torch-MLIR MLIR C API to the IREE MLIR C API.</p>","tags":["Python","PyTorch"]},{"location":"guides/ml-frameworks/pytorch/#load-the-vm-flatbuffer-into-iree","title":"Load the VM flatbuffer into IREE","text":"<p>Next, we load the flatbuffer into the IREE runtime:</p> <pre><code>config = ireert.Config(driver_name=\"local-sync\")\nctx = ireert.SystemContext(config=config)\nvm_module = ireert.VmModule.from_flatbuffer(ctx.instance, flatbuffer)\nctx.add_vm_module(vm_module)\ninvoker = ctx.modules.module\n</code></pre>","tags":["Python","PyTorch"]},{"location":"guides/ml-frameworks/pytorch/#execute-the-model-via-iree","title":"Execute the model via IREE","text":"<p>Finally, we can execute the loaded model:</p> <pre><code>result = invoker.forward(example_input.numpy())\nnumpy_result = np.asarray(result)\n</code></pre>","tags":["Python","PyTorch"]},{"location":"guides/ml-frameworks/pytorch/#training","title":"Training","text":"<p>Training with PyTorch in IREE is supported via <code>functorch</code>. The steps for loading the model into IREE, once defined, are nearly identical to the above example.</p> <p>You can find a full end-to-end example of defining a basic regression model, training with it, and running inference on it here.</p>","tags":["Python","PyTorch"]},{"location":"guides/ml-frameworks/pytorch/#native-on-device-training","title":"Native / On-device Training","text":"<p>A small (~100-250KB), self-contained binary can be built for deploying to resource-constrained environments. An example illustrating this can be found in this example. This binary runs a model without a Python interpreter.</p>","tags":["Python","PyTorch"]},{"location":"guides/ml-frameworks/pytorch/#samples","title":"Samples","text":"Colab notebooks Inference on BERT Example scripts Basic Inference and Training Example Native On-device Training Example","tags":["Python","PyTorch"]},{"location":"guides/ml-frameworks/tensorflow/","title":"TensorFlow integration","text":"<p>IREE supports compiling and running TensorFlow programs represented as <code>tf.Module</code> classes or stored in the <code>SavedModel</code> format.</p> <pre><code>graph LR\n  accTitle: TensorFlow to runtime deployment workflow overview\n  accDescr {\n    Programs start as either TensorFlow SavedModel or tf.Module programs.\n    Programs are imported into MLIR as StableHLO.\n    The IREE compiler uses the imported MLIR.\n    Compiled programs are used by the runtime.\n  }\n\n  subgraph A[TensorFlow]\n    direction TB\n    A1[SavedModel]\n    A2[tf.Module]\n\n    A1 --- A2\n  end\n\n  subgraph B[MLIR]\n    B1[StableHLO]\n  end\n\n  C[IREE compiler]\n  D[Runtime deployment]\n\n  A -- iree-import-tf --&gt; B\n  B --&gt; C\n  C --&gt; D</code></pre>","tags":["Python","TensorFlow"]},{"location":"guides/ml-frameworks/tensorflow/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Install TensorFlow by following the     official documentation:</p> <pre><code>python -m pip install tf-nightly\n</code></pre> </li> <li> <p>Install IREE packages, either by     building from source     or from pip:</p> Stable releases Nightly releases <p>Stable release packages are published to PyPI.</p> <pre><code>python -m pip install \\\niree-compiler \\\niree-runtime \\\niree-tools-tf\n</code></pre> <p>Nightly releases are published on GitHub releases.</p> <pre><code>python -m pip install \\\n--find-links https://openxla.github.io/iree/pip-release-links.html \\\n--upgrade \\\niree-compiler \\\niree-runtime \\\niree-tools-tf\n</code></pre> </li> </ol>","tags":["Python","TensorFlow"]},{"location":"guides/ml-frameworks/tensorflow/#importing-models","title":"Importing models","text":"<p>IREE compilers transform a model into its final deployable format in several sequential steps. The first step for a TensorFlow model is to use either the <code>iree-import-tf</code> command-line tool or IREE's Python APIs to import the model into a format (i.e., MLIR) compatible with the generic IREE compilers.</p>","tags":["Python","TensorFlow"]},{"location":"guides/ml-frameworks/tensorflow/#from-savedmodel-on-tensorflow-hub","title":"From SavedModel on TensorFlow Hub","text":"<p>IREE supports importing and using SavedModels from TensorFlow Hub.</p>","tags":["Python","TensorFlow"]},{"location":"guides/ml-frameworks/tensorflow/#using-the-command-line-tool","title":"Using the command-line tool","text":"<p>First download the SavedModel and load it to get the serving signature, which is used as the entry point for IREE compilation flow:</p> <pre><code>import tensorflow.compat.v2 as tf\nloaded_model = tf.saved_model.load('/path/to/downloaded/model/')\nprint(list(loaded_model.signatures.keys()))\n</code></pre> <p>Note</p> <p>If there are no serving signatures in the original SavedModel, you may add them by yourself by following \"Missing serving signature in SavedModel\".</p> <p>Then you can import the model with <code>iree-import-tf</code>. You can read the options supported via <code>iree-import-tf -help</code>. Using MobileNet v2 as an example and assuming the serving signature is <code>predict</code>:</p> <pre><code>iree-import-tf\n  --tf-import-type=savedmodel_v1 \\\n--tf-savedmodel-exported-names=predict \\\n/path/to/savedmodel -o iree_input.mlir\n</code></pre> <p>Tip</p> <p><code>iree-import-tf</code> is installed as <code>/path/to/python/site-packages/iree/tools/tf/iree-import-tf</code>. You can find out the full path to the <code>site-packages</code> directory via the <code>python -m site</code> command.</p> <p>Tip</p> <p><code>-tf-import-type</code> needs to match the SavedModel version. You can try both v1 and v2 if you see one of them gives an empty dump.</p> <p>Next, you can compile the model in <code>iree_input.mlir</code> for one of IREE's supported targets by following one of the deployment configuration guides.</p>","tags":["Python","TensorFlow"]},{"location":"guides/ml-frameworks/tensorflow/#samples","title":"Samples","text":"Colab notebooks Training an MNIST digits classifier Edge detection Pretrained ResNet50 inference TensorFlow Hub import <p>End-to-end execution tests can be found in IREE's integrations/tensorflow/e2e/ directory.</p>","tags":["Python","TensorFlow"]},{"location":"guides/ml-frameworks/tensorflow/#troubleshooting","title":"Troubleshooting","text":"","tags":["Python","TensorFlow"]},{"location":"guides/ml-frameworks/tensorflow/#missing-serving-signature-in-savedmodel","title":"Missing serving signature in SavedModel","text":"<p>Sometimes SavedModels are exported without explicit serving signatures. This happens by default for TensorFlow Hub SavedModels. However, serving signatures are required as entry points for IREE compilation flow. You can use Python to load and re-export the SavedModel to give it serving signatures. For example, for MobileNet v2, assuming we want the serving signature to be <code>predict</code> and operating on a 224x224 RGB image:</p> <pre><code>import tensorflow.compat.v2 as tf\nloaded_model = tf.saved_model.load('/path/to/downloaded/model/')\ncall = loaded_model.__call__.get_concrete_function(\n         tf.TensorSpec([1, 224, 224, 3], tf.float32))\nsignatures = {'predict': call}\ntf.saved_model.save(loaded_model,\n  '/path/to/resaved/model/', signatures=signatures)\n</code></pre> <p>The above will create a new SavedModel with a serving signature, <code>predict</code>, and save it to <code>/path/to/resaved/model/</code>.</p>","tags":["Python","TensorFlow"]},{"location":"guides/ml-frameworks/tflite/","title":"TensorFlow Lite integration","text":"<p>IREE supports compiling and running TensorFlow Lite (TFLite) programs stored as TFLite FlatBuffers. These files can be imported into an IREE-compatible format then compiled to a series of backends.</p> <pre><code>graph LR\n  accTitle: TFLite to runtime deployment workflow overview\n  accDescr {\n    Programs start as TensorFlow Lite FlatBuffers.\n    Programs are imported into MLIR's TOSA dialect using iree-import-tflite.\n    The IREE compiler uses the imported MLIR.\n    Compiled programs are used by the runtime.\n  }\n\n  subgraph A[TFLite]\n    A1[FlatBuffer]\n  end\n\n  subgraph B[MLIR]\n    B1[TOSA]\n  end\n\n  C[IREE compiler]\n  D[Runtime deployment]\n\n  A -- iree-import-tflite --&gt; B\n  B --&gt; C\n  C --&gt; D</code></pre>","tags":["Python","TensorFlow"]},{"location":"guides/ml-frameworks/tflite/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Install TensorFlow by following the     official documentation:</p> <pre><code>python -m pip install tf-nightly\n</code></pre> </li> <li> <p>Install IREE packages, either by     building from source     or from pip:</p> Stable releases Nightly releases <p>Stable release packages are published to PyPI.</p> <pre><code>python -m pip install \\\niree-compiler \\\niree-runtime \\\niree-tools-tflite\n</code></pre> <p>Nightly releases are published on GitHub releases.</p> <pre><code>python -m pip install \\\n--find-links https://openxla.github.io/iree/pip-release-links.html \\\n--upgrade \\\niree-compiler \\\niree-runtime \\\niree-tools-tflite\n</code></pre> </li> </ol>","tags":["Python","TensorFlow"]},{"location":"guides/ml-frameworks/tflite/#importing-and-compiling","title":"Importing and Compiling","text":"<p>IREE's tooling is divided into two components: import and compilation.</p> <ol> <li>The import tool converts the TFLite FlatBuffer to an IREE compatible form,   validating that only IREE compatible operations remain. Containing a combination   of TOSA and IREE operations.</li> <li>The compilation stage generates the bytecode module for a list of targets,   which can be executed by IREE.</li> </ol>","tags":["Python","TensorFlow"]},{"location":"guides/ml-frameworks/tflite/#using-command-line-tools","title":"Using Command Line Tools","text":"<p>These two stages can be completed entirely via the command line.</p> <pre><code>WORKDIR=\"/tmp/workdir\"\nTFLITE_URL=\"https://storage.googleapis.com/iree-model-artifacts/tflite-integration-tests/posenet_i8.tflite\"\nTFLITE_PATH=${WORKDIR}/model.tflite\nIMPORT_PATH=${WORKDIR}/tosa.mlir\nMODULE_PATH=${WORKDIR}/module.vmfb\n\n# Fetch the sample model\nwget ${TFLITE_URL} -O ${TFLITE_PATH}\n\n# Import the sample model to an IREE compatible form\niree-import-tflite ${TFLITE_PATH} -o ${IMPORT_PATH}\n\n# Compile for the CPU backend\niree-compile \\\n--iree-input-type=tosa \\\n--iree-hal-target-backends=llvm-cpu \\\n${IMPORT_PATH} \\\n-o ${MODULE_PATH}\n</code></pre>","tags":["Python","TensorFlow"]},{"location":"guides/ml-frameworks/tflite/#using-the-python-api","title":"Using the Python API","text":"<p>The example below demonstrates downloading, compiling, and executing a TFLite model using the Python API. This includes some initial setup to declare global variables, download the sample module, and download the sample inputs.</p> <p>Declaration of absolute paths for the sample repo and import all required libraries. The default setup uses the CPU backend as the only target. This can be reconfigured to select alternative targets.</p> <pre><code>import iree.compiler.tflite as iree_tflite_compile\nimport iree.runtime as iree_rt\nimport numpy\nimport os\nimport urllib.request\n\nfrom PIL import Image\n\nworkdir = \"/tmp/workdir\"\nos.makedirs(workdir, exist_ok=True)\n\ntfliteFile = \"/\".join([workdir, \"model.tflite\"])\njpgFile = \"/\".join([workdir, \"input.jpg\"])\ntfliteIR = \"/\".join([workdir, \"tflite.mlir\"])\ntosaIR = \"/\".join([workdir, \"tosa.mlir\"])\nbytecodeModule = \"/\".join([workdir, \"iree.vmfb\"])\n\nbackends = [\"llvm-cpu\"]\nconfig = \"local-task\"\n</code></pre> <p>The TFLite sample model and input are downloaded locally.</p> <pre><code>tfliteUrl = \"https://storage.googleapis.com/iree-model-artifacts/tflite-integration-tests/posenet_i8.tflite\"\njpgUrl = \"https://storage.googleapis.com/iree-model-artifacts/tflite-integration-tests/posenet_i8_input.jpg\"\n\nurllib.request.urlretrieve(tfliteUrl, tfliteFile)\nurllib.request.urlretrieve(jpgUrl, jpgFile)\n</code></pre> <p>Once downloaded we can compile the model for the selected backends. Both the TFLite and TOSA representations of the model are saved for debugging purposes. This is optional and can be omitted.</p> <pre><code>iree_tflite_compile.compile_file(\n  tfliteFile,\n  input_type=\"tosa\",\n  output_file=bytecodeModule,\n  save_temp_tfl_input=tfliteIR,\n  save_temp_iree_input=tosaIR,\n  target_backends=backends,\n  import_only=False)\n</code></pre> <p>After compilation is completed we configure the VmModule using the local-task configuration and compiled IREE module.</p> <pre><code>config = iree_rt.Config(\"local-task\")\ncontext = iree_rt.SystemContext(config=config)\nwith open(bytecodeModule, 'rb') as f:\n  vm_module = iree_rt.VmModule.from_flatbuffer(config.vm_instance, f.read())\n  context.add_vm_module(vm_module)\n</code></pre> <p>Finally, the IREE module is loaded and ready for execution. Here we load the sample image, manipulate to the expected input size, and execute the module. By default TFLite models include a single function named 'main'. The final results are printed.</p> <pre><code>im = numpy.array(Image.open(jpgFile).resize((192, 192))).reshape((1, 192, 192, 3))\nargs = [im]\n\ninvoke = context.modules.module[\"main\"]\niree_results = invoke(*args)\nprint(iree_results)\n</code></pre>","tags":["Python","TensorFlow"]},{"location":"guides/ml-frameworks/tflite/#samples","title":"Samples","text":"<ul> <li> <p>The tflitehub folder in the iree-samples repository contains test scripts to compile, run, and compare various TensorFlow Lite models sourced from TensorFlow Hub.</p> </li> <li> <p>An example smoke test of the TensorFlow Lite C API is available here.</p> </li> </ul> Colab notebooks Text classification with TFLite and IREE","tags":["Python","TensorFlow"]},{"location":"guides/ml-frameworks/tflite/#troubleshooting","title":"Troubleshooting","text":"<p>Failures during the import step usually indicate a failure to lower from TensorFlow Lite's operations to TOSA, the intermediate representation used by IREE. Many TensorFlow Lite operations are not fully supported, particularly those than use dynamic shapes. Please reach out on one of IREE's communication channels if you notice something missing.</p>","tags":["Python","TensorFlow"]},{"location":"reference/","title":"Reference pages","text":""},{"location":"reference/#api-bindings","title":"API bindings","text":"<p>IREE offers API bindings for compiling and running programs from various languages.</p> <ul> <li>Index page</li> </ul>"},{"location":"reference/#mlir-dialects","title":"MLIR dialects","text":"<p>Automatically generated documentation for the MLIR dialects defined in the IREE repository.</p> <ul> <li>Index page</li> </ul>"},{"location":"reference/#other-topics","title":"Other topics","text":"<ul> <li>Glossary</li> <li>Optimization options</li> <li>Extensions</li> </ul>"},{"location":"reference/extensions/","title":"Extension mechanisms","text":"<p>Note</p> <p>Much of this describes provisions for extension within IREE but until the core of the system has settled little work will be done to fully flesh-out and document them in detail. A large majority of things that would make someone want to extend IREE can instead be accomplished much easier and performantly using native MLIR dialects that are then processed by the IREE compiler.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#guidelines","title":"Guidelines","text":"<p>IREE has a compiler and runtime separation, a multi-layered architecture, and split between execution of \"host code\" that schedules compute-heavy work and SPMD \"device code\" that performs the bulk of compute operations. Each axis has a different set of extension mechanisms that can be used independently or combined.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#extension-philosophy","title":"Extension philosophy","text":"<p>Organized below are some of the mechanisms IREE provides for extending the core compiler and runtime and when they should(n't) be used. The goal of these progressively lower-level extension mechanisms is to make it easier for users to fall into the pit of success:</p> <p>Quote</p> <p>\"a well-designed system makes it easy to do the right things and annoying (but not impossible) to do the wrong things.\" - Jeff Atwood</p> <p>The amount of engineering complexity for initial bring-up and maintenance increases with each subsequently lower-level approach and it is best to start from the top and exit as fast as possible: this is a choose-your-own-adventure where you're trying to escape the dungeon with both the loot and your limbs . Avoid the temptation of immediately dropping down to making external C calls at runtime because that's how it's been done before as it's easier, more robust, and more performant to use the system as it is intended to be used.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#when-to-extend","title":"When to extend","text":"<p>The primary goal when extending any framework should first be to avoid extending it at all. There is no mechanism that is free - whether in terms of engineering effort to develop and maintain over time, include in compiler deployments, or include in runtime deployments. As a system scales in deployment configurations the available mechanisms for extension increase but so too does the chaos introduced by extensions that do not also scale with that design. Users are the only ones who can determine the tradeoffs they are willing to accept: for example, the mechanism to extend device code with a custom runtime call to a C function does not work on GPUs and gets significantly more complicated on CPUs as sandboxes/enclaves are used - but if the user scenario is for local process CPU-only execution that may not matter.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#where-to-extend-inputscompilerruntime","title":"Where to extend (inputs/compiler/runtime)","text":"<p>Consider in normal software development when one would choose to write more code (possibly packaging it into a reusable library) vs. changing the programming language or compiler they are using to compile their code vs. changing the operating systems their code runs on. The further one gets from the problem they are trying to solve the more work, coordination, and maintenance is involved and though there are reasons to make changes across the stack they should be done only when a simpler solution would not suffice.</p> <p>An author will retain more control over their logic the closer they sit to the inputs to the compiler. IREE provides several mechanisms that try to keep control with the author and robust to changes in IREE or MLIR internals and it is strongly encouraged that those looking to extend take those routes first. Contributions that help everyone are very welcome but do have a higher cost and it's often much easier to design and justify upstream changes with working examples in forks or at higher levels of the stack.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#where-to-extend-hostdevice","title":"Where to extend (host/device)","text":"<p>From a performance perspective the rule is to colocate code with the data it is acting on: tensor data, for example, should almost exclusively be manipulated by device code as tensors live on device. Attempting to use tensor data with host code will result in synchronization points and host/device transfers that can decimate performance. This can lead to seemingly paradoxical situations where swapping out compiler-generated code for a human-authored \"fast path\" can be slower than even the most naive compiler results. An important thing to keep in mind with compilers is that it is exceedingly difficult to produce code by hand that is consistently more performant across a broad range of deployments and the first temptation should always be to improve the compiler - extending it via other mechanisms when not required by the task is often just premature optimization.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#1-target-iree-input-dialects","title":"1. Target IREE input dialects","text":"<p>TL;DR</p> <p>Convert your custom ops into standard MLIR dialects.</p> <pre><code>+------------+      +--------+      +---------------+\n| Your input | -+-&gt; |  iree  | -+-&gt; | IREE compiler |\n+------------+  |   +--------+  |   +---------------+\n                |   +--------+  |\n                +-&gt; | linalg | -+\n                |   +--------+  |\n                |      ....     |\n</code></pre> <p>The easiest, cleanest, and most robust path to extend IREE is to make use of what MLIR is designed for: composing dialects and converting between them. IREE supports several input dialects such as <code>tosa</code>, <code>mhlo</code>, <code>linalg</code>, and the standard <code>arith</code>, <code>math</code>, <code>tensor</code>, and <code>scf</code> dialects. Any source IR that can be turned into that mix of dialects (directly or transitively) will work with the whole IREE pipeline for all deployment configurations and targets. If possible to express the computation in this form it will always be the best route to getting small deployments without the need to modify or include any additional code at runtime and run on all device types and execution modes.</p> <p>This mechanism can also be layered with any of the subsequent lower-level ones: if some part of the operation runs on the host and some part on device then decomposing it such that it contains as many standard ops for flow control as possible and linear algebra/custom ops for the dense math will reduce the engineering effort required on both sides and lead to an easier to maintain solution even if lower-level extension is required.</p> <p>A large majority of classic ML \"custom ops\" can be accomplished with this approach. When bringing up projects built on IREE it's best to concisely describe the operation in more elemental mathematical representations and then add optimizations where required knowing that things will still work even if those optimizations never happen.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#pros","title":"Pros","text":"<ul> <li>No IREE compiler or runtime code changes required.<ul> <li>Can use standard IREE packaged releases and tools.</li> <li>No versioning issues at runtime.</li> </ul> </li> <li>IREE's host/device partitioning can partition your code.</li> <li>Fusion and other compiler techniques (CSE/DCE/inlining/etc) work on your code.</li> <li>All target backends (CPU/GPU/accelerators/enclaves/etc) work.</li> </ul>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#cons","title":"Cons","text":"<ul> <li>Input dialects cannot natively represent all possible programs (such as file   IO and other syscalls).</li> <li>Performance-sensitive host code (b-trees and other in-memory databases) will   run through the slower VM paths if not authored as dense compute.</li> </ul>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#when-to-use","title":"When to use","text":"<ul> <li> Targeting multiple MLIR toolchains of which IREE is just   one (as little to no IREE-specific code is required).</li> <li> Operation represents host code in addition to device code.</li> <li> All code is known statically or symbolically at   compile-time (instead of independently versioned libraries at runtime).</li> <li> Complex high-performance code not representable as linear algebra.</li> <li> External runtime interactions (file/network/user IO). Use   custom modules.</li> </ul>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#implementation","title":"Implementation","text":"<p>To make use of this approach one just needs to follow the standard MLIR dialect conversion behavior: add a dialect with ops, add a conversion pass, and run that pass before providing the resulting IR to the IREE compiler. See Creating a Dialect.</p> <p>Think of this like authoring C++ sources with templates that you compile into your application: Clang (and LLVM beyond) don't know about your library details and instead just process it as it would any other code. You can take the same source and pass it to GCC and it'll be robust to underlying changes in the system.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#2-extend-host-code-with-custom-modules","title":"2. Extend host code with custom modules","text":"<p>TL;DR</p> <p>Import MLIR functions in the compiler and custom modules at runtime.</p> <pre><code>// Main user module compiled by IREE:\nmodule @model {\n  // Declare a synchronous external function:\n  func.func private @my_custom_module.sync_func(%input: tensor&lt;?xf32&gt;) -&gt; i32\n  // Declare an asynchronous external function:\n  func.func private @my_custom_module.async_func(%input: tensor&lt;?xf32&gt;) -&gt; tensor&lt;?xf32&gt; attributes {\n    iree.abi.model = \"coarse-fences\",\n    nosideeffects\n  }\n  func.func @predict() {\n    ...\n    // Call a synchronous/blocking external function:\n    %sync_result = call @my_custom_module.sync_func(%sync_input) : (tensor&lt;?xf32&gt;) -&gt; i32\n    ...\n    ...\n    // Call an asynchronous/non-blocking external function:\n    %async_result = call @my_custom_module.async_func(%async_input) : (tensor&lt;?xf32&gt;) -&gt; tensor&lt;?xf32&gt;\n    ...\n  }\n}\n</code></pre> <p>IREE provides dynamic linking at runtime via its VM interfaces. For code that runs on the host and requires syscalls or calling out to existing libraries - such as file IO, text processing, and JPEG decoding - this is an easy way to interop without paying attention to the more complex details of device code. An IREE module compiled using custom modules is portable and dynamically deployable so long as the custom module is registered at runtime.</p> <p>This approach conceptually matches what normal native binaries do in an OS: imports are declared and at runtime they are resolved based on the available exports of modules in the system. Just as with normal systems engineering design of the API between modules is up to the user and depending on rigor can have several pitfalls but these problems and their solutions are not IREE specific and anyone who has designed a shared library interface can apply the same rules here in IREE around versioning, performance, etc. One does not add 2 integers via a syscall and the same holds here: custom modules and the functions within should perform a large amount of work to hide overheads involved in the cross-module calls and users must be aware that the compiler cannot optimize across the call boundaries.</p> <p>See the synchronous tensor I/O and asynchronous tensor I/O samples.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#pros_1","title":"Pros","text":"<ul> <li>No IREE compiler code changes required.</li> <li>Produced artifacts are portable across IREE deployment configurations.</li> <li>Full system access is allowed - the VM just calls external functions.</li> <li>Runtime modules can be implemented (via shims) in other languages/runtimes.</li> </ul>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#cons_1","title":"Cons","text":"<ul> <li>Custom modules must be registered at runtime by the user.</li> <li>The VM custom module ABI goo must be authored by the user (such as with JNI or   pybind to move between java/python and C).</li> <li>All custom module code must be compiled and deployed regardless of how much   any modules use. The granularity of modules and their versioning is up to the   user.</li> <li>Custom module code cannot be optimized by the IREE compiler to avoid   host/device readbacks and unnecessary data type conversion.</li> </ul>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#when-to-use_1","title":"When to use","text":"<ul> <li> Interactions with large libraries or system calls.</li> <li> Performance-sensitive host code that cannot easily be   represented as device code (like UTF-8 string transformation using libicu).</li> <li> Extensively using tensor resources.</li> </ul>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#implementation_1","title":"Implementation","text":"<p>The runtime portion requires that the code be exported to the VM system by way of an <code>iree_vm_module_t</code> interface. A low-level native interface exists with minimal overhead and is used for example by the IREE HAL itself. There is also a C++ wrapper that is significantly easier to work with however it needs some performance improvements.</p> <p>Full end-to-end examples can be found under <code>samples/custom_modules/</code>:</p> <ul> <li>The basic sample shows how to add VM modules with custom types and take advantage of ABI features like fallback functions and optional imports.</li> <li>The synchronous tensor I/O sample shows a call taking and returning a tensor and performing blocking work.</li> <li>The asynchronous tensor I/O sample shows the same thing but with fences for asynchronous scheduling.</li> </ul>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#3-extend-target-specific-device-conversion-patterns","title":"3. Extend target-specific device conversion patterns","text":"<p>TL;DR</p> <p>Add patterns to <code>iree/Compiler/Codegen/</code> to emit target code.</p> <p>The easiest and most robust path for specializations of device code is to emit such code mixed with the IREE compiler generated code at the highest possible level of abstraction within the target pipeline. For example, if the code can be represented with the <code>vector</code> dialect then inserting conversion patterns between <code>linalg</code> and <code>vector</code> enables the emitted code to be specialized further based on user configuration and optimized with the full set of available passes that run in the pipeline. For each level lower one goes the more flexibility they gain such as being able to emit inline assembly blocks that do anything while trading off generality and multi-targeting applicability.</p> <p>How much the tradeoff matters is based on the behavior of the extension. If a pattern changing a transcendental function to an approximation can operate at the vector level then all IREE deployment targets can benefit from the pattern and as new targets are made available they will automatically receive the benefits. In contrast, a pattern at the vector level that turns generic vector operations into architecture-specific LLVM intrinsics by its nature only pertains to a single target family and can be done at a lower level. As a rule of thumb if a particular pattern is going to need ~N implementations for ~N targets that are all mostly the same it's better to try to move that higher in the stack.</p> <p>At this point the complexity of extending things is still fairly constrained: a C++ pass or pattern is verified with normal lit tests and can be upstreamed easily either into MLIR or IREE (a large number of IREE patterns are upstreamed, benefiting all users of MLIR). Cross-compilation and versioning are not a factor and the IREE artifacts can be considered durable at a coarse level (outside of major target architectural changes).</p> <p>Note that depending on the target there are various mechanisms for representing code in MLIR, up to including inline assembly snippets in IR via <code>llvm.inline_asm</code>.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#pros_2","title":"Pros","text":"<ul> <li>Not limited to what is possible to represent in any particular MLIR dialect.</li> <li>Rich target configuration available; multiple passes can contribute info.</li> <li>Produced executable binaries are hermetic and no runtime changes are required.</li> <li>Specialization can happen in MLIR dialects like <code>linalg</code> or <code>vector</code> as well   as target-specific representations like SPIR-V and LLVM IR.</li> <li>The compiler can perform deep optimizations across both the generated code and   the provided code (hoisting/loop invariant code motion/cse/etc).</li> </ul>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#cons_2","title":"Cons","text":"<ul> <li>Requires implementing the patterns as code in the IREE compiler or via TBD   interfaces.</li> </ul>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#when-to-use_2","title":"When to use","text":"<ul> <li> Code that must be emitted during target lowering - such as   something optimizing for a particular CPU architecture.</li> <li> Hot code mixed with generated code at a fine granularity   (within the innermost loop).</li> <li> External existing hand-authored libraries. Either statically   or dynamically link instead.</li> </ul>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#implementation_2","title":"Implementation","text":"<p>There are several ways to author patterns and passes in MLIR. As examples:</p> <ul> <li>A majority of patterns are authored in C++ using PatternRewriter.</li> <li>PDL is an MLIR-based way to   express rewrite operations with strong typing, compile-time verification, and   easily-readable and less-verbose IR.</li> <li><code>linalg</code> uses a python-based DSL   for defining some of its extended ops.</li> </ul> <p>There are many examples within both MLIR and IREE, one specifically being the polynomial approximation expansion patterns.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#4-include-external-target-specific-device-code","title":"4. Include external target-specific device code","text":"<p>TL;DR</p> <p>Statically link external object files into IREE executables.</p> <p>For large bodies of existing device code or library calls that are available for static linkage the work involved to reimplement them at higher levels of the stack can be cost prohibitive even if it leads to better results. In these cases just as with a normal toolchain one would just want to declare an external function, call it, and add the object file to the linker command line. In IREE the same can be performed by way of taking compatible bitcode or native object files and linking them in with the generated code. An MLIR pattern would declare and emit the call and the target-specific IREE linker would pull in the objects.</p> <p>As the linking behavior varies per target (for example, some targets like SPIR-V don't have traditional linkers) how this is performed is up to the IREE target backends. The complexity involved in producing the object files to link will also vary per-backend and the complexity of the deployment: cross-compiling for multiple architectures or compilation modes (ASAN, etc) will require unique copies of the object files matching that precise configuration.</p> <p>At this point generality is largely out as is the ability to cleanly upstream such files. It should be apparent how a few dozen lines of C++ or PDL that avoids the need for any of this complexity is more appealing. In extremely specific cases of a single platform/architecture/version for a single program deployed via a specific artifact composition it's not so bad but IREE is designed such that extreme specificity is an optional mode of the more general solution. This does not mean this mechanism is not useful in some situations and only that it should be a last-resort when one of the easier to manage solutions is not viable - not a shortcut to avoid writing some C++ patterns.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#pros_3","title":"Pros","text":"<ul> <li>Works with hand-authored code in compatible object files from any toolchain.</li> <li>No IREE runtime changes required.<ul> <li>All deployment modes still work, including multi-targeting.</li> <li>No versioning concerns as custom code is included in artifacts.</li> </ul> </li> </ul>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#cons_3","title":"Cons","text":"<ul> <li>Users must provide per-target precompiled object files on disk.</li> <li>IREE compiler changes are still needed for generating the external calls.</li> <li>Though LTO may be able to optimize across the calls it is not guaranteed.</li> </ul>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#when-to-use_3","title":"When to use","text":"<ul> <li> Existing math libraries or architecture-specific functions   that cannot be ported into a more MLIR-friendly form.</li> <li> Mixing in hand-authored code written in C/rust/etc with   generated code from MLIR.</li> <li> External code can be represented as either <code>linalg</code>,   <code>vector</code>, or LLVM IR. Use target-specific conversion patterns instead.</li> <li> External code size is large and unlikely to benefit from   link-time optimizations (such as something like libjpeg). Dynamically link   instead.</li> </ul>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#implementation_3","title":"Implementation","text":"<p>As the linking behavior varies per target backend there is no general solution at this level: if targeting the CPU then the system native linker or lld need to be provided the object files, while SPIR-V will need to merge the SPIR-V binaries directly, and Metal shader libraries will need to be constructed with the Apple-specific <code>metallib</code> tooling. Producing these files and performing the linking is outside the scope of IREE.</p> <p>If the files can be acquired then compiler changes will be required to emit calls to them and invoke the linker with the the files.</p> <p>On the CPU an alternative is to use the static library output mode where IREE produces an object file and then the user invokes the linker themselves; this still requires the compiler changes to emit the calls but avoids needing to teach the compiler how to link the files.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#5-dynamically-link-target-specific-device-code-cpu-only","title":"5. Dynamically link target-specific device code (CPU only)","text":"<p>TL;DR</p> <p>Dynamically link external C functions at runtime from device code.</p> <p>It is pitch black. You are likely to be eaten by a grue.</p> <p>This is the lowest-level integration in the system and is designed to act as an escape hatch and - as with any emergency escape hatch - it's not designed for ergonomics. Users should try first to come in through the door and attempting to use this mechanism should trigger alarms about the approach being attempted.</p> <p>IREE's execution model for device code and native machine binary deployment mechanisms are designed with several constraints in order to make all of the above approaches possible and performant. Calling arbitrary C functions from deep within the system can introduce subtle (and not-so-subtle) bugs that are extremely difficult to track down and versioning between the compiler emitting the calls and the runtime providing the implementations can cause skew unless held carefully. Consider the methods added here like syscalls in that they must be extremely focused and if they are ever likely to change (including being removed) then care will be needed just as with versioning or redirecting a syscall. Designing good stable interfaces is hard and a classic pit of failure.</p> <p>Some things to note:</p> <ul> <li>Device code executes in a tiled fashion and single dispatches may invoke the   same function many times from many threads concurrently to perform   the larger work.</li> <li>Tiles may execute in any order and on any thread; performing fine-grained   locking within the tile can lead to deadlocks.</li> <li>Device code is stateless in order to allow for access restrictions and caching   across multiple loaded models - any library state required must be externally   managed via process globals.</li> <li>Device code may be running out-of-process (sandbox/enclave) and the library   functions must be available where the dispatches run and not where they are   launched (such as being linked into the sandbox binary, if separate from the   main process binary).</li> <li>The stack must be used to pass arguments/results to external calls via a   single pointer and there is no libffi-like functionality for magically calling   arbitrary C functions. Users must provide the shims they need.</li> <li>Thread-local storage is unavailable in the called code (it may be usable, but   it is not guaranteed it'll work on all platforms and leaks are likely).</li> <li>No heap allocator is provided and the use of libc malloc is unsupported.</li> </ul> <p>Most of the constraints here come from the SPMD parallelism model, platform-agnostic deployment format, and overall data-oriented design of IREE. Code operating in this fashion has a certain shape and that is usually not the same as big legacy single-threaded CPU-focused BLAS libraries that perform their own caching, internal thread and state management, and other shenanigans. IREE is not designed to wrap such things and if any of these notes are issues it is more an indicator that the approach needs adjustment than anything else. Trying to bypass or workaround the constraints is possible - after all IREE is an open source project and any user is welcome to fork it - but unsupported by the core IREE team.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#pros_4","title":"Pros","text":"<ul> <li>Function resolution at runtime is orthogonal to compiler target specification.</li> <li>Machine code can be shared between the application and IREE artifacts.</li> </ul>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#cons_4","title":"Cons","text":"<ul> <li>IREE compiler and runtime must both be modified.</li> <li>Deeper integration with the IREE codegen compiler infrastructure required.</li> <li>ABI versioning complexity between compiler and runtime.</li> <li>Runtimes must ship the imports for the lifetime of any artifact compiled to   use them.<ul> <li>Humans are bad at predicting the future.</li> <li>Using the same artifact in different binaries at runtime requires changes   to each binary - including those that may not be owned by the person   producing the artifact.</li> <li>Weak imports and conditional usage can help but still leads to bloat.</li> </ul> </li> </ul>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#when-to-use_4","title":"When to use","text":"<ul> <li> Calling into opaque closed-source BLAS-like microkernel   libraries.</li> <li> Any other cases covered above can be used, especially   microkernels that can be represented in MLIR or as statically linked   libraries.</li> </ul>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/extensions/#implementation_4","title":"Implementation","text":"<p>The compiler is changed to produce calls to imports via a dynamic import table provided to each dispatch function. The import table is declared in the executable library for use at runtime. Runtime applications register an import provider to resolve named symbols in the import table to C functions that marshal arguments and results.</p> <p>The compiler-side needs some additional work but an example is included here: Issue 7504. The runtime-side is complete and resolution is performed by a user-supplied <code>iree_hal_executable_import_provider_t</code>.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/glossary/","title":"Glossary","text":"<p>IREE exists in an ecosystem of projects and acts as a bridge between machine learning frameworks and a variety of hardware platforms. This glossary outlines some of those projects and technologies.</p> <p>Something missing?</p> <p>Don't see a project of technology here that you think should be? We welcome contributions on our GitHub page!</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/glossary/#jax","title":"JAX","text":"<p>JAX is Python framework supporting high-performance machine learning research by bridging automatic differentiation and ML compilers like XLA and IREE.</p> <p>See the JAX Integration guide for details on how to use JAX programs with IREE.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/glossary/#mlir","title":"MLIR","text":"<p>Multi-Level Intermediate Representation (MLIR) is the compiler framework that IREE is built around. Beyond the tooling this includes a set of common dialects and transformations that IREE utilizes for its code generation system.</p> <p>For general discussion on MLIR see the project's discourse forum.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/glossary/#linalg","title":"Linalg","text":"<p>Linalg is an MLIR dialect that defines Linear Algebra operations in a generalized fashion by modeling iteration spaces together with compute payloads. Linalg includes a set of commonly used operations as well as generic interfaces.</p> <p>IREE uses the Linalg dialect during its code generation pipeline to define tensor operations then generate loop structures for its various backend targets.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/glossary/#openxla","title":"OpenXLA","text":"<p>OpenXLA is a community-driven, open source ML compiler ecosystem.</p> <p>IREE is one project under the OpenXLA GitHub Organization, and it interfaces with many of the other projects, such as StableHLO.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/glossary/#pytorch","title":"PyTorch","text":"<p>PyTorch is an optimized tensor library for deep learning.</p> <p>PyTorch uses the Torch-MLIR project to interface with projects like IREE. See the PyTorch Integration guide for details on how to use PyTorch programs with IREE.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/glossary/#spir-v","title":"SPIR-V","text":"<p>SPIR-V is a shader and kernel intermediate language for expressing parallel computation typically used for GPUs. It serves as a hardware agnostic assembly format for distributing complex, computationally intensive programs.</p> <p>IREE uses the SPIR-V MLIR Dialect in its code generation pipeline for Vulkan and other compute APIs.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/glossary/#stablehlo","title":"StableHLO","text":"<p>StableHLO is a set of versioned high-level operations (HLOs) for ML models with backward and forward compatibility guarantees. StableHLO aims to improve interoperability between frameworks (such as TensorFlow, JAX, and PyTorch) and ML compilers.</p> <p>StableHLO has both a specification and an MLIR dialect.</p> <p>IREE uses the StableHLO MLIR Dialect as one of its input formats.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/glossary/#tosa","title":"TOSA","text":"<p>Tensor Operator Set Architecture (TOSA) provides a set of tensor operations commonly employed by Deep Neural Networks. TOSA defines accuracy and compatibility constraints so frameworks that use it can trust that applications will produce similar results on a variety of hardware targets.</p> <p>TOSA has both a specification and an MLIR dialect.</p> <p>IREE uses the TOSA MLIR dialect as one of its input formats.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/glossary/#tflite","title":"TFLite","text":"<p>TensorFlow Lite (TFLite) is a library for deploying models on mobile and other edge devices.</p> <p>IREE supports running TFLite programs that have been imported into MLIR using the TOSA dialect. See the TFLite Integration guide for details on how to use TFLite programs with IREE.</p> <p>IREE also has bindings for the TFLite C API, see the <code>runtime/bindings/tflite/</code> directory for details.</p>","tags":["JAX","PyTorch","TensorFlow"]},{"location":"reference/optimization-options/","title":"Optimization options","text":"<p>This page documents various supported flags for optimizing IREE programs. Each is presented with its English name, flag to enable/disable, and default state.</p> <p>These flags can be passed to the:</p> <ul> <li><code>iree-compile</code> command line tool</li> <li><code>extra_args=[\"--flag\"]</code> argument to <code>iree.compiler.tools</code> Python wrappers</li> <li>In-process Python compiler API   <code>iree.compiler.transforms.iree-compile.CompilerOptions(\"--flag\", \"--flag2\")</code>   constructor</li> <li><code>ireeCompilerOptionsSetFlags()</code> compiler C API function</li> </ul>"},{"location":"reference/optimization-options/#high-level-program-optimizations","title":"High level program optimizations","text":""},{"location":"reference/optimization-options/#constant-evaluation-iree-opt-const-eval-on","title":"Constant evaluation (<code>--iree-opt-const-eval</code> (on))","text":"<p>Performs compile-time evaluation of any global initializers which produce the initial values for global constants, storing the global directly in the program as constant data. This extracts such constant program fragments and recursively compiles them, using the runtime to evaluate the results.</p> <p>Note that this only has any effect on computations in module initializer functions, not free-standing operations in the program which may produce constant-derived results. See <code>--iree-opt-const-expr-hoisting</code> for options to optimize these.</p>"},{"location":"reference/optimization-options/#constant-expression-hoisting-iree-opt-const-expr-hoisting-off","title":"Constant expression hoisting (<code>--iree-opt-const-expr-hoisting</code> (off))","text":"<p>Identifies all trees of constant expressions in the program and uses a heuristic to determine which would be profitable to hoist into global initializers for evaluation at module load. Together with <code>--iree-opt-const-eval</code>, this will convert eligible trees of expressions to purely static data embedded in the module.</p> <p>The heuristic is currently relatively primitive, using static information to disable hoisting of leaf operations which are metadata only (i.e. broadcasts, etc) or are expected to fold away as part of operator fusion. Notably, the current heuristic is likely to pessimize module size in the case of complicated programs with trees of constant, large tensors.</p>"},{"location":"reference/optimization-options/#numeric-precision-reduction-iree-opt-numeric-precision-reduction-off","title":"Numeric precision reduction (<code>--iree-opt-numeric-precision-reduction</code> (off))","text":"<p>Analyzes program constant data and program flow to identify math operations which can be safely evaluated with reduced precision (currently with a minimum of 8bit integers but being extended to infer any bit depth) and inserts appropriate casts. In conjunction with Constant Expression Hoisting, Constant Evaluation and other automatic optimizations, this can produce programs where large amounts (up to the whole) have had their numeric operations and constant data rewritten to lower precision types.</p> <p>This feature is actively evolving and will be the subject of dedicated documentation when ready.</p>"},{"location":"reference/optimization-options/#strip-debug-assertions-iree-opt-strip-assertions-off","title":"Strip Debug Assertions (<code>--iree-opt-strip-assertions</code> (off))","text":"<p>Strips all <code>std.assert</code> ops in the input program after useful information for optimization analysis has been extracted. Assertions provide useful user-visible error messages but can prevent critical optimizations. Assertions are not, however, a substitution for control flow and frontends that want to check errors in optimized release builds should do so via actual code - similar to when one would <code>if (foo) return false;</code> vs. <code>assert(foo);</code> in a normal program.</p>"},{"location":"reference/bindings/","title":"API bindings","text":"<p>API bindings allow for programmatic use of IREE's compiler and runtime components. The core IREE project is written in C<sup>1</sup>, allowing for API bindings to be written in a variety of other languages.</p> <p>Something missing?</p> <p>Want to use another language? Looking for something specific out of one of those already listed?</p> <p>We welcome discussions on our communication channels and contributions on our GitHub page!</p>"},{"location":"reference/bindings/#official-api-bindings","title":"Official API bindings","text":"<p>Members of the core project team and OpenXLA partners maintain these official bindings:</p> Language Compiler API? Runtime API? Published packages? C/C++  Supported  Supported  Unsupported Python  Supported  Supported  Supported JavaScript  Experimental  Experimental  Unsupported"},{"location":"reference/bindings/#cc","title":"C/C++","text":"<p>See the C API reference page.</p>"},{"location":"reference/bindings/#python","title":"Python","text":"<p>See the Python reference page.</p>"},{"location":"reference/bindings/#javascript","title":"JavaScript","text":"<ul> <li>JavaScript bindings for WebAssembly and WebGPU are under development in IREE's <code>experimental/web/</code> directory.</li> </ul>"},{"location":"reference/bindings/#unofficial-api-bindings","title":"Unofficial API bindings","text":"<p>Members of our developer community have authored bindings using other languages:</p> Language Compiler API? Runtime API? Published packages? Julia  Experimental  Experimental  Unsupported Rust  Unsupported  Experimental  Experimental"},{"location":"reference/bindings/#julia","title":"Julia","text":"<ul> <li>Coil.jl is an experimental package to lower and execute Julia tensor operations to IREE.</li> </ul>"},{"location":"reference/bindings/#rust","title":"Rust","text":"<ul> <li>iree-rs is a crate containing rustic bindings for the IREE runtime.</li> </ul> <ol> <li> <p>with some C++ tools and utilities\u00a0\u21a9</p> </li> </ol>"},{"location":"reference/bindings/c-api/","title":"C API bindings","text":""},{"location":"reference/bindings/c-api/#overview","title":"Overview","text":"<p>The IREE compiler and IREE runtime both have their own C/C++ APIs. This page introduces the available APIs and describes how to use them from your applications.</p> <p>Note</p> <p>There are multiple ways to distribute and depend on C/C++ projects, each with varying levels of portability, flexibility, and toolchain compatibility. IREE aims to support common configurations and platforms.</p>"},{"location":"reference/bindings/c-api/#compiler-api","title":"Compiler API","text":"<p>The IREE compiler is structured as a monolithic shared object with a dynamic plugin system allowing for extensions. The shared object exports symbols for versioned API functions.</p> <pre><code>graph TD\n  accTitle: IREE compiler linkage model diagram\n  accDescr {\n    The libIREECompiler.so or IREECompiler.dll shared object contains pipelines,\n    target backends, and general passes as private implementation details.\n    Compiler plugins interface with the compiler shared object to extend it with\n    custom targets, dialects, etc.\n    Applications interface with the compiler shared object through the compiler\n    C API's exported symbols.\n  }\n\n  subgraph compiler[libIREECompiler.so / IREECompiler.dll]\n    pipelines(\"Pipelines\n\n    \u2022 Flow\n    \u2022 Stream\n    \u2022 etc.\")\n\n    targets(\"Target backends\n\n    \u2022 llvm-cpu\n    \u2022 vulkan-spirv\n    \u2022 etc.\")\n\n    passes(\"General passes\n\n    \u2022 Const eval\n    \u2022 DCE\n    \u2022 etc.\")\n  end\n\n  plugins(\"Compiler plugins\n\n    \u2022 Custom targets\n    \u2022 Custom dialects\n    \u2022 etc.\")\n\n  application(Your application)\n\n  compiler &lt;-- \"Plugin API&lt;br&gt;(static or dynamic linking)\" --&gt; plugins\n  compiler -. \"Compiler C API&lt;br&gt;(exported symbols)\" .-&gt; application</code></pre> <p>API definitions can be found in the following locations:</p> Source location Overview <code>iree/compiler/embedding_api.h</code> Top-level IREE compiler embedding API <code>iree/compiler/PluginAPI/</code> directory IREE compiler plugin API <code>mlir/include/mlir-c/</code> directory MLIR C API headers"},{"location":"reference/bindings/c-api/#concepts","title":"Concepts","text":"<p>The compiler API is centered around running pipelines to translate inputs to artifacts. These are modeled via sessions, invocations, sources, and outputs.</p> <pre><code>stateDiagram-v2\n  accTitle: IREE compiler session and invocation state diagram\n  accDescr {\n    Input files are opened (or buffers are wrapped) as sources in a session.\n    Sources are parsed into invocations, which run pipelines.\n    Output files are written (or buffers are mapped) for compilation artifacts.\n    Sessions can contain multiple sources and run multiple invocations.\n  }\n\n  direction LR\n  InputFile --&gt; Source1 : open file\n  InputBuffer --&gt; Source2 : wrap buffer\n\n  state Session {\n    Source1 --&gt; Invocation1\n    Source2 --&gt; Invocation2\n    Invocation1 --&gt; Invocation1 : run pipeline\n    Invocation2 --&gt; Invocation2 : run pipeline\n  }\n\n  Invocation1 --&gt; Output1File   : write file\n  Invocation1 --&gt; Output1Buffer : map memory\n  Invocation2 --&gt; Output2Buffer : map memory</code></pre>"},{"location":"reference/bindings/c-api/#sessions","title":"Sessions","text":"<p>A session (<code>iree_compiler_session_t</code>) is a scope where one or more invocations can run.</p> <ul> <li>Internally, sessions consist of an <code>MLIRContext</code> and a private set of   options.</li> <li>Sessions may activate available plugins based on their options.</li> </ul>"},{"location":"reference/bindings/c-api/#invocations","title":"Invocations","text":"<p>An invocation (<code>iree_compiler_invocation_t</code>) is a discrete run of the compiler.</p> <ul> <li>Invocations run pipelines, consisting of passes, to translate from   sources to outputs.</li> </ul>"},{"location":"reference/bindings/c-api/#sources","title":"Sources","text":"<p>A source (<code>iree_compiler_source_t</code>) represents an input program, including operations and data.</p> <ul> <li>Sources may refer to files or buffers in memory.</li> </ul>"},{"location":"reference/bindings/c-api/#outputs","title":"Outputs","text":"<p>An output (<code>iree_compiler_output_t</code>) represents a compilation artifact.</p> <ul> <li>Outputs can be standalone files or more advanced streams.</li> </ul>"},{"location":"reference/bindings/c-api/#plugins","title":"Plugins","text":"<p>A plugin extends the compiler with some combination of target backends, options, passes, or pipelines.</p>"},{"location":"reference/bindings/c-api/#usage","title":"Usage","text":"<p>This snippet shows the general layout of the API. For working examples, see the samples below.</p> <pre><code># CMakeLists.txt\nset(_IREE_COMPILER_API \"${_IREE_COMPILER_ROOT}/bindings/c/iree/compiler\")\ntarget_include_directories(${_NAME} SYSTEM PRIVATE ${_IREE_COMPILER_API})\ntarget_link_libraries(${_NAME} iree_compiler_bindings_c_loader)\n</code></pre> <pre><code>// iree_compiler_demo.c\n\n#include &lt;iree/compiler/embedding_api.h&gt;\n#include &lt;iree/compiler/loader.h&gt;\n\nint main(int argc, char** argv) {\n// Load the compiler library then initialize it.\nireeCompilerLoadLibrary(\"libIREECompiler.so\");\nireeCompilerGlobalInitialize();\n\n// Create a session to track compiler state and set flags.\niree_compiler_session_t *session = ireeCompilerSessionCreate();\nireeCompilerSessionSetFlags(session, argc, argv);\n\n// Open a file as an input source to the compiler.\niree_compiler_source_t *source = NULL;\nireeCompilerSourceOpenFile(session, \"input.mlir\", &amp;source);\n\n// Use an invocation to compile from the input source to one or more outputs.\niree_compiler_invocation_t *inv = ireeCompilerInvocationCreate(session);\nireeCompilerInvocationPipeline(inv, IREE_COMPILER_PIPELINE_STD);\n\n// Output the compiled artifact to a file.\niree_compiler_output_t *output = NULL;\nireeCompilerOutputOpenFile(\"output.vmfb\", &amp;output);\nireeCompilerInvocationOutputVMBytecode(inv, output);\n\n// Cleanup state.\nireeCompilerInvocationDestroy(inv);\nireeCompilerOutputDestroy(output);\nireeCompilerSourceDestroy(source);\nireeCompilerSessionDestroy(session);\nireeCompilerGlobalShutdown();\n}\n</code></pre>"},{"location":"reference/bindings/c-api/#samples","title":"Samples","text":"Project Source Description iree-org/iree-template-compiler-cmake <code>hello_compiler.c</code> Compiler application template openxla/openxla-pjrt-plugin <code>iree_compiler.cc</code> JIT for TensorFlow + JAX to IREE openxla/iree <code>samples/compiler_plugins/</code> In-tree demos of compiler plugins"},{"location":"reference/bindings/c-api/#runtime-api","title":"Runtime API","text":"<p>The IREE runtime is structured as a modular set of library components. Each component is designed to be linked into applications directly and compiled with LTO style optimizations.</p> <p>The low level library components can be used directly or through a higher level API.</p> High level APILow level API <p>The high level 'runtime' API sits on top of the low level components. It is relatively terse but does not expose the full flexibility of the underlying systems.</p> <pre><code>graph TD\n  accTitle: IREE runtime high level API diagram\n  accDescr {\n    The IREE runtime includes 'base', 'HAL', and 'VM' components, each with\n    their own types and API methods.\n    A high level \"runtime API\" sits on top of these component APIs.\n    Applications can interface indirectly with the IREE runtime via this\n    high level runtime API.\n  }\n\n  subgraph iree_runtime[IREE Runtime]\n    subgraph base\n      base_types(\"Types\n\n      \u2022 allocator\n      \u2022 status\n      \u2022 etc.\")\n    end\n\n    subgraph hal[HAL]\n      hal_types(\"Types\n\n      \u2022 buffer\n      \u2022 device\n      \u2022 etc.\")\n\n      hal_drivers(\"Drivers\n\n      \u2022 local-*\n      \u2022 vulkan\n      \u2022 etc.\")\n    end\n\n    subgraph vm[VM]\n      vm_types(\"Types\n\n      \u2022 context\n      \u2022 invocation\n      \u2022 etc.\")\n    end\n\n    runtime_api(\"Runtime API\n\n    \u2022 instance\n    \u2022 session\n    \u2022 call\")\n\n    base_types &amp; hal_types &amp; hal_drivers &amp; vm_types --&gt; runtime_api\n  end\n\n  application(Your application)\n\n  runtime_api --&gt; application</code></pre> <p>Each runtime component has its own low level API. The low level APIs are typically verbose as they expose the full flexibility of each underlying system.</p> <pre><code>graph TD\n  accTitle: IREE runtime low level API diagram\n  accDescr {\n    The IREE runtime includes 'base', 'HAL', and 'VM' components, each with\n    their own types and API methods.\n    Applications can interface directly with the IREE runtime via the low\n    level component APIs.\n  }\n\n  subgraph iree_runtime[IREE Runtime]\n    subgraph base\n      base_types(\"Types\n\n      \u2022 allocator\n      \u2022 status\n      \u2022 etc.\")\n    end\n    subgraph hal[HAL]\n      hal_types(\"Types\n\n      \u2022 buffer\n      \u2022 device\n      \u2022 etc.\")\n\n      hal_drivers(\"Drivers\n\n      \u2022 local-*\n      \u2022 vulkan\n      \u2022 etc.\")\n    end\n    subgraph vm[VM]\n      vm_types(\"Types\n\n      \u2022 context\n      \u2022 invocation\n      \u2022 etc.\")\n    end\n  end\n\n  application(Your application)\n\n  base_types &amp; hal_types &amp; hal_drivers &amp; vm_types --&gt; application</code></pre> <p>Runtime API header files are organized by component:</p> Component header file Overview <code>iree/runtime/api.h</code> High level runtime API <code>iree/base/api.h</code> Core API, type definitions, ownership policies, utilities <code>iree/vm/api.h</code> VM APIs: loading modules, I/O, calling functions <code>iree/hal/api.h</code> HAL APIs: device management, synchronization, accessing hardware features"},{"location":"reference/bindings/c-api/#high-level-concepts","title":"High level concepts","text":"<p>The high level API uses instances, sessions, and calls to run programs with a small API surface.</p> <pre><code>stateDiagram-v2\n  accTitle: IREE runtime high level API state diagram\n  accDescr {\n    Instances track sessions and state: options, drivers, devices.\n    Sessions track calls and state: a device and bytecode/VM modules.\n    Calls track input and output lists.\n  }\n\n  state iree_runtime_instance_t {\n    instance_state: state&lt;br&gt;- options&lt;br&gt;- drivers&lt;br&gt;- devices\n\n    state iree_runtime_session_t {\n      session_state: state&lt;br&gt;- device&lt;br&gt;- VM / bytecode modules\n      state iree_runtime_call_t  {\n        inputs\n        outputs\n      }\n    }\n  }</code></pre>"},{"location":"reference/bindings/c-api/#instance","title":"Instance","text":"<p>An instance (<code>iree_runtime_instance_t</code>) isolates runtime usage and manages device resources.</p> <ul> <li>Instances may service multiple sessions to avoid extra device interaction   and reuse caches/pools.</li> <li>Separate instances are isolated/sandboxed from one another.</li> </ul>"},{"location":"reference/bindings/c-api/#session","title":"Session","text":"<p>A session (<code>iree_runtime_session_t</code>) contains a set of loaded modules and their state.</p> <ul> <li>Sessions that share an instance may share resources directly.</li> <li>Sessions that do not share an instance can transfer resources using   import and export APIs.</li> </ul>"},{"location":"reference/bindings/c-api/#call","title":"Call","text":"<p>A call (<code>iree_runtime_call_t</code>) is a stateful VM function call builder.</p> <ul> <li>Calls can be reused to avoid having to construct input lists for each   invocation.</li> </ul>"},{"location":"reference/bindings/c-api/#low-level-concepts","title":"Low level concepts","text":""},{"location":"reference/bindings/c-api/#base","title":"Base","text":"<p>Under construction, more coming soon</p>"},{"location":"reference/bindings/c-api/#vm","title":"VM","text":"<p>IREE uses its own Virtual Machine (VM) at runtime to interpret program instructions on the host system.</p> Tip - EmitC alternate lowering path <p>VM instructions may be further lowered to C source code for static or resource constrained deployment.</p> <p>See the <code>--output-format=vm-c</code> compiler option and the samples in <code>samples/emitc_modules/</code> for more information.</p> <p>The VM supports generic operations like loads, stores, arithmetic, function calls, and control flow. The VM builds streams of more complex program logic and dense math into HAL command buffers that are dispatched to hardware backends.</p> <ul> <li>VM instances can serve multiple isolated execution contexts.</li> <li>VM contexts are effectively sandboxes for loading modules and running   programs.</li> <li> <p>VM modules provide all functionality to execution contexts, including   access to hardware accelerators through the HAL. Compiled user programs are   also modules.</p> <pre><code>stateDiagram-v2\n  accTitle: Sample VM Modules\n  accDescr {\n    Bytecode modules contain program state, program functions, and debug\n    information.\n    HAL modules contain devices, executables, HAL functions, and HAL types.\n    Custom modules may contain external functions and custom types.\n  }\n\n  state \"Bytecode module\" as bytecode {\n    bytecode_contents: Module state&lt;br&gt;Program functions&lt;br&gt;Debug information\n  }\n\n  state \"HAL module\" as HAL {\n    hal_contents: Devices&lt;br&gt;Executables&lt;br&gt;HAL functions&lt;br&gt;HAL types\n  }\n\n  state \"Custom module\" as custom {\n    custom_contents: External functions&lt;br&gt;Custom types\n  }</code></pre> </li> </ul>"},{"location":"reference/bindings/c-api/#hal","title":"HAL","text":"<p>IREE uses a Hardware Abstraction Layer (HAL) to model and interact with hardware devices like CPUs, GPUs and other accelerators.</p> <ul> <li>HAL drivers are used to enumerate and create HAL devices.</li> <li>HAL devices interface with hardware, such as by allocating device memory,   preparing executables, recording and dispatching command buffers, and   synchronizing with the host.</li> <li>HAL buffers represent data storage and buffer views represent views into   that storage with associated shapes and types (similar to \"tensors\").</li> </ul>"},{"location":"reference/bindings/c-api/#usage_1","title":"Usage","text":"<p>This snippet shows the general layout of the API. For working examples, see the samples below.</p> <pre><code># CMakeLists.txt\ntarget_include_directories(${_NAME} SYSTEM PRIVATE ${_IREE_RUNTIME_ROOT})\ntarget_link_libraries(${_NAME} iree_runtime_runtime)\n</code></pre> <pre><code>// iree_runtime_demo.c\n\n#include &lt;iree/runtime/api.h&gt;\n\nint main(int argc, char** argv) {\n// Setup the shared runtime instance.\niree_runtime_instance_options_t instance_options;\niree_runtime_instance_options_initialize(&amp;instance_options);\niree_runtime_instance_options_use_all_available_drivers(&amp;instance_options);\niree_runtime_instance_t* instance = NULL;\niree_runtime_instance_create(\n&amp;instance_options, iree_allocator_system(), &amp;instance);\n\n// Create the HAL device used to run the workloads.\niree_hal_device_t* device = NULL;\niree_runtime_instance_try_create_default_device(\ninstance, iree_make_cstring_view(\"local-task\"), &amp;device);\n\n// Create a session to hold the module state.\niree_runtime_session_options_t session_options;\niree_runtime_session_options_initialize(&amp;session_options);\niree_runtime_session_t* session = NULL;\niree_runtime_session_create_with_device(\ninstance, &amp;session_options, device,\niree_runtime_instance_host_allocator(instance), &amp;session);\n\n// Load the compiled user module from a file.\niree_runtime_session_append_bytecode_module_from_file(\nsession, \"program.vmfb\");\n\n// Build and issue the call.\niree_runtime_call_t call;\niree_runtime_call_initialize_by_name(\nsession, iree_make_cstring_view(\"module.entry_function_name\"), &amp;call);\n// iree_runtime_call_inputs_push_back_buffer_view(...);\niree_runtime_call_invoke(&amp;call, /*flags=*/0);\n\n// Retrieve the function outputs and clean up the call.\n// iree_runtime_call_outputs_pop_front_buffer_view(...);\niree_runtime_call_deinitialize(&amp;call);\n\n// Cleanup state.\niree_runtime_session_release(session);\niree_hal_device_release(device);\niree_runtime_instance_release(instance);\n}\n</code></pre>"},{"location":"reference/bindings/c-api/#samples_1","title":"Samples","text":"Project Source Description iree-org/iree-template-runtime-cmake <code>hello_world.c</code> Runtime application template openxla/iree <code>runtime/demo/</code> In-tree demos of the high level runtime API openxla/iree <code>samples/</code> In-tree sample applications iree-org/iree-samples <code>runtime-library/</code> Shared runtime library builderBuilds <code>libireert.so</code> to aid development iml130/iree-template-cpp <code>simple_embedding.c</code> Demo integration into a project"},{"location":"reference/bindings/c-api/#compiler-runtime-jit","title":"Compiler + Runtime = JIT","text":"<p>The compiler and runtime APIs may be used together to build a \"just in time\" (JIT) execution engine. JIT compilation allows for last-minute specialization with no prior knowledge of target devices and avoids issues with version drift, but it can also constrain deployment options and usage scenarios.</p>"},{"location":"reference/bindings/python/","title":"Python bindings","text":"","tags":["Python"]},{"location":"reference/bindings/python/#overview","title":"Overview","text":"<p>IREE offers Python bindings split into several packages, covering different components:</p> PIP package name Description <code>iree-compiler</code> IREE's generic compiler tools and helpers <code>iree-runtime</code> IREE's runtime, including CPU and GPU backends <code>iree-tools-tf</code> Tools for importing from TensorFlow <code>iree-tools-tflite</code> Tools for importing from TensorFlow Lite <code>iree-jax</code> Tools for importing from JAX <p>Collectively, these packages allow for importing from frontends, compiling towards various targets, and executing compiled code on IREE's backends.</p>","tags":["Python"]},{"location":"reference/bindings/python/#prerequisites","title":"Prerequisites","text":"<p>To use IREE's Python bindings, you will first need to install Python 3 and pip, as needed.</p> Tip - Virtual environments <p>We recommend using virtual environments to manage python packages, such as through <code>venv</code> (about, tutorial):</p> LinuxmacOSWindows <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre> <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre> <pre><code>python -m venv .venv\n.venv\\Scripts\\activate.bat\n</code></pre> <p>When done, run <code>deactivate</code>.</p>","tags":["Python"]},{"location":"reference/bindings/python/#installing-iree-packages","title":"Installing IREE packages","text":"","tags":["Python"]},{"location":"reference/bindings/python/#prebuilt-packages","title":"Prebuilt packages","text":"Stable releases Nightly releases <p>Stable release packages are published to PyPI.</p> <pre><code>python -m pip install \\\niree-compiler \\\niree-runtime\n</code></pre> <p>Nightly releases are published on GitHub releases.</p> <pre><code>python -m pip install \\\n--find-links https://openxla.github.io/iree/pip-release-links.html \\\n--upgrade \\\niree-compiler \\\niree-runtime\n</code></pre>","tags":["Python"]},{"location":"reference/bindings/python/#building-from-source","title":"Building from source","text":"<p>See Building Python bindings page for instructions for building from source.</p>","tags":["Python"]},{"location":"reference/bindings/python/#usage","title":"Usage","text":"<p>Info - API reference pages</p> <p>API reference pages for IREE's runtime and compiler Python APIs are hosted on readthedocs.</p> <p>Documentation for the MLIR compiler Python APIs can be found at https://mlir.llvm.org/docs/Bindings/Python/.</p>","tags":["Python"]},{"location":"reference/bindings/python/#compile-a-program","title":"Compile a program","text":"<pre><code>from iree import compiler as ireec\n\n# Compile a module.\nINPUT_MLIR = \"\"\"\nmodule @arithmetic {\n  func.func @simple_mul(%arg0: tensor&lt;4xf32&gt;, %arg1: tensor&lt;4xf32&gt;) -&gt; tensor&lt;4xf32&gt; {\n    %0 = arith.mulf %arg0, %arg1 : tensor&lt;4xf32&gt;\n    return %0 : tensor&lt;4xf32&gt;\n  }\n}\n\"\"\"\n\n# Compile using the vmvx (reference) target:\ncompiled_flatbuffer = ireec.tools.compile_str(\n    INPUT_MLIR,\n    target_backends=[\"vmvx\"])\n</code></pre>","tags":["Python"]},{"location":"reference/bindings/python/#run-a-compiled-program","title":"Run a compiled program","text":"<pre><code>from iree import runtime as ireert\nimport numpy as np\n\n# Register the module with a runtime context.\n# Use the \"local-task\" CPU driver, which can load the vmvx executable:\nconfig = ireert.Config(\"local-task\")\nctx = ireert.SystemContext(config=config)\nvm_module = ireert.VmModule.copy_buffer(ctx.instance, compiled_flatbuffer)\nctx.add_vm_module(vm_module)\n\n# Invoke the function and print the result.\nprint(\"INVOKE simple_mul\")\narg0 = np.array([1., 2., 3., 4.], dtype=np.float32)\narg1 = np.array([4., 5., 6., 7.], dtype=np.float32)\nf = ctx.modules.arithmetic[\"simple_mul\"]\nresults = f(arg0, arg1).to_host()\nprint(\"Results:\", results)\n</code></pre>","tags":["Python"]},{"location":"reference/bindings/python/#samples","title":"Samples","text":"<p>Check out the samples in IREE's samples/colab/ directory and the iree-samples repository for examples using the Python APIs.</p>","tags":["Python"]},{"location":"reference/mlir-dialects/","title":"MLIR dialects","text":"<p>These pages contain automatically generated documentation for the MLIR dialects defined in the IREE repository. IREE also makes extensive use of dialects from the upstream MLIR repository, which are documented at https://mlir.llvm.org/docs/Dialects/.</p>"},{"location":"reference/mlir-dialects/#iree-internal-dialects","title":"IREE internal dialects","text":"<p>These dialects are an implementation detail of the IREE compiler, though they can be used by plugins and other advanced integrations. The sources for most of these dialects can be found in the <code>iree/compiler/Dialect/</code> directory.</p> Dialect Description Check Defines assertions for IREE tests Flow Models execution data flow and partitioning HAL Represents operations against the IREE HAL<sup>1</sup> HALInline Inline HAL interop runtime module dialect HALLoader HAL inline executable loader runtime module dialect Stream Model execution partitioning and scheduling Util Types and ops common across IREE subdialects VM Represents operations against an abstract virtual machine VMVX Virtual Machine Vector Extensions"},{"location":"reference/mlir-dialects/#iree-public-dialects","title":"IREE public dialects","text":"<p>The ops in these dialects are legal to include in compiler inputs. The sources for these dialects can be found in the <code>llvm-external-projects/iree-dialects/</code> directory that is designed to be used from other projects via LLVM's external projects mechanism.</p> Dialect Description IREEInput Structural ops legal as input to IREE's compiler IREELinalgExt Extensions to the Linalg dialect for specific operations <ol> <li> <p>Hardware Abstraction Layer\u00a0\u21a9</p> </li> </ol>"},{"location":"reference/mlir-dialects/Check/","title":"Check","text":""},{"location":"reference/mlir-dialects/Check/#check-dialect","title":"'check' Dialect","text":"<p>A dialect implementing test assertions for IREE modules.</p> <ul> <li>'check' Dialect<ul> <li>Operation definition<ul> <li>check.expect_all_true (Check::ExpectAllTrueOp)</li> <li>check.expect_almost_eq (Check::ExpectAlmostEqOp)</li> <li>check.expect_almost_eq_const (Check::ExpectAlmostEqConstOp)</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/mlir-dialects/Check/#operation-definition","title":"Operation definition","text":""},{"location":"reference/mlir-dialects/Check/#checkexpect_all_true-checkexpectalltrueop","title":"<code>check.expect_all_true</code> (Check::ExpectAllTrueOp)","text":"<p>Checks that the operand contains only values that are true</p> <p>Syntax:</p> <pre><code>operation ::= `check.expect_all_true` `(` $operand `)` attr-dict `:` type($operand)\n</code></pre> <p>Verifies that the operand contains true values, which are represented by any non-zero integer.</p> <p>Issues a non-fatal failure if the verification fails.</p> <pre><code>check.expect_all_true(%arg0) : !hal.buffer_view\ncheck.expect_all_true(%arg1) : tensor&lt;2x2xi32&gt;\n</code></pre>"},{"location":"reference/mlir-dialects/Check/#operands","title":"Operands:","text":"Operand Description <code>operand</code> buffer_view or tensor of signless integer values"},{"location":"reference/mlir-dialects/Check/#checkexpect_almost_eq-checkexpectalmosteqop","title":"<code>check.expect_almost_eq</code> (Check::ExpectAlmostEqOp)","text":"<p>Checks that the operands are almost equal</p> <p>Syntax:</p> <pre><code>operation ::= `check.expect_almost_eq` `(` $lhs `,` $rhs `)` attr-dict `:` type($lhs)\n</code></pre> <p>Verifies that the buffer view or tensor operands with float elements are almost equal to within an implementation-defined \"reasonable\" tolerance.</p> <p>Issues a non-fatal failure if the verification fails.</p> <pre><code>check.expect_almost_eq(%arg0, %arg1) : tensor&lt;5xf32&gt;\n</code></pre> <p>Traits: SameTypeOperands</p>"},{"location":"reference/mlir-dialects/Check/#operands_1","title":"Operands:","text":"Operand Description <code>lhs</code> buffer_view or tensor of floating-point values <code>rhs</code> buffer_view or tensor of floating-point values"},{"location":"reference/mlir-dialects/Check/#checkexpect_almost_eq_const-checkexpectalmosteqconstop","title":"<code>check.expect_almost_eq_const</code> (Check::ExpectAlmostEqConstOp)","text":"<p>Checks that the tensor operand is almost equal to some constant</p> <p>Syntax:</p> <pre><code>operation ::= `check.expect_almost_eq_const` `(` $lhs `,` $value `)` attr-dict `:` type($lhs)\n</code></pre> <p>Verifies that the tensor operand with float elements is almost equal to the constant attribute within an implementation-defined \"reasonable\" tolerance.</p> <p>Issues a non-fatal failure if the verification fails.</p> <p>This op is just a convenience wrapper around the expect_almost_eq op.</p> <pre><code>check.expect_almost_eq_const(%const0, dense&lt;[0.999999, 2.0]&gt; : tensor&lt;5xf32&gt;) : tensor&lt;5xf32&gt;\n</code></pre>"},{"location":"reference/mlir-dialects/Check/#attributes","title":"Attributes:","text":"AttributeMLIR TypeDescription <code>value</code>::mlir::ElementsAttrconstant vector/tensor attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | tensor of floating-point values   ### `check.expect_eq` (Check::ExpectEqOp)  _Checks that the tensor or buffer view operands are equal_   Syntax:  <pre><code>operation ::= `check.expect_eq` `(` $lhs `,` $rhs `)` attr-dict `:` type($lhs)\n</code></pre>  Verifies that the operands are exactly equal.  Issues a non-fatal failure if the verification fails.  <pre><code>check.expect_eq(%arg0, %arg1) : tensor&lt;5xi32&gt;\n</code></pre>  Traits: SameTypeOperands  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | buffer_view or tensor of any type values | `rhs` | buffer_view or tensor of any type values   ### `check.expect_eq_const` (Check::ExpectEqConstOp)  _Checks that the tensor operand is equal to some constant_   Syntax:  <pre><code>operation ::= `check.expect_eq_const` `(` $lhs `,` $value `)` attr-dict `:` type($lhs)\n</code></pre>  Verifies that the tensor operand is exactly equal to a constant attribute.  Issues a non-fatal failure if the verification fails.  This op is just a convenience wrapper around the expect_eq op.  <pre><code>check.expect_eq_const(%arg0, dense&lt;[1, 2]&gt; : tensor&lt;2xi32&gt;) : tensor&lt;2xi32&gt;\n</code></pre>  ##### Attributes:   AttributeMLIR TypeDescription <code>value</code>::mlir::ElementsAttrconstant vector/tensor attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | tensor of any type values   ### `check.expect_false` (Check::ExpectFalseOp)  _Checks that the operand is false_   Syntax:  <pre><code>operation ::= `check.expect_false` `(` $operand `)` attr-dict `:` type($operand)\n</code></pre>  Verifies that the operand contains a false value, which is represented by zero.  Issues a non-fatal failure if the verification fails.  <pre><code>check.expect_false(%arg0) : i32\n</code></pre>  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | signless integer   ### `check.expect_true` (Check::ExpectTrueOp)  _Checks that the operand is true_   Syntax:  <pre><code>operation ::= `check.expect_true` `(` $operand `)` attr-dict `:` type($operand)\n</code></pre>  Verifies that the operand contains a true value, which is represented by any non-zero integer.  Issues a non-fatal failure if the verification fails.  <pre><code>check.expect_true(%arg0) : i32\n</code></pre>  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | signless integer"},{"location":"reference/mlir-dialects/Flow/","title":"Flow","text":""},{"location":"reference/mlir-dialects/Flow/#flow-dialect","title":"'flow' Dialect","text":"<p>A dialect designed to model execution data flow and partitioning.</p> <p>The flow dialect is used to model regions of dense computation and the data flow between them. MLIR value-semantic tensors are used as the primary data type to allow SSA use-def to provide a bulk of the infrastructure required to perform the computation partitioning and outlining.</p> <p>The dialect is designed to ingest relatively high-level linear algebra via XLA HLO ops (that also operate on the value-semantic tensor types) and optionally MLIR standard ops for control flow and other actions. After conversion of any higher-level ops that have special semantics in the flow dialect, such as global variables, the rest are partitioned into regions containing simple and compatible computations. Finally, outlining moves the computations into executables and leaves only the execution flow encoded via dispatch operations.</p> <p>The primary unit of interest is a \"dispatch region\" containing compatible computations that can be scheduled together efficiently (and safely). \"Compatible\" here is specified as similarly shaped workloads that indicate how many invocations a computation can be parallelized across when running in a SPMD execution model. Though it depends on the particular runtime backends this more concretely means things like the untiled workload (or tiled workgroups) used in GPU dispatches or similar thread pool executors.</p> <p>After identification of the dispatchable regions a set of transformations performs folding and simplification to reduce the total number of dispatches. Heuristics are used in certain cases to more efficiently schedule special ops (such as GEMM) and the design is amenable to profile- guided analysis that can be added in the future.</p> <p>The resulting outlined executable modules containing the dispatchable code can be translated to one or more backends (such as SPIR-V for Vulkan, or LLVM IR for running on the CPU, etc). The IR that is outlined is untouched and in the input format (such as XLA HLO ops) allowing conversion using any MLIR target that supports ingesting such input. A few special ops are used to communicate statically available information such as the expected workload size, shapes of inputs and outputs, etc.</p> <ul> <li>'flow' Dialect<ul> <li>Operation definition<ul> <li>Collective communication ops<ul> <li>flow.channel.count (Flow::ChannelCountOp)</li> <li>flow.channel.default (Flow::ChannelDefaultOp)</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/mlir-dialects/Flow/#operation-definition","title":"Operation definition","text":""},{"location":"reference/mlir-dialects/Flow/#collective-communication-ops","title":"Collective communication ops","text":""},{"location":"reference/mlir-dialects/Flow/#flowchannelcount-flowchannelcountop","title":"<code>flow.channel.count</code> (Flow::ChannelCountOp)","text":"<p>Returns the total number of participants in the group</p> <p>Syntax:</p> <pre><code>operation ::= `flow.channel.count` $channel `:` type($result)\n              attr-dict-with-keyword\n</code></pre> <p>Returns the total participant count in the collective communicator group.</p> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/Flow/#operands","title":"Operands:","text":"Operand Description <code>channel</code> a collecive communication channel"},{"location":"reference/mlir-dialects/Flow/#results","title":"Results:","text":"Result Description <code>result</code> index"},{"location":"reference/mlir-dialects/Flow/#flowchanneldefault-flowchanneldefaultop","title":"<code>flow.channel.default</code> (Flow::ChannelDefaultOp)","text":"<p>Returns a default collective communication channel</p> <p>Syntax:</p> <pre><code>operation ::= `flow.channel.default` ($group^)?\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre> <p>Returns a channel initialized using the runtime environment.</p> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/Flow/#attributes","title":"Attributes:","text":"AttributeMLIR TypeDescription <code>group</code>::mlir::StringAttrstring attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | a collecive communication channel  #### `flow.channel.rank` (Flow::ChannelRankOp)  _Returns the rank of the local participant in the group_   Syntax:  <pre><code>operation ::= `flow.channel.rank` $channel `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns the rank the channel represents as a participant in a collective group in `[0, count)`.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `channel` | a collecive communication channel  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index  #### `flow.channel.split` (Flow::ChannelSplitOp)  _Splits a collective communication channel_   Syntax:  <pre><code>operation ::= `flow.channel.split` $channel `,` $color `,` $key\n              `:` type($channel) `-&gt;` type($result)\n              attr-dict-with-keyword\n</code></pre>  Partitions the group associated with the given channel into disjoint subgroups for each unique value of color. Each new subgroup contains all participants of the same color and within each subgroup the key argument is used to define the rank order. When multiple participants in a group use the same key the tie will be broken using their rank in the parent group.  Interfaces: InferTypeOpInterface, OpAsmOpInterface  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `channel` | a collecive communication channel | `color` | index | `key` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | a collecive communication channel  #### `flow.collective.all_gather` (Flow::CollectiveAllGatherOp)  _Performs all-gather operation_   Syntax:  <pre><code>operation ::= `flow.collective.all_gather` $element_type `,` $target `,` $source `,` $channel `:`\n              `(` type($target) `,` type($source) `,` type($channel) `)` `-&gt;`\n              custom&lt;ShapedTiedResult&gt;(type($result), $target_dims, $tied_operands)\n              attr-dict-with-keyword\n</code></pre>  It gathers data from all ranks and concatenates them on the 0-th dimension. Interfaces: InferTypeOpInterface, TiedOpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>element_type</code>::mlir::iree_compiler::IREE::Flow::CollectiveElementTypeAttrvalid CollectiveElementType{{% markdown %}}Enum cases: * si8 (`Sint8`) * ui8 (`Uint8`) * si16 (`Sint16`) * ui16 (`Uint16`) * si32 (`Sint32`) * ui32 (`Uint32`) * si64 (`Sint64`) * ui64 (`Uint64`) * f16 (`Float16`) * f32 (`Float32`) * f64 (`Float64`) * bf16 (`BFloat16`){{% /markdown %}} <code>tied_operands</code>::mlir::ArrayAttr64-bit integer array attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `target` | ranked tensor of any type values | `target_dims` | index | `source` | ranked tensor of any type values | `channel` | a collecive communication channel  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values  #### `flow.collective.all_reduce` (Flow::CollectiveAllReduceOp)  _Performs all-reduce operation_   Syntax:  <pre><code>operation ::= `flow.collective.all_reduce` $reduction_op `,` $element_type `,` $target `,` $source `,` $channel `:`\n              `(` type($target) `,` type($source) `,` type($channel) `)` `-&gt;`\n              custom&lt;ShapedTiedResult&gt;(type($result), $target_dims, $tied_operands)\n              attr-dict-with-keyword\n</code></pre>  The operation reduces data across all the ranks in the channel. Interfaces: InferTypeOpInterface, TiedOpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>reduction_op</code>mlir::iree_compiler::IREE::Flow::CollectiveReductionOpAttrvalid CollectiveReductionOp{{% markdown %}}Enum cases: * none (`None`) * sum (`ReductionSum`) * product (`ReductionProduct`) * minimum (`ReductionMinimum`) * maximum (`ReductionMaximum`) * average (`ReductionAverage`){{% /markdown %}} <code>element_type</code>::mlir::iree_compiler::IREE::Flow::CollectiveElementTypeAttrvalid CollectiveElementType{{% markdown %}}Enum cases: * si8 (`Sint8`) * ui8 (`Uint8`) * si16 (`Sint16`) * ui16 (`Uint16`) * si32 (`Sint32`) * ui32 (`Uint32`) * si64 (`Sint64`) * ui64 (`Uint64`) * f16 (`Float16`) * f32 (`Float32`) * f64 (`Float64`) * bf16 (`BFloat16`){{% /markdown %}} <code>tied_operands</code>::mlir::ArrayAttr64-bit integer array attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `target` | ranked tensor of any type values | `target_dims` | index | `source` | ranked tensor of any type values | `channel` | a collecive communication channel  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values  #### `flow.collective.all_to_all` (Flow::CollectiveAllToAllOp)  _Performs all-to-all operation_   Syntax:  <pre><code>operation ::= `flow.collective.all_to_all` $element_type `,` $target `,` $source `,` $channel `:`\n              `(` type($target) `,` type($source) `,` type($channel) `)` `-&gt;`\n              custom&lt;ShapedTiedResult&gt;(type($result), $target_dims, $tied_operands)\n              attr-dict-with-keyword\n</code></pre>  This operation mutually exchanges data acrosss all of the ranks in the channel. Interfaces: InferTypeOpInterface, TiedOpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>element_type</code>::mlir::iree_compiler::IREE::Flow::CollectiveElementTypeAttrvalid CollectiveElementType{{% markdown %}}Enum cases: * si8 (`Sint8`) * ui8 (`Uint8`) * si16 (`Sint16`) * ui16 (`Uint16`) * si32 (`Sint32`) * ui32 (`Uint32`) * si64 (`Sint64`) * ui64 (`Uint64`) * f16 (`Float16`) * f32 (`Float32`) * f64 (`Float64`) * bf16 (`BFloat16`){{% /markdown %}} <code>tied_operands</code>::mlir::ArrayAttr64-bit integer array attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `target` | ranked tensor of any type values | `target_dims` | index | `source` | ranked tensor of any type values | `channel` | a collecive communication channel  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values  #### `flow.collective.reduce_scatter` (Flow::CollectiveReduceScatterOp)  _Performs reduce and scatter operations_   Syntax:  <pre><code>operation ::= `flow.collective.reduce_scatter` $reduction_op `,` $element_type `,` $target `,` $source `,` $channel `:`\n              `(` type($target) `,` type($source) `,` type($channel) `)` `-&gt;`\n              custom&lt;ShapedTiedResult&gt;(type($result), $target_dims, $tied_operands)\n              attr-dict-with-keyword\n</code></pre>  The operation reduces data across all the ranks in the channel and     scatters the result to each rank. Interfaces: InferTypeOpInterface, TiedOpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>reduction_op</code>mlir::iree_compiler::IREE::Flow::CollectiveReductionOpAttrvalid CollectiveReductionOp{{% markdown %}}Enum cases: * none (`None`) * sum (`ReductionSum`) * product (`ReductionProduct`) * minimum (`ReductionMinimum`) * maximum (`ReductionMaximum`) * average (`ReductionAverage`){{% /markdown %}} <code>element_type</code>::mlir::iree_compiler::IREE::Flow::CollectiveElementTypeAttrvalid CollectiveElementType{{% markdown %}}Enum cases: * si8 (`Sint8`) * ui8 (`Uint8`) * si16 (`Sint16`) * ui16 (`Uint16`) * si32 (`Sint32`) * ui32 (`Uint32`) * si64 (`Sint64`) * ui64 (`Uint64`) * f16 (`Float16`) * f32 (`Float32`) * f64 (`Float64`) * bf16 (`BFloat16`){{% /markdown %}} <code>tied_operands</code>::mlir::ArrayAttr64-bit integer array attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `target` | ranked tensor of any type values | `target_dims` | index | `source` | ranked tensor of any type values | `channel` | a collecive communication channel  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values  #### `flow.collective.send_recv` (Flow::CollectiveSendRecvOp)  _Performs a grouped send and receive operation_   Syntax:  <pre><code>operation ::= `flow.collective.send_recv` $element_type `,` $target `,` $source `,` $channel `,` $send `,` $recv `:`\n              `(` type($target) `,` type($source) `,` type($channel) `,` type($send) `,` type($recv) `)` `-&gt;`\n              custom&lt;ShapedTiedResult&gt;(type($result), $target_dims, $tied_operands)\n              attr-dict-with-keyword\n</code></pre>  The operation sends data to the rank specificied by send     and receives data from the rank specified by recv. If send is -1, this rank     will not send any data. If recv is -1, this rank will not receive any data     and the output will be all zeros. Interfaces: InferTypeOpInterface, TiedOpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>element_type</code>::mlir::iree_compiler::IREE::Flow::CollectiveElementTypeAttrvalid CollectiveElementType{{% markdown %}}Enum cases: * si8 (`Sint8`) * ui8 (`Uint8`) * si16 (`Sint16`) * ui16 (`Uint16`) * si32 (`Sint32`) * ui32 (`Uint32`) * si64 (`Sint64`) * ui64 (`Uint64`) * f16 (`Float16`) * f32 (`Float32`) * f64 (`Float64`) * bf16 (`BFloat16`){{% /markdown %}} <code>tied_operands</code>::mlir::ArrayAttr64-bit integer array attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `target` | ranked tensor of any type values | `target_dims` | index | `source` | ranked tensor of any type values | `channel` | a collecive communication channel | `send` | index | `recv` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values   ### Dispatch ops    #### `flow.dispatch` (Flow::DispatchOp)  _A dispatch of workgroups across a grid_   Syntax:  <pre><code>operation ::= `flow.dispatch` $entry_point\n              (`[` $workload^ `]`)? ``\n              `(` $arguments `)` attr-dict `:`\n              custom&lt;ShapedFunctionType&gt;(ref($arguments),\n              type($arguments), $argument_dims,\n              type($results), $result_dims,\n              $tied_operands)\n</code></pre>  Dispatches workgroups across an grid defined by the captured workload parameters carrying the information required to compute the workgroup count at runtime. The function for converting the workload into a 3D workgroup count is attached to the dispatch entry point and may contain arbitrary host logic.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments  Interfaces: ConditionallySpeculatable, FLOW_StreamableOp, NoMemoryEffect (MemoryEffectOpInterface), SymbolUserOpInterface, TiedOpInterface, Util_ShapeAwareOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>entry_point</code>::mlir::SymbolRefAttrsymbol reference attribute <code>tied_operands</code>::mlir::ArrayAttr64-bit integer array attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `workload` | index | `arguments` | any type | `argument_dims` | index | `result_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | any type   ### Executable ops  Executables for outlined regions.  #### `flow.executable_end` (Flow::ExecutableEndOp)  _Terminator pseudo-op for the executable op_   Syntax:  <pre><code>operation ::= `flow.executable_end` attr-dict\n</code></pre>   Traits: HasParent, Terminator  #### `flow.executable.export` (Flow::ExecutableExportOp)  _Defines an executable entry point for dispatch operations_   Syntax:  <pre><code>operation ::= `flow.executable.export` custom&lt;SymbolVisibility&gt;($sym_visibility)\n              custom&lt;SymbolAlias&gt;($sym_name, $function_ref)\n              custom&lt;WorkgroupCountRegion&gt;($workgroup_count)\n              attr-dict-with-keyword\n</code></pre>  Specifies an exported function with an externally-visible alias. Multiple exports can reference the same internal function.  Each entry point can have a unique workgroup count calculation region. This region takes the workload parameters passed to each flow.dispatch and produces an XYZ workgroup count for the 3D grid dispatch.  Traits: HasParent, IsolatedFromAbove  Interfaces: Symbol  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>sym_name</code>::mlir::StringAttrstring attribute <code>function_ref</code>::mlir::FlatSymbolRefAttrflat symbol reference attribute   #### `flow.executable` (Flow::ExecutableOp)  _Generic executable module_   Syntax:  <pre><code>operation ::= `flow.executable` custom&lt;SymbolVisibility&gt;($sym_visibility)\n              $sym_name\n              attr-dict-with-keyword\n              regions\n</code></pre>  An executable module containing one or more public functions. The contents of the functions are safe to dispatch and can be lowered further to target-specific backend IR representations.  Traits: IsolatedFromAbove, SingleBlock, SingleBlockImplicitTerminator, SymbolTable  Interfaces: Symbol  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>sym_name</code>::mlir::StringAttrstring attribute    ### Partitioned region ops    #### `flow.dispatch.region` (Flow::DispatchRegionOp)  _A group of ops_  This op is a container/grouping of ops. It represents a fusion group before being lowered to a dispatch region. Ops are collected inside of the region body of the op. Values from parent regions can be captured. Results are yielded with a `return` terminator and returned from this op.  `dispatch.region` ops are lowered to `dispatch.workgroups` ops. Workgroups isolated from above. `dispatch.region` ops are a more lightweight abstraction for implementing fusion heuristics, i.e., the process of deciding which ops should form a dispatch region.  This op also has a second region: `workload_count`. The arguments to the region represent the workload for the dispatch, and returns the number of workgroups for the dispatch. The region is lowered directly to `workload_count` region of `dispatch.workgroups`.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), Util_ShapeAwareOp  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `result_dims` | index | `workload` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | any type  #### `flow.dispatch.tensor.load` (Flow::DispatchTensorLoadOp)  _Loads a tensor from a dispatch input placeholder_   Syntax:  <pre><code>operation ::= `flow.dispatch.tensor.load` $source\n              `,` `offsets` `=` custom&lt;DynamicIndexList&gt;(\n              $offsets, $static_offsets)\n              `,` `sizes` `=` custom&lt;DynamicIndexList&gt;(\n              $sizes, $static_sizes)\n              `,` `strides` `=` custom&lt;DynamicIndexList&gt;(\n              $strides, $static_strides)\n              attr-dict `:` type($source) (`{` $source_dims^ `}`)?  `-&gt;` type($result)\n</code></pre>  Loads an input tensor or subtensor from an input placeholder. As each workgroup executes concurrently all workgroups will receive identical loaded results of regions that may overlap.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OffsetSizeAndStrideOpInterface, ReifyRankedShapedTypeOpInterface, TiedOpInterface, Util_ShapeAwareOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>static_offsets</code>::mlir::DenseI64ArrayAttri64 dense array attribute <code>static_sizes</code>::mlir::DenseI64ArrayAttri64 dense array attribute <code>static_strides</code>::mlir::DenseI64ArrayAttri64 dense array attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | dispatch.tensor | `source_dims` | index | `offsets` | index | `sizes` | index | `strides` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values  #### `flow.dispatch.tensor.store` (Flow::DispatchTensorStoreOp)  _Stores a tensor into a dispatch output placeholder_   Syntax:  <pre><code>operation ::= `flow.dispatch.tensor.store` $value `,` $target\n              `,` `offsets` `=` custom&lt;DynamicIndexList&gt;(\n              $offsets, $static_offsets)\n              `,` `sizes` `=` custom&lt;DynamicIndexList&gt;(\n              $sizes, $static_sizes)\n              `,` `strides` `=` custom&lt;DynamicIndexList&gt;(\n              $strides, $static_strides)\n              attr-dict `:` type($value) `-&gt;` type($target) (`{` $target_dims^ `}`)?\n</code></pre>  Stores a tensor or subtensor into an output tensor placeholder. As each workgroup executes concurrently behavior is undefined if more than one workgroup stores into overlapping regions of the full output tensor.  Traits: AttrSizedOperandSegments  Interfaces: OffsetSizeAndStrideOpInterface, Util_ShapeAwareOp  ##### Attributes:   AttributeMLIR TypeDescription <code>static_offsets</code>::mlir::DenseI64ArrayAttri64 dense array attribute <code>static_sizes</code>::mlir::DenseI64ArrayAttri64 dense array attribute <code>static_strides</code>::mlir::DenseI64ArrayAttri64 dense array attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | ranked tensor of any type values | `target` | dispatch.tensor | `target_dims` | index | `offsets` | index | `sizes` | index | `strides` | index  #### `flow.dispatch.tie_shape` (Flow::DispatchTieShapeOp)  _Ties a runtime shape to a dispatch I/O argument_   Syntax:  <pre><code>operation ::= `flow.dispatch.tie_shape` $operand attr-dict\n              `:` type($result) (`{` $dynamic_dims^ `}`)?\n</code></pre>  Metadata op used to tie a runtime-computed shape with dynamic dimensions to a dispatch input/output argument. All uses of the argument should use the pass-through result of this op to allow for SSA-based shape resolution.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), ReifyRankedShapedTypeOpInterface, Util_ShapeAwareOp  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | dispatch.tensor | `dynamic_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | dispatch.tensor  #### `flow.dispatch.workgroup.count` (Flow::DispatchWorkgroupCountOp)  _Returns the total workgroup count of the grid_   Syntax:  <pre><code>operation ::= `flow.dispatch.workgroup.count` `[` $dimension `]` attr-dict `:` type($result)\n</code></pre>  The total number of workgroups along each dimension in the dispatch grid.  Represented as a 3D grid classically written as XYZ. Corresponds to the `NumWorkgroups` SPIR-V built-in and the `gridDim` CUDA built-in variable.  <pre><code>%x = flow.dispatch.workgroup.count[0] : index\n%y = flow.dispatch.workgroup.count[1] : index\n%z = flow.dispatch.workgroup.count[2] : index\n</code></pre>  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>dimension</code>::mlir::IntegerAttrindex attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index  #### `flow.dispatch.workgroup.id` (Flow::DispatchWorkgroupIDOp)  _Returns the index of the current workgroup in the grid_   Syntax:  <pre><code>operation ::= `flow.dispatch.workgroup.id` `[` $dimension `]` attr-dict `:` type($result)\n</code></pre>  The global workgroup ID of the current workgroup in the range of `[0, flow.dispatch.workgroup.count)` along each dimension.  Represented as a 3D grid classically written as XYZ. Corresponds to the `WorkgroupId` SPIR-V built-in and the `blockIdx` CUDA built-in variable.  <pre><code>%x = flow.dispatch.workgroup.id[0] : index\n%y = flow.dispatch.workgroup.id[1] : index\n%z = flow.dispatch.workgroup.id[2] : index\n</code></pre>  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>dimension</code>::mlir::IntegerAttrindex attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index  #### `flow.dispatch.workgroup.size` (Flow::DispatchWorkgroupSizeOp)  _Returns the size of each workgroup in invocations_   Syntax:  <pre><code>operation ::= `flow.dispatch.workgroup.size` `[` $dimension `]` attr-dict `:` type($result)\n</code></pre>  The number of local invocations within the current workgroup along each dimension. Depending on backend this may map to the SIMT thread count or inner loop nest parameters.  Workgroup sizes are not determined at the flow dialect level as they are dependent on the target backend determined when lowering into the HAL. It's still possible to use the symbolic workgroup size inside of dispatch executables as a placeholder for the resolved value once in the HAL.  Represented as a 3D grid classically written as XYZ. Corresponds to the `WorkgroupSize` SPIR-V built-in and the `blockDim` CUDA built-in variable.  <pre><code>%x = flow.dispatch.workgroup.size[0] : index\n%y = flow.dispatch.workgroup.size[1] : index\n%z = flow.dispatch.workgroup.size[2] : index\n</code></pre>  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>dimension</code>::mlir::IntegerAttrindex attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index  #### `flow.dispatch.workgroups` (Flow::DispatchWorkgroupsOp)  _A dispatch of workgroups across a 3-dimensional grid_   Syntax:  <pre><code>operation ::= `flow.dispatch.workgroups` (`[` $workload^ `]`)? ``\n              `(` $arguments `)` `:`\n              custom&lt;ShapedFunctionType&gt;(ref($arguments),\n              type($arguments), $argument_dims,\n              type($results), $result_dims,\n              $tied_operands)\n              attr-dict-with-keyword\n              `=` `\\n` ` ` ` ` ` `\n              custom&lt;DispatchWorkgroupBody&gt;(ref(type($arguments)),\n              ref(type($results)),\n              $workgroup_body)\n              `` custom&lt;DispatchWorkgroupsCountRegion&gt;($workgroup_count)\n</code></pre>  Dispatches some number of workgroups across a 3-dimensional grid. The body region will be invoked for each workgroup with a unique `flow.dispatch.workgroup.id` in the range of `[0, flow.dispatch.workgroup.count)` (along each dimension XYZ).  From the outside the dispatch operation has value semantics: some tensors (and optionally other primitive types) are consumed and one or more new result tensors are produced. Inside each workgroup, however, the input and output tensors are available for arbitrary loads and stores. In many cases each workgroup will load some particular tile(s) from the input tensors and store some particular tile(s) to the output tensors unique to that workgroup. Though it's possible for multiple workgroups to load the same regions of the input tensors behavior is undefined if multiple workgroups store to the same regions of the output tensors.  Though the representation is similar to the GPU-style grid dispatch model here we still have not yet allocated buffers, determined the target device for execution, or even completed fully resolving shapes/types/etc. Because of this it's important that the workgroup body use the `flow.dispatch.workgroup.*` ops to query the workgroup ID/count/size instead of hardcoding them to a particular set of values. Assume that any workgroup dispatch may end up being specialized for several different target devices and even several different variants for a particular target device (differing workgroup sizes, etc).  Because at this point in the layering devices have not yet been selected the workgroup count cannot be fully evaluated. Instead workload parameters are captured that are then passed to a function that when later evaluated computes the actual workgroup count based on target information. The workload is not limited to the 3D XYZ grid dispatch of the workgroup count and can contain any number of parameters used to compute it.  <pre><code>%r = flow.dispatch.workgroups[%c5, %c5](%0, %1)\n    : (tensor&lt;5x5xf32&gt;, tensor&lt;5xf32&gt;) -&gt; tensor&lt;5x5xf32&gt; =\n          (%arg0: !flow.dispatch.tensor&lt;readonly:tensor&lt;5x5xf32&gt;&gt;,\n           %arg1: !flow.dispatch.tensor&lt;readonly:tensor&lt;5xf32&gt;&gt;,\n           %arg2: !flow.dispatch.tensor&lt;writeonly:tensor&lt;5x5xf32&gt;&gt;) {\n  ...\n}\n</code></pre>  The number of results of the operation is equal to the number of results in the type signature (`(tensor&lt;5x5xf32&gt;, tensor&lt;5xf32&gt;) -&gt; tensor&lt;5x5xf32&gt;`). Each tensor argument and result in the type signature has a corresponding block argument of type `!flow.dispatch.tensor`. Furthermore, each argument has a corresponding `arguments` operand.  There are no `arguments` operands for results, but a result can be tied an argument by writing the argument operand's SSA value instead of its type: E.g., in the above example, `-&gt; %0` would tie the first argument to the result. In that case, there would be no separate block argument for the result.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments, IsolatedFromAbove  Interfaces: ClosureOpInterface, ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), TiedOpInterface, Util_ShapeAwareOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>tied_operands</code>::mlir::ArrayAttr64-bit integer array attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `workload` | index | `arguments` | any type | `argument_dims` | index | `result_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | any type  #### `flow.return` (Flow::ReturnOp)  _Return from a flow.dispatch_region_   Syntax:  <pre><code>operation ::= `flow.return` attr-dict ($operands^ `:` type($operands))?\n</code></pre>  Returns the given values from the region and back to the host code.  Traits: AlwaysSpeculatableImplTrait, ReturnLike, Terminator  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), RegionBranchTerminatorOpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operands` | any type   ### Streamable call ops    #### `flow.call` (Flow::CallOp)  _Calls a streamable external host function_   Syntax:  <pre><code>operation ::= `flow.call` $callee\n              `(` $arguments `)` attr-dict `:`\n              custom&lt;ShapedFunctionType&gt;(ref($arguments),\n              type($arguments), $argument_dims,\n              type($results), $result_dims,\n              $tied_operands)\n</code></pre>  Calls a function taking/returning tensor values with stream semantics. Tensors have their shapes captured and may be tied to denote in-place operations. Asynchronous calls must have no side-effects.  Note that returned tensors must have their shapes declared prior to the call as this is what allows the call to be made on the stream. If external host logic is required to compute the shape (avoid at all costs!) a separate func.call can be used outside of the stream to do so. If shapes are unknowable until the operation is performed it should be made as a normal asynchronous host call with 'coarse-fences' instead.  Traits: AttrSizedOperandSegments  Interfaces: CallOpInterface, FLOW_StreamableOp, SymbolUserOpInterface, TiedOpInterface, Util_ShapeAwareOp  ##### Attributes:   AttributeMLIR TypeDescription <code>callee</code>::mlir::FlatSymbolRefAttrflat symbol reference attribute <code>tied_operands</code>::mlir::ArrayAttr64-bit integer array attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `arguments` | any type | `argument_dims` | index | `result_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | any type  #### `flow.func` (Flow::FuncOp)  _Streamable function declaration_   Syntax:  <pre><code>operation ::= `flow.func` custom&lt;SymbolVisibility&gt;($sym_visibility)\n              $sym_name\n              ``\n              custom&lt;ShapedFunctionSignature&gt;($function_type,\n              $tied_operands,\n              $arg_attrs,\n              $res_attrs)\n              attr-dict-with-keyword\n              ($body^)?\n</code></pre>  Declares a function that can be called as an asynchronous streaming operation via `flow.call`. Today only external functions are allowed.  Traits: IsolatedFromAbove  Interfaces: CallableOpInterface, FunctionOpInterface, Symbol  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_name</code>::mlir::StringAttrstring attribute <code>function_type</code>::mlir::TypeAttrtype attribute of function type <code>tied_operands</code>::mlir::ArrayAttr64-bit integer array attribute <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>arg_attrs</code>::mlir::ArrayAttrArray of dictionary attributes <code>res_attrs</code>::mlir::ArrayAttrArray of dictionary attributes    ### Tensor ops    #### `flow.dispatch.workgroup_count_from_dag_root` (Flow::DispatchWorkgroupCountFromDagRootOp)  _     workgroup count computed based on iteration range of the root of the DAG     for ops within the dispatch.   _   Syntax:  <pre><code>operation ::= `flow.dispatch.workgroup_count_from_dag_root` attr-dict $operands\n</code></pre>  When using tile + distribution of the root of the DAG (Directed Acyclic Graph) of ops within the dispatch to split the work amongst workgroups. The workload captured is the size of the iteration space of the root of the DAG. This op represents the computation that given the workload returns the number of workgroups to use. The backends are responsible for lowering this op into actual computation (typically based on the tile sizes used to tile and distribute the root of the DAG).  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operands` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `x` | index | `y` | index | `z` | index  #### `flow.dispatch.workgroup_count_from_slice` (Flow::DispatchWorkgroupCountFromSliceOp)  _     Place holder to signify default workgroup count calculation.   _   Syntax:  <pre><code>operation ::= `flow.dispatch.workgroup_count_from_slice` attr-dict $operands\n</code></pre>  The default computation of the number of workgroups (or workgroup count) assumes that the dispatch + captured values is enough to compute the workgroup count. It does so by using a program slice of the values within the dispatch that represent the number of workgroups when available within the dispatch. Currently the arguments of index types captured by the `flow.dispatch.workgroups` is treated as the workload for the operation. It is a requirement that the slice of the program that computes the number of workgroups will need to have its leaves be these captured values.  TODO: This could be generalized in future to allow the slices to encompass arbitrary computation. The computation of the workgroup count can then be done on the device itself, if this is data dependent. In such cases the workload could be more than just values of index types.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operands` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `x` | index | `y` | index | `z` | index  #### `flow.dispatch.workload.ordinal` (Flow::DispatchWorkloadOrdinalOp)  _     Annotates the values captured as workload within the body of     `flow.dispatch.workgroups` op.   _   Syntax:  <pre><code>operation ::= `flow.dispatch.workload.ordinal` attr-dict $operand `,` $ordinal `:` type($operand)\n</code></pre>  The arguments that represent the captured/returned values of the `flow.dispatch.workgroups, i.e. the signature of the body of the op is not preserved during IREEs compilation. Since the workloads are derived from the operands captured by the operation, this op denotes the values captured as workloads. This can be used in the backends to map back to the workload values while materializing the workgroup count computation.  TODO: Find a better way to represent this information, either by somehow propagating the signature of the created dispatch workgroup op through the compilation stack until the codegen backends, or as a separate list/attribute that can be plumbed through without using explicit ops.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>ordinal</code>::mlir::IntegerAttrindex attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index  #### `flow.tensor.alloca` (Flow::TensorAllocaOp)  _An empty tensor allocation with undefined contents_   Syntax:  <pre><code>operation ::= `flow.tensor.alloca` `:` type($result) (`{` $result_dims^ `}`)?\n              attr-dict-with-keyword\n</code></pre>  Returns a new transient tensor allocation with undefined contents. Subsequent writes must populate any ranges of the tensor that are later read. The resulting tensor may be long-lived and allocated as part of a dedicated allocation. Prefer using `flow.tensor.empty` whenever possible as this op disables nearly all allocation-related optimizations performed by the compiler. The presence of this op is often an indication of an improper lowering.  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), Util_ShapeAwareOp  Effects: MemoryEffects::Effect{MemoryEffects::Allocate on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `result_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values  #### `flow.tensor.clone` (Flow::TensorCloneOp)  _Performs a full tensor clone operation_   Syntax:  <pre><code>operation ::= `flow.tensor.clone` $operand `:` type($result) (`{` $argument_dims^ `}`)?\n              attr-dict-with-keyword\n</code></pre>  Clones the input tensor into an identical output tensor.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, FLOW_StreamableOp, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), Util_ShapeAwareOp  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | ranked tensor of any type values | `argument_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values  #### `flow.tensor.constant` (Flow::TensorConstantOp)  _Tensor constant that can have dynamic dimensions_   Syntax:  <pre><code>operation ::= `flow.tensor.constant` $value attr-dict `-&gt;` type($result)\n</code></pre>  Allows specifying a constant where the return value can erase shape information. This operation is declared as having side effects and has no folder, so will not be optimized away by the compiler. The underlying shape information should be hidden from the compiler and resolved at runtime.  <pre><code>%c = flow.tensor.constant tensor&lt;2x2xf32&gt; -&gt; tensor&lt;?x?xf32&gt;\n%res = math.absf %c : tensor&lt;?x?xf32&gt;\n</code></pre>  ##### Attributes:   AttributeMLIR TypeDescription <code>value</code>::mlir::ElementsAttrconstant vector/tensor attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | tensor of any type values  #### `flow.tensor.empty` (Flow::TensorEmptyOp)  _An empty tensor carrying metadata but no contents_   Syntax:  <pre><code>operation ::= `flow.tensor.empty` `:` type($result) (`{` $result_dims^ `}`)?\n              attr-dict-with-keyword\n</code></pre>  Returns a tensor with undefined contents. Subsequent writes must populate any ranges of the tensor that are later read.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, FLOW_StreamableOp, NoMemoryEffect (MemoryEffectOpInterface), Util_ShapeAwareOp  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `result_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values  #### `flow.tensor.load` (Flow::TensorLoadOp)  _Loads a value from a tensor element_   Syntax:  <pre><code>operation ::= `flow.tensor.load` $source (`[` $indices^ `]`)? `:`\n              type($source) (`{` $source_dims^ `}`)?\n              attr-dict-with-keyword\n</code></pre>  Returns the element at the given location from within the tensor.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), Util_ShapeAwareOp  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | ranked tensor of any type values | `source_dims` | index | `indices` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index or signless integer or floating-point or complex-type or vector of any type values  #### `flow.tensor.reshape` (Flow::TensorReshapeOp)  _Reshapes a tensor_   Syntax:  <pre><code>operation ::= `flow.tensor.reshape` $source `:`\n              type($source) (`{` $source_dims^ `}`)? `-&gt;`\n              type($result) (`{` $result_dims^ `}`)?\n              attr-dict-with-keyword\n</code></pre>  Reshapes a tensor to a new shape without modifying the contents.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments  Interfaces: ConditionallySpeculatable, FLOW_StreamableOp, NoMemoryEffect (MemoryEffectOpInterface), TiedOpInterface, Util_ShapeAwareOp  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | ranked tensor of any type values | `source_dims` | index | `result_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values  #### `flow.tensor.slice` (Flow::TensorSliceOp)  _Slices out a subregion of a tensor_   Syntax:  <pre><code>operation ::= `flow.tensor.slice` $source `[` $start_indices `for` $lengths `]` `:`\n              type($source) (`{` $source_dims^ `}`)? `-&gt;`\n              type($result) (`{` $result_dims^ `}`)?\n              attr-dict-with-keyword\n</code></pre>  Clones a subregion of a tensor.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments  Interfaces: ConditionallySpeculatable, FLOW_StreamableOp, NoMemoryEffect (MemoryEffectOpInterface), Util_ShapeAwareOp  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | ranked tensor of any type values | `source_dims` | index | `start_indices` | index | `lengths` | index | `result_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values  #### `flow.tensor.splat` (Flow::TensorSplatOp)  _Splats a value into a shaped tensor_   Syntax:  <pre><code>operation ::= `flow.tensor.splat` $value `:` type($result) (`{` $result_dims^ `}`)?\n              attr-dict-with-keyword\n</code></pre>  Returns a tensor initialized to the given primitive value.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, FLOW_StreamableOp, NoMemoryEffect (MemoryEffectOpInterface), Util_ShapeAwareOp  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | index or signless integer or floating-point or complex-type | `result_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values  #### `flow.tensor.store` (Flow::TensorStoreOp)  _Stores a value into a tensor element_   Syntax:  <pre><code>operation ::= `flow.tensor.store` $value `,` $target (`[` $indices^ `]`)? `:`\n              type($target) (`{` $target_dims^ `}`)?\n              attr-dict-with-keyword\n</code></pre>  Returns a tensor with the element at the given index set to the given value.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), Util_ShapeAwareOp  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | index or signless integer or floating-point or complex-type or vector of any type values | `target` | ranked tensor of any type values | `target_dims` | index | `indices` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values  #### `flow.tensor.tie_shape` (Flow::TensorTieShapeOp)  _Ties a runtime shape to a tensor value_   Syntax:  <pre><code>operation ::= `flow.tensor.tie_shape` $operand attr-dict\n              `:` type($result) (`{` $dynamic_dims^ `}`)?\n</code></pre>  Metadata op used to tie tensors with their runtime-computed dynamic dimensions. This only exists transiently in the IR as a witness to shape calculations and is removed during lowering.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), ReifyRankedShapedTypeOpInterface, Util_ShapeAwareOp  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | ranked tensor of any type values | `dynamic_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values  #### `flow.tensor.trace` (Flow::TensorTraceOp)  _Trace value(s) operation_   Syntax:  <pre><code>operation ::= `flow.tensor.trace` attr-dict ($operands^ `:` type($operands))?\n</code></pre>  Traces out to a runtime trace sink (console, log file, etc) the given tensors and titles them with the given key. The key is informational only and useful for titling/marking specific sets of tensors for easier searching.  ##### Attributes:   AttributeMLIR TypeDescription <code>key</code>::mlir::StringAttrstring attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operands` | ranked tensor of any type values  #### `flow.tensor.update` (Flow::TensorUpdateOp)  _Updates a tensor with the contents of another tensor_   Syntax:  <pre><code>operation ::= `flow.tensor.update` $update `,` $target `[` $start_indices `]` `:`\n              type($update) (`{` $update_dims^ `}`)? `-&gt;`\n              custom&lt;ShapedTiedResult&gt;(type($result), $target_dims)\n              attr-dict-with-keyword\n</code></pre>  Updates the target tensor with the contents of the update tensor at the given offset indices.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments  Interfaces: ConditionallySpeculatable, FLOW_StreamableOp, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), TiedOpInterface, Util_ShapeAwareOp  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `target` | ranked tensor of any type values | `target_dims` | index | `start_indices` | index | `update` | ranked tensor of any type values | `update_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values   ## Attribute definition  ### DummyAttr    Syntax: `#flow.dummy`   ## Type constraint definition  ### dispatch.tensor  A placeholder for a dispatch region input/output operand. This can be used to query the metadata about the tensor (such as its shape) as well as both load and store from the backing tensor representation.   ### dispatch.tensor  A placeholder for a dispatch region input operand. This can be used to query the metadata about the tensor (such as its shape) as well as load from the backing tensor representation.   ### dispatch.tensor  A placeholder for a dispatch region output operand. This can be used to query the metadata about the tensor (such as its shape) as well as store to the backing tensor representation.   ## Type definition  ### ChannelType  a collecive communication channel  Syntax: `!flow.channel`  Represents a single participant in a collective clique. Multiple channels may exist within the same program to allow for partial operations or hierarchical operations.  In programs that have already been partitioned prior to being compiled there will often exist only one channel and `flow.channel.default` can be used to reference it. In programs that model SPMD behavior internally channels can be created or provided by hosting applications.  ### DummyType    Syntax: `!flow.dummy`"},{"location":"reference/mlir-dialects/HAL/","title":"HAL","text":""},{"location":"reference/mlir-dialects/HAL/#hal-dialect","title":"'hal' Dialect","text":"<p>A dialect representing operations against the IREE HAL.</p> <p>This can be thought of as a Vulkan-like model with all of the graphics bits chopped out.</p> <p>The type set is limited to those that can be represented in the IREE HAL design: buffers and views, synchronization primitives like semaphores, and and command buffers. The intent is that if a device could implement the HAL interface the sequencer ops could run on that device, such as being able to run on a GPU via indirect command buffers.</p> <p>Though this is mostly a 1:1 mapping to the iree::hal API there are some methods omitted as they are not likely to be needed in IR. It's assumed that either sequencer interfaces will encapsulate the logic (such as device resolution) or that certain features are unsafe to expose to user-defined input.</p> <ul> <li>'hal' Dialect<ul> <li>Operation definition<ul> <li>Allocator ops<ul> <li>hal.allocator.allocate (HAL::AllocatorAllocateOp)</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/mlir-dialects/HAL/#operation-definition","title":"Operation definition","text":""},{"location":"reference/mlir-dialects/HAL/#allocator-ops","title":"Allocator ops","text":"<p>Ops for <code>!hal.allocator</code> / <code>iree_hal_allocator_t</code>.</p>"},{"location":"reference/mlir-dialects/HAL/#halallocatorallocate-halallocatorallocateop","title":"<code>hal.allocator.allocate</code> (HAL::AllocatorAllocateOp)","text":"<p>Empty buffer allocation operation</p> <p>Syntax:</p> <pre><code>operation ::= `hal.allocator.allocate` `&lt;` $allocator `:` type($allocator) `&gt;`\n              `affinity` `(` $queue_affinity `)`\n              `type` `(` $memory_types `)`\n              `usage` `(` $buffer_usage `)`\n              `:` custom&lt;SizeAwareType&gt;(type($result), $result_size)\n              attr-dict-with-keyword\n</code></pre> <p>Allocates a buffer of the given size from the allocator. The size of the buffer returned may be larger than the requested size if the allocator has specific alignment requirements or minimum allocation sizes.</p> <p>Interfaces: OpAsmOpInterface, SizeAwareOpInterface</p>"},{"location":"reference/mlir-dialects/HAL/#attributes","title":"Attributes:","text":"AttributeMLIR TypeDescription <code>memory_types</code>mlir::iree_compiler::IREE::HAL::MemoryTypeBitfieldAttrvalid MemoryType{{% markdown %}}Enum cases: * None (`None`) * Optimal (`Optimal`) * HostVisible (`HostVisible`) * HostCoherent (`HostCoherent`) * HostCached (`HostCached`) * HostLocal (`HostLocal`) * DeviceVisible (`DeviceVisible`) * DeviceLocal (`DeviceLocal`){{% /markdown %}} <code>buffer_usage</code>mlir::iree_compiler::IREE::HAL::BufferUsageBitfieldAttrvalid BufferUsage{{% markdown %}}Enum cases: * None (`None`) * TransferSource (`TransferSource`) * TransferTarget (`TransferTarget`) * Transfer (`Transfer`) * DispatchIndirectParams (`DispatchIndirectParams`) * DispatchUniformRead (`DispatchUniformRead`) * DispatchStorageRead (`DispatchStorageRead`) * DispatchStorageWrite (`DispatchStorageWrite`) * DispatchStorage (`DispatchStorage`) * DispatchImageRead (`DispatchImageRead`) * DispatchImageWrite (`DispatchImageWrite`) * DispatchImage (`DispatchImage`) * SharingExport (`SharingExport`) * SharingReplicate (`SharingReplicate`) * SharingConcurrent (`SharingConcurrent`) * SharingImmutable (`SharingImmutable`) * MappingScoped (`MappingScoped`) * MappingPersistent (`MappingPersistent`) * MappingOptional (`MappingOptional`) * MappingAccessRandom (`MappingAccessRandom`) * MappingAccessSequentialWrite (`MappingAccessSequentialWrite`) * Mapping (`Mapping`){{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `allocator` | allocator | `queue_affinity` | 64-bit signless integer | `result_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | buffer  #### `hal.allocator.import` (HAL::AllocatorImportOp)  _Allocator-supported host buffer import operation_   Syntax:  <pre><code>operation ::= `hal.allocator.import` `&lt;` $allocator `:` type($allocator) `&gt;`\n              `source` `(` $source `:` type($source) `)` `` `[` $offset `,` $length `]`\n              `affinity` `(` $queue_affinity `)`\n              `type` `(` $memory_types `)`\n              `usage` `(` $buffer_usage `)`\n              `:` type($did_import) `,` type($result)\n              attr-dict-with-keyword\n</code></pre>  Tries importing host memory backed by the given byte buffer into a device accessible `!hal.buffer`. The returned buffer may be host-only and not directly usable on devices. If the mapping cannot be completed (such as trying to map the host memory as device-local on devices with discrete memory) then `did_import` will indicate that the returned buffer is null.  Interfaces: OpAsmOpInterface, SizeAwareOpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>memory_types</code>mlir::iree_compiler::IREE::HAL::MemoryTypeBitfieldAttrvalid MemoryType{{% markdown %}}Enum cases: * None (`None`) * Optimal (`Optimal`) * HostVisible (`HostVisible`) * HostCoherent (`HostCoherent`) * HostCached (`HostCached`) * HostLocal (`HostLocal`) * DeviceVisible (`DeviceVisible`) * DeviceLocal (`DeviceLocal`){{% /markdown %}} <code>buffer_usage</code>mlir::iree_compiler::IREE::HAL::BufferUsageBitfieldAttrvalid BufferUsage{{% markdown %}}Enum cases: * None (`None`) * TransferSource (`TransferSource`) * TransferTarget (`TransferTarget`) * Transfer (`Transfer`) * DispatchIndirectParams (`DispatchIndirectParams`) * DispatchUniformRead (`DispatchUniformRead`) * DispatchStorageRead (`DispatchStorageRead`) * DispatchStorageWrite (`DispatchStorageWrite`) * DispatchStorage (`DispatchStorage`) * DispatchImageRead (`DispatchImageRead`) * DispatchImageWrite (`DispatchImageWrite`) * DispatchImage (`DispatchImage`) * SharingExport (`SharingExport`) * SharingReplicate (`SharingReplicate`) * SharingConcurrent (`SharingConcurrent`) * SharingImmutable (`SharingImmutable`) * MappingScoped (`MappingScoped`) * MappingPersistent (`MappingPersistent`) * MappingOptional (`MappingOptional`) * MappingAccessRandom (`MappingAccessRandom`) * MappingAccessSequentialWrite (`MappingAccessSequentialWrite`) * Mapping (`Mapping`){{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `allocator` | allocator | `queue_affinity` | 64-bit signless integer | `source` | a reference counted byte buffer | `offset` | index | `length` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `did_import` | 1-bit signless integer | `result` | buffer   ### Buffer ops  Ops for `!hal.buffer` / `iree_hal_buffer_t`.  #### `hal.buffer.assert` (HAL::BufferAssertOp)  _Buffer compatibility assertion_   Syntax:  <pre><code>operation ::= `hal.buffer.assert` `&lt;` $buffer `:` type($buffer) `&gt;`\n              `message` `(` $message `)`\n              `allocator` `(` $allocator `:` type($allocator) `)`\n              `minimum_length` `(` $minimum_length `)`\n              `type` `(` $memory_types `)`\n              `usage` `(` $buffer_usage `)`\n              attr-dict-with-keyword\n</code></pre>  Asserts that the buffer is compatible with the given allocator and usage. Program execution will abort as if `std.assert` had been used.  This only checks that the buffer can be used and not that it matches the given parameters exactly. Buffers may be from other allocators so long as the allocators are compatible (devices can address each other's memory), the type and usage contain all the requested bits (having more bits is ok), and the length is at least the requested minimum (as padding may be ignored).  ##### Attributes:   AttributeMLIR TypeDescription <code>message</code>::mlir::StringAttrstring attribute <code>memory_types</code>mlir::iree_compiler::IREE::HAL::MemoryTypeBitfieldAttrvalid MemoryType{{% markdown %}}Enum cases: * None (`None`) * Optimal (`Optimal`) * HostVisible (`HostVisible`) * HostCoherent (`HostCoherent`) * HostCached (`HostCached`) * HostLocal (`HostLocal`) * DeviceVisible (`DeviceVisible`) * DeviceLocal (`DeviceLocal`){{% /markdown %}} <code>buffer_usage</code>mlir::iree_compiler::IREE::HAL::BufferUsageBitfieldAttrvalid BufferUsage{{% markdown %}}Enum cases: * None (`None`) * TransferSource (`TransferSource`) * TransferTarget (`TransferTarget`) * Transfer (`Transfer`) * DispatchIndirectParams (`DispatchIndirectParams`) * DispatchUniformRead (`DispatchUniformRead`) * DispatchStorageRead (`DispatchStorageRead`) * DispatchStorageWrite (`DispatchStorageWrite`) * DispatchStorage (`DispatchStorage`) * DispatchImageRead (`DispatchImageRead`) * DispatchImageWrite (`DispatchImageWrite`) * DispatchImage (`DispatchImage`) * SharingExport (`SharingExport`) * SharingReplicate (`SharingReplicate`) * SharingConcurrent (`SharingConcurrent`) * SharingImmutable (`SharingImmutable`) * MappingScoped (`MappingScoped`) * MappingPersistent (`MappingPersistent`) * MappingOptional (`MappingOptional`) * MappingAccessRandom (`MappingAccessRandom`) * MappingAccessSequentialWrite (`MappingAccessSequentialWrite`) * Mapping (`Mapping`){{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `buffer` | buffer | `allocator` | allocator | `minimum_length` | index  #### `hal.buffer.length` (HAL::BufferLengthOp)  _Buffer byte length accessor_   Syntax:  <pre><code>operation ::= `hal.buffer.length` `&lt;` $buffer `:` type($buffer) `&gt;`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns the allocated size of a buffer in bytes. May be less than the underlying buffer allocation if this is a subspan or view into another buffer.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `buffer` | buffer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index  #### `hal.buffer.load` (HAL::BufferLoadOp)  _Buffer element load operation_   Syntax:  <pre><code>operation ::= `hal.buffer.load` `&lt;` $source_buffer `:` type($source_buffer) `&gt;`\n              `` `[` $source_offset `]`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Loads a value from a buffer by mapping it.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source_buffer` | buffer | `source_offset` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index or signless integer or floating-point or complex-type or vector of any type values  #### `hal.buffer.store` (HAL::BufferStoreOp)  _Buffer element store operation_   Syntax:  <pre><code>operation ::= `hal.buffer.store` `&lt;` $target_buffer `:` type($target_buffer) `&gt;`\n              `` `[` $target_offset `]`\n              `value` `(` $value `:` type($value) `)`\n              attr-dict-with-keyword\n</code></pre>  Stores a value into a buffer by mapping it.  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | index or signless integer or floating-point or complex-type or vector of any type values | `target_buffer` | buffer | `target_offset` | index  #### `hal.buffer.subspan` (HAL::BufferSubspanOp)  _Buffer subspan operation_   Syntax:  <pre><code>operation ::= `hal.buffer.subspan` `&lt;` $source_buffer `:` type($source_buffer) `&gt;`\n              `` `[` $source_offset `,` $length `]`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns a reference to a subspan of the buffer.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, SizeAwareOpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source_buffer` | buffer | `source_offset` | index | `length` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | buffer   ### Buffer view ops  Ops for `!hal.buffer_view` / `iree_hal_buffer_view_t`.  #### `hal.buffer_view.assert` (HAL::BufferViewAssertOp)  _Buffer view contents assertion_   Syntax:  <pre><code>operation ::= `hal.buffer_view.assert` `&lt;` $buffer_view `:` type($buffer_view) `&gt;`\n              `message` `(` $message `)`\n              `shape` `(` `[` $shape `]` `)`\n              `type` `(` $element_type `)`\n              `encoding` `(` $encoding_type `)`\n              attr-dict-with-keyword\n</code></pre>  Asserts that the buffer view contains a data compatible tensor with the given encoding. Program execution will abort as if `std.assert` had been used.  ##### Attributes:   AttributeMLIR TypeDescription <code>message</code>::mlir::StringAttrstring attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `buffer_view` | buffer_view | `element_type` | 32-bit signless integer | `encoding_type` | 32-bit signless integer | `shape` | index  #### `hal.buffer_view.buffer` (HAL::BufferViewBufferOp)  _Buffer view buffer accessor_   Syntax:  <pre><code>operation ::= `hal.buffer_view.buffer` `&lt;` $buffer_view `:` type($buffer_view) `&gt;`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns the buffer backing this view's contents.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `buffer_view` | buffer_view  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | buffer  #### `hal.buffer_view.create` (HAL::BufferViewCreateOp)  _Buffer view reference initializer_   Syntax:  <pre><code>operation ::= `hal.buffer_view.create` `buffer` `(` $source_buffer `:` type($source_buffer) `)`\n              `` `[` $source_offset `,` $source_length `]`\n              `shape` `(` `[` $shape `]` `)`\n              `type` `(` $element_type `)`\n              `encoding` `(` $encoding_type `)`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Creates a reference to a buffer with a particular shape and element type. The buffer is not copied and both the original and view references must be synchronized. This makes it easier to associate commonly-carried metadata along with the contents.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source_buffer` | buffer | `source_offset` | index | `source_length` | index | `element_type` | 32-bit signless integer | `encoding_type` | 32-bit signless integer | `shape` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | buffer_view  #### `hal.buffer_view.dim` (HAL::BufferViewDimOp)  _Buffer view dimension value query_   Syntax:  <pre><code>operation ::= `hal.buffer_view.dim` `&lt;` $buffer_view `:` type($buffer_view) `&gt;`\n              `` `[` $index `]`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns the value of the given dimension.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>index</code>::mlir::IntegerAttrindex attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `buffer_view` | buffer_view  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index  #### `hal.buffer_view.element_type` (HAL::BufferViewElementTypeOp)  _Buffer view element type query_   Syntax:  <pre><code>operation ::= `hal.buffer_view.element_type` `&lt;` $buffer_view `:` type($buffer_view) `&gt;`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns the element type of the buffer view.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `buffer_view` | buffer_view  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `hal.buffer_view.encoding_type` (HAL::BufferViewEncodingTypeOp)  _Buffer view encoding type query_   Syntax:  <pre><code>operation ::= `hal.buffer_view.encoding_type` `&lt;` $buffer_view `:` type($buffer_view) `&gt;`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns the encoding type of the buffer view.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `buffer_view` | buffer_view  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `hal.buffer_view.rank` (HAL::BufferViewRankOp)  _Buffer view rank query_   Syntax:  <pre><code>operation ::= `hal.buffer_view.rank` `&lt;` $buffer_view `:` type($buffer_view) `&gt;`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns the rank of the buffer view.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `buffer_view` | buffer_view  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index  #### `hal.buffer_view.trace` (HAL::BufferViewTraceOp)  _Trace value(s) operation_   Syntax:  <pre><code>operation ::= `hal.buffer_view.trace` $operands `:` type($operands)\n              attr-dict-with-keyword\n</code></pre>  Traces out to a runtime trace sink (console, log file, etc) the given buffer views and titles them with the given key. The key is informational only and useful for titling/marking specific sets of buffers for easier searching.  ##### Attributes:   AttributeMLIR TypeDescription <code>key</code>::mlir::StringAttrstring attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operands` | buffer_view   ### Channel ops  Ops for `!hal.channel` / `iree_hal_channel_t`.  #### `hal.channel.create` (HAL::ChannelCreateOp)  _Creates a new channel for collective communication_   Syntax:  <pre><code>operation ::= `hal.channel.create` `device` `(` $device `:` type($device) `)`\n              `affinity` `(` $queue_affinity `)`\n              `flags` `(` $flags `)`\n              `id` `(` $id `)`\n              `group` `(` $group `)`\n              `rank` `(` $rank `)`\n              `count` `(` $count `)`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns a new channel with the given rank associated with the given device queue. Collective operations using this channel must only be submitted on compatible queues.  The group and ID are optional and may be null. A rank or count of -1 can be used to indicate a default inherited from the environment or device configuration.  Interfaces: OpAsmOpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>flags</code>::mlir::IntegerAttr32-bit signless integer attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `device` | device | `queue_affinity` | 64-bit signless integer | `id` | a reference counted byte buffer | `group` | a reference counted byte buffer | `rank` | 32-bit signless integer | `count` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | collective.channel  #### `hal.channel.rank_and_count` (HAL::ChannelRankAndCountOp)  _Returns the rank of the local participant in the group_   Syntax:  <pre><code>operation ::= `hal.channel.rank_and_count` `&lt;` $channel `:` type($channel) `&gt;`\n              `:` type($rank) `,` type($count)\n              attr-dict-with-keyword\n</code></pre>  Returns the rank the channel represents as a participant in a collective group in `[0, count)` and the total participant count.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `channel` | collective.channel  ##### Results:  | Result | Description | | :----: | ----------- | | `rank` | 32-bit signless integer | `count` | 32-bit signless integer  #### `hal.channel.split` (HAL::ChannelSplitOp)  _Splits a collective communication channel_   Syntax:  <pre><code>operation ::= `hal.channel.split` `&lt;` $channel `:` type($channel) `&gt;`\n              `color` `(` $color `)`\n              `key` `(` $key `)`\n              `flags` `(` $flags `)`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Partitions the group associated with the given channel into disjoint subgroups for each unique value of color. Each new subgroup contains all participants of the same color and within each subgroup the key argument is used to define the rank order. When multiple participants in a group use the same key the tie will be broken using their rank in the parent group. A color of -1 indicates that the rank does not participate in any subgroup and will return a null channel.  Interfaces: OpAsmOpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>flags</code>::mlir::IntegerAttr32-bit signless integer attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `channel` | collective.channel | `color` | 32-bit signless integer | `key` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | collective.channel   ### Command buffer ops  Ops for `!hal.command_buffer` / `iree_hal_command_buffer_t`.  #### `hal.command_buffer.begin_debug_group` (HAL::CommandBufferBeginDebugGroupOp)  _Pushes a command buffer debug group label_   Syntax:  <pre><code>operation ::= `hal.command_buffer.begin_debug_group` `&lt;` $command_buffer `:` type($command_buffer) `&gt;`\n              `label` `(` $label `)`\n              attr-dict-with-keyword\n</code></pre>  Pushes a new debug group with the given label. All commands between this and a mandatory matching call to `hal.command_buffer.end_debug_group` will be grouped together with the given label.  ##### Attributes:   AttributeMLIR TypeDescription <code>label</code>::mlir::StringAttrstring attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `command_buffer` | command_buffer  #### `hal.command_buffer.collective` (HAL::CommandBufferCollectiveOp)  _Command buffer collective dispatch recording operation_   Syntax:  <pre><code>operation ::= `hal.command_buffer.collective` `&lt;` $command_buffer `:` type($command_buffer) `&gt;`\n              `channel` `(` $channel `:` type($channel) `)`\n              `op` `(` $op `)`\n              (`param` `(` $param^ `:` type($param) `)`)?\n              (`send` `(` $send_buffer^ `:` type($send_buffer) `)`\n              `` `[` $send_offset `,` $send_length `]`)?\n              (`recv` `(` $recv_buffer^ `:` type($recv_buffer) `)`\n              `` `[` $recv_offset `,` $recv_length `]`)?\n              `count` `(` $element_count `)`\n              attr-dict-with-keyword\n</code></pre>  Dispatches a collective operation defined by op using the given buffers.  Traits: AttrSizedOperandSegments  ##### Attributes:   AttributeMLIR TypeDescription <code>op</code>::mlir::iree_compiler::IREE::HAL::CollectiveAttrcollective operation and specification{{% markdown %}}     Specifies the collective operation to perform and any mode bits required.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `command_buffer` | command_buffer | `channel` | collective.channel | `element_count` | index | `param` | 32-bit signless integer | `send_buffer` | buffer | `send_offset` | index | `send_length` | index | `recv_buffer` | buffer | `recv_offset` | index | `recv_length` | index  #### `hal.command_buffer.copy_buffer` (HAL::CommandBufferCopyBufferOp)  _Command buffer buffer copy recording operation_   Syntax:  <pre><code>operation ::= `hal.command_buffer.copy_buffer` `&lt;` $command_buffer `:` type($command_buffer) `&gt;`\n              `source` `(` $source_buffer `:` type($source_buffer) `)`\n              `` `[` $source_offset `]`\n              `target` `(` $target_buffer `:` type($target_buffer) `)`\n              `` `[` $target_offset `]`\n              `length` `(` $length `)`\n              attr-dict-with-keyword\n</code></pre>  Copies a range of one buffer to another.  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `command_buffer` | command_buffer | `source_buffer` | buffer | `source_offset` | index | `target_buffer` | buffer | `target_offset` | index | `length` | index  #### `hal.command_buffer.create` (HAL::CommandBufferCreateOp)  _Command buffer allocation operation_   Syntax:  <pre><code>operation ::= `hal.command_buffer.create` `device` `(` $device `:` type($device) `)`\n              `mode` `(` $modes `)`\n              `categories` `(` $command_categories `)`\n              (`bindings` `(` $binding_capacity^ `)`)?\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns a command buffer from the device pool ready to begin recording.  Interfaces: OpAsmOpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>modes</code>mlir::iree_compiler::IREE::HAL::CommandBufferModeBitfieldAttrvalid CommandBufferMode{{% markdown %}}Enum cases: * None (`None`) * OneShot (`OneShot`) * Nested (`Nested`) * AllowInlineExecution (`AllowInlineExecution`){{% /markdown %}} <code>command_categories</code>mlir::iree_compiler::IREE::HAL::CommandCategoryBitfieldAttrvalid CommandCategory{{% markdown %}}Enum cases: * None (`None`) * Transfer (`Transfer`) * Dispatch (`Dispatch`){{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `device` | device | `binding_capacity` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | command_buffer  #### `hal.command_buffer.device` (HAL::CommandBufferDeviceOp)  _Command buffer device query operation_   Syntax:  <pre><code>operation ::= `hal.command_buffer.device` `&lt;` $command_buffer `:` type($command_buffer) `&gt;`\n              `:` type($device)\n              attr-dict-with-keyword\n</code></pre>  Used during conversion to access the device used to create a command buffer.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `command_buffer` | command_buffer  ##### Results:  | Result | Description | | :----: | ----------- | | `device` | device  #### `hal.command_buffer.dispatch.indirect` (HAL::CommandBufferDispatchIndirectOp)  _Command buffer indirect dispatch recording operation_   Syntax:  <pre><code>operation ::= `hal.command_buffer.dispatch.indirect` `&lt;` $command_buffer `:` type($command_buffer) `&gt;`\n              `target` `(` $executable `:` type($executable) `)`\n              `` `[` $entry_point `]`\n              `workgroups` `(` $workgroups_buffer `:` type($workgroups_buffer) `)`\n              `` `[` $workgroups_offset `]`\n              attr-dict-with-keyword\n</code></pre>  Dispatches an execution request with the dispatch parameters loaded from the given buffer.  ##### Attributes:   AttributeMLIR TypeDescription <code>entry_point</code>::mlir::IntegerAttrsize_t   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `command_buffer` | command_buffer | `executable` | executable | `workgroups_buffer` | buffer | `workgroups_offset` | index  #### `hal.command_buffer.dispatch.indirect.symbol` (HAL::CommandBufferDispatchIndirectSymbolOp)  _Command buffer indirect dispatch recording operation, using symbolref_   Syntax:  <pre><code>operation ::= `hal.command_buffer.dispatch.indirect.symbol` `&lt;` $command_buffer `:` type($command_buffer) `&gt;`\n              `target` `(` $entry_point `)`\n              `workgroups` `(` $workgroups_buffer `:` type($workgroups_buffer) `)`\n              `` `[` $workgroups_offset `]`\n              attr-dict-with-keyword\n</code></pre>  Dispatches an execution request with the dispatch parameters loaded from the given buffer, using using a nested symbol reference to the entry point.  <pre><code>hal.command_buffer.dispatch.indirect.symbol %cmd, @executable::@target::@entry,\n                                            workgroups = %buffer[%offset]\n</code></pre>  ##### Attributes:   AttributeMLIR TypeDescription <code>entry_point</code>::mlir::SymbolRefAttrsymbol reference attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `command_buffer` | command_buffer | `workgroups_buffer` | buffer | `workgroups_offset` | index  #### `hal.command_buffer.dispatch` (HAL::CommandBufferDispatchOp)  _Command buffer dispatch recording operation_   Syntax:  <pre><code>operation ::= `hal.command_buffer.dispatch` `&lt;` $command_buffer `:` type($command_buffer) `&gt;`\n              `target` `(` $executable `:` type($executable) `)`\n              `` `[` $entry_point `]`\n              `workgroups` `(` `[`\n              $workgroup_x `,`\n              $workgroup_y `,`\n              $workgroup_z\n              `]` `)`\n              attr-dict-with-keyword\n</code></pre>  Dispatches an execution request.  ##### Attributes:   AttributeMLIR TypeDescription <code>entry_point</code>::mlir::IntegerAttrsize_t   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `command_buffer` | command_buffer | `executable` | executable | `workgroup_x` | index | `workgroup_y` | index | `workgroup_z` | index  #### `hal.command_buffer.dispatch.symbol` (HAL::CommandBufferDispatchSymbolOp)  _Command buffer dispatch recording operation, using symbolref_   Syntax:  <pre><code>operation ::= `hal.command_buffer.dispatch.symbol` `&lt;` $command_buffer `:` type($command_buffer) `&gt;`\n              `target` `(` $entry_point `)`\n              `workgroups` `(` `[`\n              $workgroup_x `,`\n              $workgroup_y `,`\n              $workgroup_z\n              `]` `)`\n              attr-dict-with-keyword\n</code></pre>  Dispatches an execution request, using a nested symbol reference to the entry point.  ##### Attributes:   AttributeMLIR TypeDescription <code>entry_point</code>::mlir::SymbolRefAttrsymbol reference attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `command_buffer` | command_buffer | `workgroup_x` | index | `workgroup_y` | index | `workgroup_z` | index  #### `hal.command_buffer.end_debug_group` (HAL::CommandBufferEndDebugGroupOp)  _Pops a command buffer debug group label_   Syntax:  <pre><code>operation ::= `hal.command_buffer.end_debug_group` `&lt;` $command_buffer `:` type($command_buffer) `&gt;`\n              attr-dict-with-keyword\n</code></pre>  Pops a debug group from the stack.  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `command_buffer` | command_buffer  #### `hal.command_buffer.execution_barrier` (HAL::CommandBufferExecutionBarrierOp)  _Command buffer execution barrier recording operation_   Syntax:  <pre><code>operation ::= `hal.command_buffer.execution_barrier` `&lt;` $command_buffer `:` type($command_buffer) `&gt;`\n              `source` `(` $source_stage_mask `)`\n              `target` `(` $target_stage_mask `)`\n              `flags` `(` $flags `)`\n              attr-dict-with-keyword\n</code></pre>  Defines an execution dependency between all commands recorded before the barrier and all commands recorded after the barrier. Only the stages provided will be affected.  ##### Attributes:   AttributeMLIR TypeDescription <code>source_stage_mask</code>mlir::iree_compiler::IREE::HAL::ExecutionStageBitfieldAttrvalid ExecutionStage{{% markdown %}}Enum cases: * None (`None`) * CommandIssue (`CommandIssue`) * CommandProcess (`CommandProcess`) * Dispatch (`Dispatch`) * Transfer (`Transfer`) * CommandRetire (`CommandRetire`) * Host (`Host`){{% /markdown %}} <code>target_stage_mask</code>mlir::iree_compiler::IREE::HAL::ExecutionStageBitfieldAttrvalid ExecutionStage{{% markdown %}}Enum cases: * None (`None`) * CommandIssue (`CommandIssue`) * CommandProcess (`CommandProcess`) * Dispatch (`Dispatch`) * Transfer (`Transfer`) * CommandRetire (`CommandRetire`) * Host (`Host`){{% /markdown %}} <code>flags</code>mlir::iree_compiler::IREE::HAL::ExecutionBarrierFlagBitfieldAttrvalid ExecutionBarrierFlag{{% markdown %}}Enum cases: * None (`None`) * Reserved (`Reserved`){{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `command_buffer` | command_buffer  #### `hal.command_buffer.fill_buffer` (HAL::CommandBufferFillBufferOp)  _Command buffer buffer fill recording operation_   Syntax:  <pre><code>operation ::= `hal.command_buffer.fill_buffer` `&lt;` $command_buffer `:` type($command_buffer) `&gt;`\n              `target` `(` $target_buffer `:` type($target_buffer) `)`\n              `` `[` $target_offset `,` $length `]`\n              `pattern` `(` $pattern `:` type($pattern) `)`\n              attr-dict-with-keyword\n</code></pre>  Fills the target buffer with the given repeating value.  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `command_buffer` | command_buffer | `target_buffer` | buffer | `target_offset` | index | `length` | index | `pattern` | 8-bit signless integer or 16-bit signless integer or 32-bit signless integer  #### `hal.command_buffer.finalize` (HAL::CommandBufferFinalizeOp)  _Finalizes command buffer recording_   Syntax:  <pre><code>operation ::= `hal.command_buffer.finalize` `&lt;` $command_buffer `:` type($command_buffer) `&gt;`\n              attr-dict-with-keyword\n</code></pre>  Ends recording into the command buffer and prepares it for submission. No more commands may be recorded into the command buffer.  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `command_buffer` | command_buffer  #### `hal.command_buffer.push_constants` (HAL::CommandBufferPushConstantsOp)  _Command buffer push constants operation_   Syntax:  <pre><code>operation ::= `hal.command_buffer.push_constants` `&lt;` $command_buffer `:` type($command_buffer) `&gt;`\n              `layout` `(` $pipeline_layout `:` type($pipeline_layout) `)`\n              `offset` `(` $offset `)`\n              `values` `(` `[` $values `]` `)`\n              `:` type($values)\n              attr-dict-with-keyword\n</code></pre>  Pushes an inline set of constants that can be accessed by subsequent dispatches using a compatible pipeline layout.  Push constants are always 4-byte values and treated as opaque, meaning that they may be bit-casted floats, bit-packed booleans, etc.  ##### Attributes:   AttributeMLIR TypeDescription <code>offset</code>::mlir::IntegerAttrindex attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `command_buffer` | command_buffer | `pipeline_layout` | pipeline_layout | `values` | 32-bit signless integer  #### `hal.command_buffer.push_descriptor_set` (HAL::CommandBufferPushDescriptorSetOp)  _Command buffer descriptor set push binding operation_   Syntax:  <pre><code>operation ::= `hal.command_buffer.push_descriptor_set` `&lt;` $command_buffer `:` type($command_buffer) `&gt;`\n              `layout` `(` $pipeline_layout `:` type($pipeline_layout) `)`\n              `` `[` $set `]`\n              `bindings` `(` `[`\n              custom&lt;DescriptorSetBindings&gt;($binding_ordinals,\n              $binding_buffers,\n              type($binding_buffers),\n              $binding_offsets,\n              $binding_lengths)\n              `]` `)`\n              attr-dict-with-keyword\n</code></pre>  Pushes an inline-defined descriptor set to the command buffer. The provided buffers may either be HAL buffers or indirect references into the command buffer binding table.  Traits: SameVariadicOperandSize  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `command_buffer` | command_buffer | `pipeline_layout` | pipeline_layout | `set` | index | `binding_ordinals` | index | `binding_buffers` | index or buffer | `binding_offsets` | index | `binding_lengths` | index   ### Descriptor set layout ops  Ops for `!hal.descriptor_set_layout` / `iree_hal_descriptor_set_layout_t`.   #### `hal.descriptor_set_layout.create` (HAL::DescriptorSetLayoutCreateOp)  _Creates a descriptor set layout_   Syntax:  <pre><code>operation ::= `hal.descriptor_set_layout.create` `device` `(` $device `:` type($device) `)`\n              `flags` `(` $flags `)`\n              `bindings` `(` $bindings `)`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Creates a descriptor set layout that defines the bindings used within a set. The same descriptor set layout may be shared with many different executable layouts and by doing so some runtime binding overhead when switching between executables that use the same set layouts can be reduced.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>flags</code>::mlir::iree_compiler::IREE::HAL::DescriptorSetLayoutFlagsAttrvalid DescriptorSetLayout flags{{% markdown %}}Enum cases: * None (`None`) * Indirect (`Indirect`){{% /markdown %}} <code>bindings</code>::mlir::ArrayAttrHAL descriptor set layout binding array attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `device` | device  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | descriptor_set_layout  #### `hal.descriptor_set_layout.lookup` (HAL::DescriptorSetLayoutLookupOp)  _Descriptor set layout cache lookup pseudo-op_   Syntax:  <pre><code>operation ::= `hal.descriptor_set_layout.lookup` `device` `(` $device `:` type($device) `)`\n              `flags` `(` $flags `)`\n              `bindings` `(` $bindings `)`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Used during conversion to provide a placeholder for a globally cached and possibly lazy-initialized descriptor set layout.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>flags</code>::mlir::iree_compiler::IREE::HAL::DescriptorSetLayoutFlagsAttrvalid DescriptorSetLayout flags{{% markdown %}}Enum cases: * None (`None`) * Indirect (`Indirect`){{% /markdown %}} <code>bindings</code>::mlir::ArrayAttrHAL descriptor set layout binding array attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `device` | device  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | descriptor_set_layout   ### Device ops  Ops for `!hal.device` / `iree_hal_device_t`.  #### `hal.device.allocator` (HAL::DeviceAllocatorOp)  _Device allocator accessor operation_   Syntax:  <pre><code>operation ::= `hal.device.allocator` `&lt;` $device `:` type($device) `&gt;` `:` type($result) attr-dict-with-keyword\n</code></pre>  Returns the allocator that can be used to allocate buffers compatible with the device.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `device` | device  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | allocator  #### `hal.device.query` (HAL::DeviceQueryOp)  _Returns a runtime configuration parameter from the device_   Syntax:  <pre><code>operation ::= `hal.device.query` `&lt;` $device `:` type($device) `&gt;`\n              `key` `(` $category `:` `` `:` $key `)`\n              `:` type($ok) `,` type($value)\n              (`=` $default_value^)?\n              attr-dict-with-keyword\n</code></pre>  Queries a device configuration parameter with the given key. Returns a status indicating whether the pair was recognized/available and if it was the value converted to the specified type. Queries must return the same value for the lifetime of the module though may vary from run to run.  This is roughly equivalent to the `sysconf` linux syscall (https://man7.org/linux/man-pages/man3/sysconf.3.html) in that the exact set of keys available and their interpretation is target-dependent. If there is a HAL match attribute (`#hal.device.match.*`) or op (`hal.device.match.*`) prefer to use that in order to get compile-time propagation when the target is specified and elide the runtime query and get compile-time verification when a runtime query is required.  Users of the op must check the `ok` result before using the value as what set of keys is available may change over time. If in doubt: don't use this. Each key used adds additional versioning and testing complexity as runtime code path changes will explode combinatorially and should be treated with as much care as a binary file format change. Keys should be prefixed with `ex.` when experimental indicating that they are not expected to be present forever; all non-experimental keys should be vetted.  Well-known keys:  * hal.executable.format :: {some format}   Returns 1 if the given format is supported by the device loader.  * hal.device :: concurrency   The maximum concurrently executable submissions, mapping roughly to the   queue count. The actual concurrency available may be less than this based   on dynamic runtime parameters such as power/thermal modes, quota limits,   or user choice.  * hal.dispatch :: concurrency   The maximum concurrently executable workgroups for a particular dispatch.   The actual concurrency available may be less depending on device state.  Traits: AlwaysSpeculatableImplTrait, HAL_DeviceQuery  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>category</code>::mlir::StringAttrstring attribute <code>key</code>::mlir::StringAttrstring attribute <code>default_value</code>::mlir::TypedAttrTypedAttr instance{{% markdown %}}     This interface is used for attributes that have a type. The type of an     attribute is understood to represent the type of the data contained in the     attribute and is often used as the type of a value with this data.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `device` | device  ##### Results:  | Result | Description | | :----: | ----------- | | `ok` | 1-bit signless integer | `value` | any type  #### `hal.device.queue.alloca` (HAL::DeviceQueueAllocaOp)  _Allocates a queue-ordered transient buffer_   Syntax:  <pre><code>operation ::= `hal.device.queue.alloca` `&lt;` $device `:` type($device) `&gt;`\n              `affinity` `(` $queue_affinity `)`\n              `wait` `(` $wait_fence `)`\n              `signal` `(` $signal_fence `)`\n              `pool` `(` $pool `)`\n              `type` `(` $memory_types `)`\n              `usage` `(` $buffer_usage `)`\n              `:` custom&lt;SizeAwareType&gt;(type($result), $result_size)\n              attr-dict-with-keyword\n</code></pre>  Returns a queue-ordered transient buffer that will be available for use when the signal fence is reached. The allocation will not be made until the wait fence has been reached.  The size of the buffer returned may be larger than the requested size if the allocator has specific alignment requirements or minimum allocation sizes.  The buffer handle will remain live so long as there are retainers but the contents are undefined before the allocation signal fence has been signaled and after the deallocation wait fence has been reached.  Interfaces: OpAsmOpInterface, SizeAwareOpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>memory_types</code>mlir::iree_compiler::IREE::HAL::MemoryTypeBitfieldAttrvalid MemoryType{{% markdown %}}Enum cases: * None (`None`) * Optimal (`Optimal`) * HostVisible (`HostVisible`) * HostCoherent (`HostCoherent`) * HostCached (`HostCached`) * HostLocal (`HostLocal`) * DeviceVisible (`DeviceVisible`) * DeviceLocal (`DeviceLocal`){{% /markdown %}} <code>buffer_usage</code>mlir::iree_compiler::IREE::HAL::BufferUsageBitfieldAttrvalid BufferUsage{{% markdown %}}Enum cases: * None (`None`) * TransferSource (`TransferSource`) * TransferTarget (`TransferTarget`) * Transfer (`Transfer`) * DispatchIndirectParams (`DispatchIndirectParams`) * DispatchUniformRead (`DispatchUniformRead`) * DispatchStorageRead (`DispatchStorageRead`) * DispatchStorageWrite (`DispatchStorageWrite`) * DispatchStorage (`DispatchStorage`) * DispatchImageRead (`DispatchImageRead`) * DispatchImageWrite (`DispatchImageWrite`) * DispatchImage (`DispatchImage`) * SharingExport (`SharingExport`) * SharingReplicate (`SharingReplicate`) * SharingConcurrent (`SharingConcurrent`) * SharingImmutable (`SharingImmutable`) * MappingScoped (`MappingScoped`) * MappingPersistent (`MappingPersistent`) * MappingOptional (`MappingOptional`) * MappingAccessRandom (`MappingAccessRandom`) * MappingAccessSequentialWrite (`MappingAccessSequentialWrite`) * Mapping (`Mapping`){{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `device` | device | `queue_affinity` | 64-bit signless integer | `wait_fence` | fence | `signal_fence` | fence | `pool` | 64-bit signless integer | `result_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | buffer  #### `hal.device.queue.dealloca` (HAL::DeviceQueueDeallocaOp)  _Deallocates a queue-ordered transient buffer_   Syntax:  <pre><code>operation ::= `hal.device.queue.dealloca` `&lt;` $device `:` type($device) `&gt;`\n              `affinity` `(` $queue_affinity `)`\n              `wait` `(` $wait_fence `)`\n              `signal` `(` $signal_fence `)`\n              `buffer` `(` $buffer `:` type($buffer) `)`\n              attr-dict-with-keyword\n</code></pre>  Deallocates a queue-ordered transient buffer. The deallocation will not be made until the wait fence has been reached and once the storage is available for reuse the signal fence will be signaled.  After deallocation the contents of the buffer may still be accessible but will have undefined contents as other operations reuse the memory.  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `device` | device | `queue_affinity` | 64-bit signless integer | `wait_fence` | fence | `signal_fence` | fence | `buffer` | buffer  #### `hal.device.queue.execute` (HAL::DeviceQueueExecuteOp)  _Enqueues command buffer execution_   Syntax:  <pre><code>operation ::= `hal.device.queue.execute` `&lt;` $device `:` type($device) `&gt;`\n              `affinity` `(` $queue_affinity `)`\n              `wait` `(` $wait_fence `)`\n              `signal` `(` $signal_fence `)`\n              (`commands` `(` `[` $command_buffers^ `]` `)`)?\n              attr-dict-with-keyword\n</code></pre>  Executes one or more command buffers on a device queue. The command buffers are executed in order as if they were recorded as one. No commands will execute until the wait fence has been reached and the signal fence will be signaled when all commands have completed.  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `device` | device | `queue_affinity` | 64-bit signless integer | `wait_fence` | fence | `signal_fence` | fence | `command_buffers` | command_buffer  #### `hal.device.queue.flush` (HAL::DeviceQueueFlushOp)  _Flushes locally-pending submissions to the queue_   Syntax:  <pre><code>operation ::= `hal.device.queue.flush` `&lt;` $device `:` type($device) `&gt;`\n              `affinity` `(` $queue_affinity `)`\n              attr-dict-with-keyword\n</code></pre>  Flushes any locally-pending submissions in the queue. When submitting many queue operations this can be used to eagerly flush earlier submissions while later ones are still being constructed. This may be a no-op.  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `device` | device | `queue_affinity` | 64-bit signless integer  #### `hal.device.queue.read` (HAL::DeviceQueueReadOp)  _Reads a segment from a file into a device buffer_   Syntax:  <pre><code>operation ::= `hal.device.queue.read` `&lt;` $device `:` type($device) `&gt;`\n              `affinity` `(` $queue_affinity `)`\n              `wait` `(` $wait_fence `)`\n              `signal` `(` $signal_fence `)`\n              `source` `(` $source_file `:` type($source_file) `)`\n              `` `[` $source_offset `]`\n              `target` `(` $target_buffer `:` type($target_buffer) `)`\n              `` `[` $target_offset `]`\n              `length` `(` $length `)`\n              `flags` `(` $flags `)`\n              attr-dict-with-keyword\n</code></pre>  Enqueues a file read operation that streams a segment of the source file defined by the source offset and length into the target HAL buffer at the specified target offset. The queue affinity should be set to where the target buffer will be consumed. The source file must have read permission and the target buffer must have transfer-target usage. Read failure will result in propagated semaphore failure or device loss.  ##### Attributes:   AttributeMLIR TypeDescription <code>flags</code>::mlir::IntegerAttr32-bit signless integer attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `device` | device | `queue_affinity` | 64-bit signless integer | `wait_fence` | fence | `signal_fence` | fence | `source_file` | buffer | `source_offset` | 64-bit signless integer | `target_buffer` | buffer | `target_offset` | index | `length` | index  #### `hal.device.queue.write` (HAL::DeviceQueueWriteOp)  _Writes a segment from a device buffer into a file_   Syntax:  <pre><code>operation ::= `hal.device.queue.write` `&lt;` $device `:` type($device) `&gt;`\n              `affinity` `(` $queue_affinity `)`\n              `wait` `(` $wait_fence `)`\n              `signal` `(` $signal_fence `)`\n              `source` `(` $source_buffer `:` type($source_buffer) `)`\n              `` `[` $source_offset `]`\n              `target` `(` $target_file `:` type($target_file) `)`\n              `` `[` $target_offset `]`\n              `length` `(` $length `)`\n              `flags` `(` $flags `)`\n              attr-dict-with-keyword\n</code></pre>  Enqueues a file write operation that streams a segment of the source HAL buffer defined by the source offset and length into the target file at the specified target offset. The queue affinity should be set to where the source buffer was produced. The source buffer must have transfer-source usage and the target file must have write permission. Write failure will result in propagated semaphore failure or device loss.  ##### Attributes:   AttributeMLIR TypeDescription <code>flags</code>::mlir::IntegerAttr32-bit signless integer attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `device` | device | `queue_affinity` | 64-bit signless integer | `wait_fence` | fence | `signal_fence` | fence | `source_buffer` | buffer | `source_offset` | index | `target_file` | buffer | `target_offset` | 64-bit signless integer | `length` | index  #### `hal.device.switch` (HAL::DeviceSwitchOp)  _Runtime device switch pseudo op_  Switches between multiple regions based on the runtime device type. The provided regions are matched against the runtime backend of the given device and executed only when the device matches the conditions.  Conditions can match on wildcards and be folded to enable conditions that have similar bodies to be folded. The patterns themselves are only matched once at startup and then the results are cached; the runtime overhead is equivalent to a normal switch statement. In cases where the compiler can statically identify the device type entire cases can be folded away.  Supported conditions: * `#hal.match...`: execute the region if the expression matches.  Supported match expressions: * `#hal.match.always`: always matches; useful for defaults. * `#hal.match.any&lt;[...]&gt;`: matches if any of the nested expressions match. * `#hal.match.all&lt;[...]&gt;`: matches only if all of the nested expressions   match. * `#hal.device.match.id&lt;\"pattern*-?-*\"&gt;`: matches against the device   identifier. The pattern is evaluated with standard file path wildcards   (`*` for zero or more characters and `?` for one character).  If more than one condition is satisfied the first listed will be chosen. More specific conditions should be earlier in the set. If no condition is matched but there are return values the switch will abort at runtime. It's strongly recommend that all switches that return values end with a trailing `#hal.match.always` condition to handle the fallthrough case.  Upon creation each condition region will have an empty entry block with the specified operands available as arguments. Each region must be setup to return the same types.  <pre><code>%c0 = arith.constant 0 : i32\n%c1 = arith.constant 1 : i32\n%c2 = arith.constant 2 : i32\n%device = ... : !hal.device\n%0 = hal.device.switch&lt;%device : !hal.device&gt; -&gt; i32\n  #hal.device.match.id&lt;\"vulkan-v1.?-*\"&gt; {\n    hal.return %c1 : i32\n  },\n  #hal.match.any&lt;[#hal.device.match.id&lt;\"vmvx\"&gt;, #hal.device.match.id&lt;\"vulkan-*\"&gt;]&gt; {\n    hal.return %c2 : i32\n  },\n  #hal.match.always {\n    hal.return %c0 : i32\n  }\n</code></pre>  Traits: NoRegionArguments, RecursiveMemoryEffects  ##### Attributes:   AttributeMLIR TypeDescription <code>conditions</code>::mlir::ArrayAttrarray attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `device` | device  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | any type  #### `hal.return` (HAL::ReturnOp)  _Return from a hal.device.switch region_   Syntax:  <pre><code>operation ::= `hal.return` ($operands^ `:` type($operands))? attr-dict\n</code></pre>  Returns the given values from the region and back to the host code.  Traits: Terminator  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operands` | any type   ### Executable ops  Ops for `!hal.executable` / `iree_hal_executable_t`.  #### `hal.executable.binary` (HAL::ExecutableBinaryOp)  _Compiled executable binary data_   Syntax:  <pre><code>operation ::= `hal.executable.binary` custom&lt;SymbolVisibility&gt;($sym_visibility)\n              $sym_name\n              attr-dict-with-keyword\n</code></pre>  A compiled executable binary with an optional nested module containing the IR prior to serialization (for debugging).  Traits: HasParent  Interfaces: Symbol  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>sym_name</code>::mlir::StringAttrstring attribute <code>format</code>::mlir::StringAttrstring attribute <code>data</code>::mlir::DenseIntElementsAttr8-bit signless integer elements attribute <code>mime_type</code>::mlir::StringAttrstring attribute   #### `hal.executable.calculate_workgroups` (HAL::ExecutableCalculateWorkgroupsOp)  _Calculates workgroup count from workload for an exported function_   Syntax:  <pre><code>operation ::= `hal.executable.calculate_workgroups` `device` `(` $device `:` type($device) `)`\n              `target` `(` $entry_point `)`\n              (`workload` `(` `[` $workload^ `]` `)`)?\n              `:` type($workgroup_x) `,` type($workgroup_y) `,` type($workgroup_z)\n              attr-dict-with-keyword\n</code></pre>  Calculates the workgroup count (grid XYZ) based on the given workload using the workgroup count calculation region of the target `hal.executable.export` op.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>entry_point</code>::mlir::SymbolRefAttrsymbol reference attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `device` | device | `workload` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `workgroup_x` | index | `workgroup_y` | index | `workgroup_z` | index  #### `hal.executable.constant.block` (HAL::ExecutableConstantBlockOp)  _Executable constant block initializer_  Initializes one or more constants in the executable constant block by returning one value per identified constant. Each constant block is evaluated on the host prior to instantiating the executable for a given device and allows for the executable to be specialized based on device capabilities and limits.  The keys specified are unique per variant and will be deduplicated across multiple constant blocks when present. They are only used during lowering and will not survive to runtime so they need only have descriptive enough names to avoid collisions and represent the semantics of the value.  Constant values can be loaded in the device code with the `hal.executable.constant.load` op:  <pre><code>hal.executable.variant public @target {\n  hal.executable.constant.block(%device: !hal.device) -&gt; (i32, i32) as (\"foo\", \"bar\") {\n    %0 = hal.device.query&lt;%device&gt; key(\"some.device.prop\")...\n    %1 = hal.device.query&lt;%device&gt; key(\"another.device.prop\")...\n    hal.return %0, %1 : i32, i32\n  }\n  builtin.module {\n    func @dispatch0() {\n      %0 = hal.executable.constant.load \"foo\" : i32\n      %1 = hal.executable.constant.load \"bar\" : i32\n      return\n    }\n  }\n}\n</code></pre>  Each target backend will implement the constant initialization and access in a way compatible with its execution model. Examples: - CPU: read-only buffer initialized on load and passed to each dispatch - CUDA: read-only buffer initialized on load and passed to each dispatch - SPIR-V: specialization constants - Metal: function constants - WebGPU: pipeline-overridable constants  Traits: HasParent, IsolatedFromAbove  Interfaces: CallableOpInterface, FunctionOpInterface, Symbol  ##### Attributes:   AttributeMLIR TypeDescription <code>function_type</code>::mlir::TypeAttrtype attribute of function type <code>keys</code>::mlir::ArrayAttrarray attribute <code>arg_attrs</code>::mlir::ArrayAttrArray of dictionary attributes <code>res_attrs</code>::mlir::ArrayAttrArray of dictionary attributes   #### `hal.executable.constant.load` (HAL::ExecutableConstantLoadOp)  _Loads a constant value from the executable constant block_   Syntax:  <pre><code>operation ::= `hal.executable.constant.load` $key attr-dict `:` type($result)\n</code></pre>  Loads a scalar constant value from the static executable constant block. The value provided by a constant block with the given key will be loaded and bitcast (possibly with truncation or zero-extension) to the result type.  Note that backends are allowed to implement their own mechanisms for referencing constant block values and this is provided only as a default for those not needing special behavior.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>key</code>::mlir::StringAttrstring attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index or signless integer or floating-point or complex-type  #### `hal.executable.create` (HAL::ExecutableCreateOp)  _Creates an executable_   Syntax:  <pre><code>operation ::= `hal.executable.create` `device` `(` $device `:` type($device) `)`\n              `target` `(` $executable_target `)`\n              `layouts` `(` `[` $layouts `]` `)`\n              (`constants` `(` `[` $constants^ `]` `)`)?\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Creates a target-dependent executable cached on the provided device. Entry points contained within the executable can be dispatched using the resulting executable handle.  Depending on the driver creation may take a non-trivial amount of time (such as when JITing/etc). As the cache is internally synchronized callers can issue preparation requests from multiple threads - even for the same executables - and calls will block until preparation completes.  Optional constants provide for specialization of the executable based on runtime-derived parameters.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>executable_target</code>::mlir::SymbolRefAttrsymbol reference attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `device` | device | `layouts` | pipeline_layout | `constants` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | executable  #### `hal.executable_end` (HAL::ExecutableEndOp)  _Terminator pseudo-op for the executable op_   Syntax:  <pre><code>operation ::= `hal.executable_end` attr-dict\n</code></pre>   Traits: HasParent, Terminator  #### `hal.executable.export` (HAL::ExecutableExportOp)  _Executable entry point declaration_  An entry point exported by the executable with statically-available information describing the IO interface it uses and other dispatch metadata.  The `calculate_workgroup_count` region represents the computation that returns the number of workgroups to use in the 3D grid dispatch. The arguments to the region represents the workload as captured by each dispatch. It returns the number of workgroups along x, y, and z.  Traits: HasParent, IsolatedFromAbove  Interfaces: Symbol  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>sym_name</code>::mlir::StringAttrstring attribute <code>ordinal</code>::mlir::IntegerAttrsize_t <code>layout</code>::mlir::iree_compiler::IREE::HAL::PipelineLayoutAttrexecutable entry point layout specification{{% markdown %}}     Specifies the layout information used for interacting with executable     functions. This allows host code to correctly map parameters to the     lower-level target-specific argument passing behavior.   {{% /markdown %}} <code>workgroup_size</code>::mlir::ArrayAttrindex array attribute <code>subgroup_size</code>::mlir::IntegerAttrsize_t <code>workgroup_local_memory</code>::mlir::IntegerAttrindex attribute   #### `hal.executable.lookup` (HAL::ExecutableLookupOp)  _Executable cache lookup pseudo-op_   Syntax:  <pre><code>operation ::= `hal.executable.lookup` `device` `(` $device `:` type($device) `)`\n              `executable` `(` $executable `)`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Used during conversion to provide a placeholder for a globally cached and possibly lazy-initialized executable.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>executable</code>::mlir::FlatSymbolRefAttrflat symbol reference attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `device` | device  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | executable  #### `hal.executable` (HAL::ExecutableOp)  _Target-specific executable module_   Syntax:  <pre><code>operation ::= `hal.executable` custom&lt;SymbolVisibility&gt;($sym_visibility)\n              $sym_name\n              attr-dict-with-keyword\n              regions\n</code></pre>  An executable module representing a target-specific compiled kernel/shader/etc.  Traits: IsolatedFromAbove, SingleBlock, SingleBlockImplicitTerminator, SymbolTable  Interfaces: Symbol  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>sym_name</code>::mlir::StringAttrstring attribute   #### `hal.executable.source_end` (HAL::ExecutableSourceEndOp)  _Terminator pseudo-op for the executable source op_   Syntax:  <pre><code>operation ::= `hal.executable.source_end` attr-dict\n</code></pre>   Traits: HasParent, Terminator  #### `hal.executable.source` (HAL::ExecutableSourceOp)  _Generic source contents of an executable op_   Syntax:  <pre><code>operation ::= `hal.executable.source` custom&lt;SymbolVisibility&gt;($sym_visibility)\n              $sym_name\n              attr-dict-with-keyword\n              ``\n              regions\n</code></pre>  This is an unspecialized source representation of an executable module without an assigned target. This is useful for hand-authoring executables prior to device specification.  Traits: IsolatedFromAbove, SingleBlock, SingleBlockImplicitTerminator, SymbolTable  Interfaces: Symbol  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>sym_name</code>::mlir::StringAttrstring attribute <code>objects</code>::mlir::iree_compiler::IREE::HAL::ExecutableObjectsAttrtarget-specific object file references{{% markdown %}}     A dictionary mapping executable target specifications to a list of objects.     This is used to allow layers of the stack that support multi-targeting to     specify information used during lowering into each particular target.      The key attributes are matched against each target variant based on the     backend and format as well as any configuration data provided. When     comparing the configuration only fields present in both the key and     target variant will be checked and must match. This allows specification of     generic sets (\"all x86_64 targets get these objects\") as well as specific     ones (\"only x86_64 targets with vector_size = 64 get these objects\").      Example:     <pre><code>#hal.executable.objects&lt;{\n  #hal.executable.target&lt;\"llvm-cpu\", \"embedded-elf-arm_64\"&gt; = [\n    #hal.executable.object&lt;{path = \"some/file_arm_64.obj\"}&gt;\n  ],\n  #hal.executable.target&lt;\"llvm-cpu\", \"embedded-elf-x86_64\"&gt; = [\n    #hal.executable.object&lt;{path = \"some/file_x86_64.obj\"}&gt;\n  ]\n}&gt;\n</code></pre>   {{% /markdown %}}   #### `hal.executable.variant_end` (HAL::ExecutableVariantEndOp)  _Terminator pseudo-op for the executable variant op_   Syntax:  <pre><code>operation ::= `hal.executable.variant_end` attr-dict\n</code></pre>   Traits: HasParent, Terminator  #### `hal.executable.variant` (HAL::ExecutableVariantOp)  _Target-specific variant of an executable op_   Syntax:  <pre><code>operation ::= `hal.executable.variant` custom&lt;SymbolVisibility&gt;($sym_visibility)\n              $sym_name\n              `,` `target` `=` $target\n              (`,` `objects` `=` $objects^ )?\n              attr-dict-with-keyword\n              regions\n</code></pre>  The target IR for the executable. This can be preserved for debugging but is usually removed during transformation.  Traits: HasParent, IsolatedFromAbove, SingleBlock, SingleBlockImplicitTerminator, SymbolTable  Interfaces: Symbol  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>sym_name</code>::mlir::StringAttrstring attribute <code>target</code>::mlir::iree_compiler::IREE::HAL::ExecutableTargetAttrgeneric executable target specification{{% markdown %}}     Specifies how to compile an executable for a specific target backend.     A backend is used to translate and serialize the executable into the final     form passed to the runtime. The format of the executable is a     target-specific value indicating the required runtime support to load the     deployed artifact. An optionally provided configuration dictionary overrides     backend-specific defaults.      Example:     <pre><code>  // Produce a system-native ELF for x86-64 systems using the LLVM backend:\n  #hal.executable.target&lt;\"llvm-cpu\", \"system-elf-x86_64\", {\n    triple = \"x86_64-unknown-linux-elf\",\n    cpu = \"host\",\n    cpu_features = \"host\",\n    abi = \"lp32\",\n    ...\n  }&gt;\n</code></pre>      The same compilation backend may be used to translate executables for     several different runtime devices. Likewise the same runtime device may use     one of many different executable targets. Assume an N:M mapping between the     two in all cases.   {{% /markdown %}} <code>objects</code>::mlir::ArrayAttrHAL executable object references    ### Experimental ops  Temporary hack ops expected to be removed in the future.  #### `hal.ex.file.from_memory` (HAL::ExFileFromMemoryOp)  _Creates a file mapped into a byte range of a host buffer_   Syntax:  <pre><code>operation ::= `hal.ex.file.from_memory` `device` `(` $device `:` type($device) `)`\n              `affinity` `(` $queue_affinity `)`\n              `access` `(` $access `)`\n              `buffer` `(` $buffer `:` type($buffer) `)`\n              `` `[` $offset `for` $length `]`\n              `flags` `(` $flags `)`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns a file handle that is backed by the given `buffer` contents. Behavior is undefined if the buffer contents change while the accesses are in-flight.  Experimental as the exact interface for getting files from module contents still needs iteration. Most hardware APIs require a file descriptor or native platform handle but here we only have host pointers. When memory-mapped some systems allow for retrieval of the platform handle from a virtual address (GetMappedFileNameA/posix_mem_offset) but the APIs are sketchy and likely slow. Instead we should probably have a way to query for a file handle derived from the calling module by stack-walking and asking the VM module for its handle. Until we can figure this out this method will be marked epxerimental.  Interfaces: OpAsmOpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>access</code>mlir::iree_compiler::IREE::HAL::MemoryAccessBitfieldAttrvalid MemoryAccess{{% markdown %}}Enum cases: * None (`None`) * Read (`Read`) * Write (`Write`) * Discard (`Discard`) * MayAlias (`MayAlias`) * Unaligned (`Unaligned`) * Any (`Any`){{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `device` | device | `queue_affinity` | 64-bit signless integer | `buffer` | a reference counted byte buffer | `offset` | index | `length` | index | `flags` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | buffer  #### `hal.ex.shared_device` (HAL::ExSharedDeviceOp)  Syntax:  <pre><code>operation ::= `hal.ex.shared_device` attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | device   ### Fence ops  Ops for `!hal.fence` / `iree_hal_fence_t`.  #### `hal.fence.await` (HAL::FenceAwaitOp)  _Asynchronous fence wait operation_   Syntax:  <pre><code>operation ::= `hal.fence.await` `until` `(` `[` $fences `]` `)`\n              `timeout_millis` `(` $timeout_millis `)`\n              `:` type($status)\n              attr-dict-with-keyword\n</code></pre>  Yields the caller until all fences is reached. Returns the `status` of the fence after the wait, with a non-zero value indicating failure.  Traits: Util_YieldPoint  Interfaces: OpAsmOpInterface  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `timeout_millis` | 32-bit signless integer | `fences` | fence  ##### Results:  | Result | Description | | :----: | ----------- | | `status` | 32-bit signless integer  #### `hal.fence.create` (HAL::FenceCreateOp)  _Creates an unsignaled fence_   Syntax:  <pre><code>operation ::= `hal.fence.create` `device` `(` $device `:` type($device) `)`\n              `flags` `(` $flags `)`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns a fence that defines a point in time. By default fences will remain unsignaled unless they are explicitly signaled with `hal.fence.signal` or asynchronously signaled by the device by passing them as an operand to queue submission ops.  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{MemoryEffects::Allocate on ::mlir::SideEffects::DefaultResource}  ##### Attributes:   AttributeMLIR TypeDescription <code>flags</code>mlir::iree_compiler::IREE::HAL::FenceFlagBitfieldAttrvalid FenceFlag{{% markdown %}}Enum cases: * None (`None`) * Reserved (`Reserved`){{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `device` | device  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | fence  #### `hal.fence.fail` (HAL::FenceFailOp)  _Fence failure operation_   Syntax:  <pre><code>operation ::= `hal.fence.fail` `&lt;` $fence `:` type($fence) `&gt;`\n              `status` `(` $status `)`\n              attr-dict-with-keyword\n</code></pre>  Signals the fence with a failure. The `status` will be returned from each timepoint semaphores `hal.semaphore.query` and `hal.semaphore.signal` for the lifetime of each semaphore.  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `fence` | fence | `status` | 32-bit signless integer  #### `hal.fence.join` (HAL::FenceJoinOp)  _Creates a fence from the given timepoints_   Syntax:  <pre><code>operation ::= `hal.fence.join` `at` `(` `[` $fences `]` `)`\n              `-&gt;` type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns a fence that joins the input fences as a wait-all operation.  Interfaces: OpAsmOpInterface  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `fences` | fence  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | fence  #### `hal.fence.query` (HAL::FenceQueryOp)  _Fence query operation_   Syntax:  <pre><code>operation ::= `hal.fence.query` `&lt;` $fence `:` type($fence) `&gt;`\n              `:` type($status)\n              attr-dict-with-keyword\n</code></pre>  Queries whether the fence has been reached and its status. Returns OK if the fence has been signaled successfully, DEFERRED if it is unsignaled, and otherwise an error indicating the failure.  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `fence` | fence  ##### Results:  | Result | Description | | :----: | ----------- | | `status` | 32-bit signless integer  #### `hal.fence.signal` (HAL::FenceSignalOp)  _Fence signal operation_   Syntax:  <pre><code>operation ::= `hal.fence.signal` `&lt;` $fence `:` type($fence) `&gt;`\n              attr-dict-with-keyword\n</code></pre>  Signals the fence to indicate that the timepoints contained have been reached. Waiting work may begin immediately.  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `fence` | fence   ### Instrument ops  Ops for `!hal.instrument.*`.  #### `hal.instrument.memory.load` (HAL::InstrumentMemoryLoadOp)  _Emits a memory load instrumentation event_   Syntax:  <pre><code>operation ::= `hal.instrument.memory.load` `` `[` $buffer `:` type($buffer) `for` $workgroupKey `]`\n              $base `[` $indices `]` `,` $loadValue\n              attr-dict `:` type($base) `,` type($result)\n</code></pre>  Emits a workgroup-specific memory load event indicating that a number of bytes from the given resolved pointer have been loaded by the workgroup.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `buffer` | memref of any type values | `workgroupKey` | index | `loadValue` | any type | `base` | memref of any type values | `indices` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | any type  #### `hal.instrument.memory.store` (HAL::InstrumentMemoryStoreOp)  _Emits a memory store instrumentation event_   Syntax:  <pre><code>operation ::= `hal.instrument.memory.store` `` `[` $buffer `:` type($buffer) `for` $workgroupKey `]`\n              $base `[` $indices `]` `,` $storeValue\n              attr-dict `:` type($base) `,` type($result)\n</code></pre>  Emits a workgroup-specific memory store event indicating that a number of bytes have been stored to the given resolved pointer by the workgroup.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `buffer` | memref of any type values | `workgroupKey` | index | `storeValue` | any type | `base` | memref of any type values | `indices` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | any type  #### `hal.instrument.print` (HAL::InstrumentPrintOp)  _Emits a human-readable printf-style string event_   Syntax:  <pre><code>operation ::= `hal.instrument.print` `` `[` $buffer `:` type($buffer) `for` $workgroupKey `]`\n              $format (`*` `(` $values^ `:` type($values) `)`)?\n              attr-dict\n</code></pre>  Formats a string using a limited subset of printf format specifiers and the provided values and then emits an `iree_instrument_dispatch_print_t` event. Final formatted string lengths may be limited to as much as 1024 characters and should be kept as small as possible to avoid easily exceeding the instrumentation storage buffers with redundant strings.  ##### Attributes:   AttributeMLIR TypeDescription <code>format</code>::mlir::StringAttrstring attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `buffer` | memref of any type values | `workgroupKey` | index | `values` | any type  #### `hal.instrument.value` (HAL::InstrumentValueOp)  _Emits a scalar value instrumentation event_   Syntax:  <pre><code>operation ::= `hal.instrument.value` `` `[` $buffer `:` type($buffer) `for` $workgroupKey `]`\n              $ordinal `=` $operand attr-dict `:` type($operand)\n</code></pre>  Emits a workgroup-specific typed value with the given workgroup-relative ordinal.  This op will be preserved even if the output is not used as it is only for debugging purposes.  ##### Attributes:   AttributeMLIR TypeDescription <code>ordinal</code>::mlir::IntegerAttr8-bit integer attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `buffer` | memref of any type values | `workgroupKey` | index | `operand` | any type  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | any type  #### `hal.instrument.workgroup` (HAL::InstrumentWorkgroupOp)  _Emits a dispatch workgroup instrumentation event_   Syntax:  <pre><code>operation ::= `hal.instrument.workgroup` `` `[` $buffer `:` type($buffer) `]`\n              `dispatch` `(` $dispatchId `)`\n              attr-dict `:` type($workgroupKey)\n</code></pre>  Emits an `iree_instrument_dispatch_workgroup_t` event into the instrumentation stream. The workgroup event identifies the unique dispatch, its workgroup count, and the ID of the emitting workgroup within the dispatch. Optionally targets that support querying the processor ID executing the workgroup can attach that information for tracking purposes.  On targets such as CPUs where entire workgroups execute as atomic units only one workgroup event should be emitted. On targets such as GPUs where there may be multiple invocations executing as part of a single workgroup only the first invocation within the workgroup should emit the workgroup event (by checking if the LocalInvocationIndex or threadIdx == 0, etc).  The resulting workgroup key is used by subsequent workgroup-specific instrumentation events.  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `buffer` | memref of any type values | `dispatchId` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `workgroupKey` | index   ### Interface ops  Ops for `!hal.interface.*`.  #### `hal.interface.binding.subspan` (HAL::InterfaceBindingSubspanOp)  _Returns an alias to a subspan of interface binding data_   Syntax:  <pre><code>operation ::= `hal.interface.binding.subspan` `set` `(` $set `)`\n              `binding` `(` $binding `)`\n              `type` `(` custom&lt;DescriptorType&gt;($descriptor_type) `)`\n              (`alignment` `(` $alignment^ `)`)?\n              (`offset` `(` $byte_offset^ `)`)?\n              (`flags` `(` $descriptor_flags^ `)`)?\n              attr-dict `:` type($result) (`{` $dynamic_dims^ `}`)?\n</code></pre>  Returns a subspan of an interface binding storage buffer in a generic type. The exact shape, type, and alignment of the returned type are defined by the result type (tensor, memref, etc).  An optional alignment indicates the byte alignment of the base binding resource. Note that the byte offset is added to the base and the alignment will be the minimum of the two.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), Util_ShapeAwareOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>set</code>::mlir::IntegerAttrindex attribute <code>binding</code>::mlir::IntegerAttrindex attribute <code>descriptor_type</code>::mlir::iree_compiler::IREE::HAL::DescriptorTypeAttrvalid DescriptorType{{% markdown %}}Enum cases: * uniform_buffer (`UniformBuffer`) * storage_buffer (`StorageBuffer`){{% /markdown %}} <code>alignment</code>::mlir::IntegerAttrindex attribute <code>descriptor_flags</code>::mlir::iree_compiler::IREE::HAL::DescriptorFlagsAttrvalid Descriptor flags{{% markdown %}}Enum cases: * None (`None`) * ReadOnly (`ReadOnly`){{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `byte_offset` | index | `dynamic_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | any type  #### `hal.interface.constant.load` (HAL::InterfaceConstantLoadOp)  _Loads a constant value from the interface constant block_   Syntax:  <pre><code>operation ::= `hal.interface.constant.load` `` `[` $index `]`\n              (`alignment` `(` $alignment^ `)`)?\n              (`values` `(` $values^ `)`)?\n              attr-dict `:` type($result)\n</code></pre>  Loads a scalar constant value from an executable IO push constant block. The value will be loaded from the given constant offset and will be bitcast (possibly with truncation or zero-extension) to the result type.  An optional alignment indicates the byte alignment of potential values for the constant when it could be determined from analysis. If omitted the value may be anything and its interpretation is up to the usage. This is intended to provide pointer alignment-like semantics to constants that are used to index into binding resources.  An optional set of values indicates all possible values that can be passed to the constant from all dispatch sites in the program. If omitted the value may be from an unanalyzable source (outside of the program, indirect, etc) and must be assumed to have any value.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>index</code>::mlir::IntegerAttrsize_t <code>alignment</code>::mlir::IntegerAttrindex attribute <code>values</code>::mlir::ArrayAttrarray attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index or signless integer or floating-point or complex-type  #### `hal.interface.workgroup.count` (HAL::InterfaceWorkgroupCountOp)  _Returns the total workgroup count of the grid_   Syntax:  <pre><code>operation ::= `hal.interface.workgroup.count` `[` $dimension `]` attr-dict `:` type($result)\n</code></pre>  The total number of workgroups along each dimension in the dispatch grid. Matches what was passed to the `hal.command_buffer.dispatch` command (or what was indirectly specified).  Corresponds to the `NumWorkgroups` SPIR-V built-in and the `gridDim` CUDA built-in variable.  <pre><code>%x = hal.interface.workgroup.count[0] : index\n%y = hal.interface.workgroup.count[1] : index\n%z = hal.interface.workgroup.count[2] : index\n</code></pre>  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>dimension</code>::mlir::IntegerAttrindex attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index  #### `hal.interface.workgroup.id` (HAL::InterfaceWorkgroupIDOp)  _Returns the index of the current workgroup in the grid_   Syntax:  <pre><code>operation ::= `hal.interface.workgroup.id` `[` $dimension `]` attr-dict `:` type($result)\n</code></pre>  The global workgroup ID of the current tile in the range of `[0, hal.interface.workgroup.count)` along each XYZ dimension.  Corresponds to the `WorkgroupId` SPIR-V built-in and the `blockIdx` CUDA built-in variable.  <pre><code>%x = hal.interface.workgroup.id[0] : index\n%y = hal.interface.workgroup.id[1] : index\n%z = hal.interface.workgroup.id[2] : index\n</code></pre>  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>dimension</code>::mlir::IntegerAttrindex attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index  #### `hal.interface.workgroup.size` (HAL::InterfaceWorkgroupSizeOp)  _Returns the size of each workgroup in invocations_   Syntax:  <pre><code>operation ::= `hal.interface.workgroup.size` `[` $dimension `]` attr-dict `:` type($result)\n</code></pre>  The number of local invocations within the current workgroup along each dimension. Depending on backend this may map to the SIMT thread count or inner loop nest parameters.  Corresponds to the `WorkgroupSize` SPIR-V built-in and the `blockDim` CUDA built-in variable.  <pre><code>%x = hal.interface.workgroup.size[0] : index\n%y = hal.interface.workgroup.size[1] : index\n%z = hal.interface.workgroup.size[2] : index\n</code></pre>  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>dimension</code>::mlir::IntegerAttrindex attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index   ### Pipeline layout ops  Ops for `!hal.pipeline_layout` / `iree_hal_pipeline_layout_t`.   #### `hal.pipeline_layout.create` (HAL::PipelineLayoutCreateOp)  _Creates an pipeline layout_   Syntax:  <pre><code>operation ::= `hal.pipeline_layout.create` `device` `(` $device `:` type($device) `)`\n              `push_constants` `(` $push_constants `)`\n              `layouts` `(` `[` $set_layouts `]` `)`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Creates an pipeline layout from the given descriptor sets and push constant required size. Pipeline layouts can be shared across any executable that uses the same layout and push constant information. Sharing the layout between executables will reduce runtime binding overhead and it is often worth the cost to allow a small number of unused bindings in one executable such that it can share layouts with others that will be scheduled adjacent to it.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>push_constants</code>::mlir::IntegerAttrindex attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `device` | device | `set_layouts` | descriptor_set_layout  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | pipeline_layout  #### `hal.pipeline_layout.lookup` (HAL::PipelineLayoutLookupOp)  _Pipeline layout cache lookup pseudo-op_   Syntax:  <pre><code>operation ::= `hal.pipeline_layout.lookup` `device` `(` $device `:` type($device) `)`\n              `layout` `(` $layout `)`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Used during conversion to provide a placeholder for a globally cached and possibly lazy-initialized pipeline layout.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>layout</code>::mlir::iree_compiler::IREE::HAL::PipelineLayoutAttrexecutable entry point layout specification{{% markdown %}}     Specifies the layout information used for interacting with executable     functions. This allows host code to correctly map parameters to the     lower-level target-specific argument passing behavior.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `device` | device  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | pipeline_layout   ### Pseudo Ops  Pseudo ops for conversion support.  #### `hal.tensor.barrier` (HAL::TensorBarrierOp)  _Signals a fence when all tensors are available_   Syntax:  <pre><code>operation ::= `hal.tensor.barrier` `join` `` `(` $sources `:` type($sources) `)`\n              `=` `` `&gt;`\n              $signal_fence `:` type($signal_fence)\n              attr-dict-with-keyword\n</code></pre>  Defines a barrier that is used to indicate availability of an entire set of tensors by signaling a fence. The source tensors are returned for chaining.  Interfaces: TiedOpInterface  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `sources` | tensor of any type values | `signal_fence` | fence  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | tensor of any type values  #### `hal.tensor.export` (HAL::TensorExportOp)  _Exports a tensor to a HAL buffer view_   Syntax:  <pre><code>operation ::= `hal.tensor.export` $source\n              ($name^)?\n              (`into` `(` $target_storage^ `:` type($target_storage) `)`)?\n              `:`\n              custom&lt;TypeAlias&gt;($source_encoding, type($source)) (`{` $source_dims^ `}`)?\n              `-&gt;`\n              type($target)\n              attr-dict\n</code></pre>  Defines an export of an SSA-form tensor to an external HAL buffer view.  The provided `source_encoding`, if different from the `source` type, indicates that the ABI-facing type may differ from the internal representation. The types must be bitcastable (same storage size) and dynamically shaped values must have the same number of dynamic dimensions. This allows for casting between rank-0 and rank-N types, different element types, etc.  An optional `target_storage` buffer can be provided to hold the exported result. The export will fail at runtime if the storage is null or if it has insufficient capacity to store the output. The storage must be device-visible and defined for transfer-target and dispatch usage.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), TiedOpInterface, Util_ShapeAwareOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>source_encoding</code>::mlir::TypeAttrany type attribute <code>name</code>::mlir::StringAttrstring attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | tensor of any type values | `source_dims` | index | `target_storage` | buffer or buffer_view  ##### Results:  | Result | Description | | :----: | ----------- | | `target` | buffer or buffer_view  #### `hal.tensor.import` (HAL::TensorImportOp)  _Imports a tensor from a HAL buffer view_   Syntax:  <pre><code>operation ::= `hal.tensor.import` (`wait` `(` $wait_fence^ `)` `=` `` `&gt;`)?\n              $source\n              ($name^)?\n              `:` type($source) `-&gt;`\n              custom&lt;TypeAlias&gt;($target_encoding, type($target)) (`{` $target_dims^ `}`)?\n              attr-dict\n</code></pre>  Defines an import of an external HAL buffer view into a SSA-form tensor. An optional semaphore timepoint can be specified indicating when the buffer view is available for use. If no semaphore timepoint is provided it is assumed the buffer view is immediately available.  The provided `target_encoding`, if different from the `target` type, indicates that the ABI-facing type may differ from the internal representation. The types must be bitcastable (same storage size) and dynamically shaped values must have the same number of dynamic dimensions. This allows for casting between rank-0 and rank-N types, different element types, etc.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), TiedOpInterface, Util_ShapeAwareOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>target_encoding</code>::mlir::TypeAttrany type attribute <code>name</code>::mlir::StringAttrstring attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | buffer or buffer_view | `target_dims` | index | `wait_fence` | fence  ##### Results:  | Result | Description | | :----: | ----------- | | `target` | tensor of any type values   ## Attribute definition  ### AffinityQueueAttr  specifies a set of allowed queues for an operation  WIP; see [#10765](https://github.com/openxla/iree/issues/10765). This may change in the future to either be a nested attribute on a larger affinity struct or be defined by an implementation of the affinity attr interface. For now this allows higher levels of the stack to specify queues such that the stream dialect can understand them and they can be lowered into the HAL dialect.  Specifies that an annotated operation or scope is only allowed to execute on the set of queues (0-64) provided. Operations will not run on other queues.  Example: <pre><code>// any queue\n#hal.affinity.queue&lt;*&gt;\n// queues 4 and 5\n#hal.affinity.queue&lt;[4, 5]&gt;\n</code></pre>  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | mask | `int64_t` |  |  ### CollectiveAttr  collective operation and specification  Syntax:  <pre><code>#hal.collective&lt;\n  CollectiveKind,   # kind\n  std::optional&lt;CollectiveReductionOp&gt;,   # reduction\n  CollectiveElementType   # element_type\n&gt;\n</code></pre>  Specifies the collective operation to perform and any mode bits required.  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | kind | `CollectiveKind` |  | | reduction | `std::optional` |  | | element_type | `CollectiveElementType` |  |  ### DescriptorSetBindingAttr  descriptor set binding specification  Syntax:  <pre><code>#hal.descriptor_set.binding&lt;\n  int64_t,   # ordinal\n  DescriptorType,   # type\n  std::optional&lt;DescriptorFlags&gt;   # flags\n&gt;\n</code></pre>  Specifies a single binding within a descriptor set layout.  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | ordinal | `int64_t` |  | | type | `DescriptorType` |  | | flags | `std::optional` |  |  ### DescriptorSetLayoutAttr  descriptor set layout specification  Syntax:  <pre><code>#hal.descriptor_set.layout&lt;\n  int64_t,   # ordinal\n  ::llvm::ArrayRef&lt;DescriptorSetBindingAttr&gt;,   # bindings\n  std::optional&lt;DescriptorSetLayoutFlags&gt;   # flags\n&gt;\n</code></pre>  Specifies the layout information of a single set of descriptors used within an pipeline layout. Multiple of these sets may be used by a single entry point to allow for bindings with similar update frequencies to be grouped.  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | ordinal | `int64_t` |  | | bindings | `::llvm::ArrayRef` |  | | flags | `std::optional` |  |  ### DescriptorTypeAttr  valid DescriptorType  Syntax:  <pre><code>#hal.descriptor_type&lt;\n  ::mlir::iree_compiler::IREE::HAL::DescriptorType   # value\n&gt;\n</code></pre>  Enum cases: * uniform_buffer (`UniformBuffer`) * storage_buffer (`StorageBuffer`) ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | value | `::mlir::iree_compiler::IREE::HAL::DescriptorType` | an enum of type DescriptorType |  ### DeviceMatchArchitectureAttr  matches against a device architecture pattern  Matches a device by its runtime architecture. The format of the architecture pattern is device-dependent.  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | pattern | `StringAttr` |  |  ### DeviceMatchExecutableFormatAttr  matches when a device supports the given executable format  Matches a device only if it claims to support the given executable format pattern. It's still possible that the executable cannot be loaded such as if it uses unavailable device features. This is used for queries such as \"can you load ELF libraries?\" to quickly get to a set of executables to attempt without needing to try dozens that definitely cannot be loaded.  Note that different devices may share the same executable formats: for example a local synchronous CPU executor and a remote asynchronous CPU executor can both load ELF libraries. It's also possible for the same device to support multiple formats such as being able to load both platform-agnostic ELF libraries and platform-specific DLL/MachO/etc libraries.  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | pattern | `StringAttr` |  |  ### DeviceMatchFeatureAttr  matches against a supported device feature pattern  Matches a device that supports the given feature. The format of the feature pattern is device-dependent.  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | pattern | `StringAttr` |  |  ### DeviceMatchIDAttr  matches against a device ID pattern  Matches a device by its canonical compiler/runtime ID.  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | pattern | `StringAttr` |  |  ### DeviceTargetAttr  generic device target specification  Specifies the properties of a target runtime device. Target devices are specified with a canonical identifier matching those used by the runtime (such as `cpu`, `vulkan`, etc). Target devices may support several target executable formats specified with `#hal.executable.target`. An optional configuration dictionary allows for overriding backend defaults.  Example: <pre><code>#hal.device.target&lt;\"llvm-cpu\", {\n  executable_targets = [\n    #hal.executable.target&lt;\"llvm-cpu\", \"embedded-elf-arm_32\"&gt;,\n    #hal.executable.target&lt;\"llvm-cpu\", \"embedded-elf-arm_64\"&gt;,\n  ]\n}&gt;\n</code></pre>  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | deviceID | `StringAttr` |  | | configuration | `DictionaryAttr` |  |  ### ExecutableObjectAttr  object file reference  Defines an object file that can be linked into executables. Today this is only supported for external file references with paths the compiler can successfully resolve from its current working directory. Inlined data can optionally be provided to avoid the need for file system access and ensure the data source is attached to the IR as it makes its way through multiple compiler stages or reproducers.  Future revisions may change this to an interface that allows both internal and external resources to define the object contents. Linking needs to be updated to support various object compositions and certain backends may require additional infrastructure support.  In the long term the goal is to allow combinations of declared objects and generated code in order to give control of linking behavior to frontends. Instead of needing global command line flags to link in additional blobs the frontend can emit executables with the dependencies already defined per variant without needing to reach into the IREE compiler code.  Example: <pre><code>#hal.executable.object&lt;{path = \"some/file.obj\"}&gt;\n#hal.executable.object&lt;{\n  path = \"some/embedded/file.obj\",\n  data = dense&lt;[...]&gt; : vector&lt;2048xi8&gt;\n}&gt;\n</code></pre>  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | path | `StringAttr` |  | | data | `DenseIntElementsAttr` |  |  ### ExecutableObjectsAttr  target-specific object file references  A dictionary mapping executable target specifications to a list of objects. This is used to allow layers of the stack that support multi-targeting to specify information used during lowering into each particular target.  The key attributes are matched against each target variant based on the backend and format as well as any configuration data provided. When comparing the configuration only fields present in both the key and target variant will be checked and must match. This allows specification of generic sets (\"all x86_64 targets get these objects\") as well as specific ones (\"only x86_64 targets with vector_size = 64 get these objects\").  Example: <pre><code>#hal.executable.objects&lt;{\n  #hal.executable.target&lt;\"llvm-cpu\", \"embedded-elf-arm_64\"&gt; = [\n    #hal.executable.object&lt;{path = \"some/file_arm_64.obj\"}&gt;\n  ],\n  #hal.executable.target&lt;\"llvm-cpu\", \"embedded-elf-x86_64\"&gt; = [\n    #hal.executable.object&lt;{path = \"some/file_x86_64.obj\"}&gt;\n  ]\n}&gt;\n</code></pre>  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | targets | `ArrayAttr` |  | | targetObjects | `ArrayAttr` |  |  ### ExecutableTargetAttr  generic executable target specification  Specifies how to compile an executable for a specific target backend. A backend is used to translate and serialize the executable into the final form passed to the runtime. The format of the executable is a target-specific value indicating the required runtime support to load the deployed artifact. An optionally provided configuration dictionary overrides backend-specific defaults.  Example: <pre><code>  // Produce a system-native ELF for x86-64 systems using the LLVM backend:\n  #hal.executable.target&lt;\"llvm-cpu\", \"system-elf-x86_64\", {\n    triple = \"x86_64-unknown-linux-elf\",\n    cpu = \"host\",\n    cpu_features = \"host\",\n    abi = \"lp32\",\n    ...\n  }&gt;\n</code></pre>  The same compilation backend may be used to translate executables for several different runtime devices. Likewise the same runtime device may use one of many different executable targets. Assume an N:M mapping between the two in all cases.  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | backend | `StringAttr` |  | | format | `StringAttr` |  | | configuration | `DictionaryAttr` |  |  ### InterfaceBindingAttr  interface binding specification  Syntax:  <pre><code>#hal.interface.binding&lt;\n  int64_t,   # set\n  int64_t   # binding\n&gt;\n</code></pre>  Specifies the descriptor set and binding ordinal of a particular layout binding.  Example: <pre><code>#hal.interface.binding&lt;0, 1&gt;\n</code></pre>  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | set | `int64_t` |  | | binding | `int64_t` |  |  ### MatchAllAttr  matches if all subexpressions match  Returns true only if all subexpressions return true (logical AND) or empty.  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | conditions | `ArrayAttr` |  |  ### MatchAlwaysAttr  always matches  Syntax: `#hal.match.always`  Returns true (constant true).  ### MatchAnyAttr  matches if any subexpression matches  Returns true if any subexpression matches (logical OR) and not empty.  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | conditions | `ArrayAttr` |  |  ### PipelineLayoutAttr  executable entry point layout specification  Syntax:  <pre><code>#hal.pipeline.layout&lt;\n  int64_t,   # pushConstants\n  ::llvm::ArrayRef&lt;DescriptorSetLayoutAttr&gt;   # setLayouts\n&gt;\n</code></pre>  Specifies the layout information used for interacting with executable functions. This allows host code to correctly map parameters to the lower-level target-specific argument passing behavior.  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | pushConstants | `int64_t` |  | | setLayouts | `::llvm::ArrayRef` |  |  ## Type constraint definition  ### allocator  Allocates buffers for a particular device memory space.   ### buffer  A memory buffer with a specific memory_type that is used to describe the capabilities and behavior of the backing memory of the buffer. Buffers may be any mix of host-accessible, host-coherent, or device-accessible for various usages. Depending on these memory types the buffers may be mapped for access on the host as memory though certain restrictions may be imposed.   ### buffer_view  A shaped and typed buffer reference. This just wraps an existing hal.buffer with its associated metadata to make it easier to pass across ABI boundaries. In most cases buffer views can be elided entirely by the compiler and they'll only be seen when calling external functions.   ### collective.channel  Channel identifier used to allow for participation in multiple collective groups.   ### command_buffer  Asynchronous command buffer recording interface. Commands are recorded by the implementation for later submission to command queues.   ### descriptor_set_layout  Descriptor set layout.   ### device  Logical device instance.   ### event  Events are used for defining synchronization scopes within CommandBuffers. An event only exists within a single CommandBuffer and must not be used across CommandBuffers from the same device or others.   ### executable  A prepared and ready-to-dispatch executable.   ### fence  A set of semaphore timepoints defining a common point in time across multiple timelines.   ### buffer  A stateless file handle that can be read/written using queue-ordered transfer operations.   ### pipeline_layout  A pipeline layout describing the descriptor sets and push constants used."},{"location":"reference/mlir-dialects/HALInline/","title":"HALInline","text":""},{"location":"reference/mlir-dialects/HALInline/#hal_inline-dialect","title":"'hal_inline' Dialect","text":"<p>IREE inline HAL interop runtime module dialect.</p> <p>Low-level dialect for limited in-process ABI interop with the full HAL. Only operates synchronously, single-threaded, and on host-local buffers. Use the full HAL for all other cases.</p> <p>This dialect can be used alongside the full HAL but is intended for use in standalone configurations or paired with the <code>hal_loader</code> dialect which also carries the same usage restrictions.</p> <p>See <code>hal_inline.imports.mlir</code> for the full list of exported functions.</p> <ul> <li>'hal_inline' Dialect<ul> <li>Operation definition<ul> <li>Buffer ops<ul> <li>hal_inline.buffer.allocate.initialized (HAL::Inline::BufferAllocateInitializedOp)</li> <li>hal_inline.buffer.allocate (HAL::Inline::BufferAllocateOp)</li> <li>hal_inline.buffer.length (HAL::Inline::BufferLengthOp)</li> <li>hal_inline.buffer.storage (HAL::Inline::BufferStorageOp)</li> <li>hal_inline.buffer.subspan (HAL::Inline::BufferSubspanOp)</li> <li>hal_inline.buffer.wrap (HAL::Inline::BufferWrapOp)</li> </ul> </li> <li>Buffer view ops<ul> <li>hal_inline.buffer_view.assert (HAL::Inline::BufferViewAssertOp)</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/mlir-dialects/HALInline/#operation-definition","title":"Operation definition","text":""},{"location":"reference/mlir-dialects/HALInline/#buffer-ops","title":"Buffer ops","text":"<p>Ops for <code>!hal.buffer</code> / <code>iree_hal_buffer_t</code>.</p>"},{"location":"reference/mlir-dialects/HALInline/#hal_inlinebufferallocateinitialized-halinlinebufferallocateinitializedop","title":"<code>hal_inline.buffer.allocate.initialized</code> (HAL::Inline::BufferAllocateInitializedOp)","text":"<p>Buffer allocation with cloning</p> <p>Syntax:</p> <pre><code>operation ::= `hal_inline.buffer.allocate.initialized` `source` `(` $source `:` type($source) `)` `` `[` $offset `,` $length `]`\n              `alignment` `(` $minimum_alignment `)`\n              `:` custom&lt;SizeAwareType&gt;(type($result), ref($length)) `in` type($storage)\n              attr-dict-with-keyword\n</code></pre> <p>Allocates a buffer with a copy of the provided contents.</p> <p>Interfaces: OpAsmOpInterface, SizeAwareOpInterface</p>"},{"location":"reference/mlir-dialects/HALInline/#operands","title":"Operands:","text":"Operand Description <code>minimum_alignment</code> index <code>source</code> a reference counted byte buffer <code>offset</code> index <code>length</code> index"},{"location":"reference/mlir-dialects/HALInline/#results","title":"Results:","text":"Result Description <code>result</code> buffer <code>storage</code> a reference counted byte buffer"},{"location":"reference/mlir-dialects/HALInline/#hal_inlinebufferallocate-halinlinebufferallocateop","title":"<code>hal_inline.buffer.allocate</code> (HAL::Inline::BufferAllocateOp)","text":"<p>Empty buffer allocation operation</p> <p>Syntax:</p> <pre><code>operation ::= `hal_inline.buffer.allocate` `alignment` `(` $minimum_alignment `)`\n              `:` custom&lt;SizeAwareType&gt;(type($result), $allocation_size) `in` type($storage)\n              attr-dict-with-keyword\n</code></pre> <p>Allocates a buffer of the given size. The size of the buffer returned may be larger than the requested size if the allocator has specific alignment requirements or minimum allocation sizes.</p> <p>Interfaces: OpAsmOpInterface, SizeAwareOpInterface</p>"},{"location":"reference/mlir-dialects/HALInline/#operands_1","title":"Operands:","text":"Operand Description <code>minimum_alignment</code> index <code>allocation_size</code> index"},{"location":"reference/mlir-dialects/HALInline/#results_1","title":"Results:","text":"Result Description <code>result</code> buffer <code>storage</code> a reference counted byte buffer"},{"location":"reference/mlir-dialects/HALInline/#hal_inlinebufferlength-halinlinebufferlengthop","title":"<code>hal_inline.buffer.length</code> (HAL::Inline::BufferLengthOp)","text":"<p>Buffer byte length accessor</p> <p>Syntax:</p> <pre><code>operation ::= `hal_inline.buffer.length` `&lt;` $buffer `:` type($buffer) `&gt;`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre> <p>Returns the allocated size of a buffer in bytes. May be less than the underlying buffer allocation if this is a subspan or view into another buffer.</p> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/HALInline/#operands_2","title":"Operands:","text":"Operand Description <code>buffer</code> buffer"},{"location":"reference/mlir-dialects/HALInline/#results_2","title":"Results:","text":"Result Description <code>result</code> index"},{"location":"reference/mlir-dialects/HALInline/#hal_inlinebufferstorage-halinlinebufferstorageop","title":"<code>hal_inline.buffer.storage</code> (HAL::Inline::BufferStorageOp)","text":"<p>Buffer backing storage accessor</p> <p>Syntax:</p> <pre><code>operation ::= `hal_inline.buffer.storage` `&lt;` $buffer `:` type($buffer) `&gt;`\n              `:` type($storage)\n              attr-dict-with-keyword\n</code></pre> <p>Returns the host backing storage of the HAL buffer as a subspan limited to to the buffer's logical range (meaning that byte 0 of the returned buffer is byte 0 of the HAL buffer).</p> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/HALInline/#operands_3","title":"Operands:","text":"Operand Description <code>buffer</code> buffer"},{"location":"reference/mlir-dialects/HALInline/#results_3","title":"Results:","text":"Result Description <code>storage</code> a reference counted byte buffer"},{"location":"reference/mlir-dialects/HALInline/#hal_inlinebuffersubspan-halinlinebuffersubspanop","title":"<code>hal_inline.buffer.subspan</code> (HAL::Inline::BufferSubspanOp)","text":"<p>Buffer subspan operation</p> <p>Syntax:</p> <pre><code>operation ::= `hal_inline.buffer.subspan` `&lt;` $source_buffer `:` type($source_buffer) `&gt;`\n              `` `[` $source_offset `,` $length `]`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre> <p>Returns a reference to a subspan of the buffer.</p> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, SizeAwareOpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/HALInline/#operands_4","title":"Operands:","text":"Operand Description <code>source_buffer</code> buffer <code>source_offset</code> index <code>length</code> index"},{"location":"reference/mlir-dialects/HALInline/#results_4","title":"Results:","text":"Result Description <code>result</code> buffer"},{"location":"reference/mlir-dialects/HALInline/#hal_inlinebufferwrap-halinlinebufferwrapop","title":"<code>hal_inline.buffer.wrap</code> (HAL::Inline::BufferWrapOp)","text":"<p>Host buffer wrapping operation</p> <p>Syntax:</p> <pre><code>operation ::= `hal_inline.buffer.wrap` `source` `(` $source `:` type($source) `)` `` `[` $offset `,` $length `]`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre> <p>Tries wrapping a !hal.buffer around host memory backed by the given byte buffer.</p> <p>Interfaces: OpAsmOpInterface, SizeAwareOpInterface</p>"},{"location":"reference/mlir-dialects/HALInline/#operands_5","title":"Operands:","text":"Operand Description <code>source</code> a reference counted byte buffer <code>offset</code> index <code>length</code> index"},{"location":"reference/mlir-dialects/HALInline/#results_5","title":"Results:","text":"Result Description <code>result</code> buffer"},{"location":"reference/mlir-dialects/HALInline/#buffer-view-ops","title":"Buffer view ops","text":"<p>Ops for <code>!hal.buffer_view</code> / <code>iree_hal_buffer_view_t</code>.</p>"},{"location":"reference/mlir-dialects/HALInline/#hal_inlinebuffer_viewassert-halinlinebufferviewassertop","title":"<code>hal_inline.buffer_view.assert</code> (HAL::Inline::BufferViewAssertOp)","text":"<p>Buffer view contents assertion</p> <p>Syntax:</p> <pre><code>operation ::= `hal_inline.buffer_view.assert` `&lt;` $buffer_view `:` type($buffer_view) `&gt;`\n              `message` `(` $message `)`\n              `shape` `(` `[` $shape `]` `)`\n              `type` `(` $element_type `)`\n              `encoding` `(` $encoding_type `)`\n              attr-dict-with-keyword\n</code></pre> <p>Asserts that the buffer view contains a data compatible tensor with the given encoding. Program execution will abort as if <code>std.assert</code> had been used.</p>"},{"location":"reference/mlir-dialects/HALInline/#attributes","title":"Attributes:","text":"AttributeMLIR TypeDescription <code>message</code>::mlir::StringAttrstring attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `buffer_view` | buffer_view | `element_type` | 32-bit signless integer | `encoding_type` | 32-bit signless integer | `shape` | index  #### `hal_inline.buffer_view.buffer` (HAL::Inline::BufferViewBufferOp)  _Buffer view buffer accessor_   Syntax:  <pre><code>operation ::= `hal_inline.buffer_view.buffer` `&lt;` $buffer_view `:` type($buffer_view) `&gt;`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns the buffer backing this view's contents.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `buffer_view` | buffer_view  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | buffer  #### `hal_inline.buffer_view.create` (HAL::Inline::BufferViewCreateOp)  _Buffer view reference initializer_   Syntax:  <pre><code>operation ::= `hal_inline.buffer_view.create` `buffer` `(` $source_buffer `:` type($source_buffer) `)`\n              `` `[` $source_offset `,` $source_length `]`\n              `shape` `(` `[` $shape `]` `)`\n              `type` `(` $element_type `)`\n              `encoding` `(` $encoding_type `)`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Creates a reference to a buffer with a particular shape and element type. The buffer is not copied and both the original and view references must be synchronized. This makes it easier to associate commonly-carried metadata along with the contents.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source_buffer` | buffer | `source_offset` | index | `source_length` | index | `element_type` | 32-bit signless integer | `encoding_type` | 32-bit signless integer | `shape` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | buffer_view  #### `hal_inline.buffer_view.dim` (HAL::Inline::BufferViewDimOp)  _Buffer view dimension value query_   Syntax:  <pre><code>operation ::= `hal_inline.buffer_view.dim` `&lt;` $buffer_view `:` type($buffer_view) `&gt;`\n              `` `[` $index `]`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns the value of the given dimension.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>index</code>::mlir::IntegerAttrindex attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `buffer_view` | buffer_view  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index  #### `hal_inline.buffer_view.element_type` (HAL::Inline::BufferViewElementTypeOp)  _Buffer view element type query_   Syntax:  <pre><code>operation ::= `hal_inline.buffer_view.element_type` `&lt;` $buffer_view `:` type($buffer_view) `&gt;`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns the element type of the buffer view.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `buffer_view` | buffer_view  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `hal_inline.buffer_view.encoding_type` (HAL::Inline::BufferViewEncodingTypeOp)  _Buffer view encoding type query_   Syntax:  <pre><code>operation ::= `hal_inline.buffer_view.encoding_type` `&lt;` $buffer_view `:` type($buffer_view) `&gt;`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns the encoding type of the buffer view.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `buffer_view` | buffer_view  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `hal_inline.buffer_view.rank` (HAL::Inline::BufferViewRankOp)  _Buffer view rank query_   Syntax:  <pre><code>operation ::= `hal_inline.buffer_view.rank` `&lt;` $buffer_view `:` type($buffer_view) `&gt;`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns the rank of the buffer view.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `buffer_view` | buffer_view  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index  #### `hal_inline.buffer_view.trace` (HAL::Inline::BufferViewTraceOp)  _Trace value(s) operation_   Syntax:  <pre><code>operation ::= `hal_inline.buffer_view.trace` $operands `:` type($operands)\n              attr-dict-with-keyword\n</code></pre>  Traces out to a runtime trace sink (console, log file, etc) the given buffer views and titles them with the given key. The key is informational only and useful for titling/marking specific sets of buffers for easier searching.  ##### Attributes:   AttributeMLIR TypeDescription <code>key</code>::mlir::StringAttrstring attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operands` | buffer_view   ### Device ops  Ops for `!hal.device` / `iree_hal_device_t`.  #### `hal_inline.device.query` (HAL::Inline::DeviceQueryOp)  _Returns a runtime configuration parameter from the device_   Syntax:  <pre><code>operation ::= `hal_inline.device.query` `key` `(` $category `:` `` `:` $key `)`\n              `:` type($ok) `,` type($value)\n              (`=` $default_value^)?\n              attr-dict-with-keyword\n</code></pre>  Queries a device configuration parameter with the given key. Returns a status indicating whether the pair was recognized/available and if it was the value converted to the specified type. Queries must return the same value for the lifetime of the module though may vary from run to run.  This is roughly equivalent to the `sysconf` linux syscall (https://man7.org/linux/man-pages/man3/sysconf.3.html) in that the exact set of keys available and their interpretation is target-dependent. If there is a HAL match attribute (`#hal.device.match.*`) or op (`hal.device.match.*`) prefer to use that in order to get compile-time propagation when the target is specified and elide the runtime query and get compile-time verification when a runtime query is required.  Users of the op must check the `ok` result before using the value as what set of keys is available may change over time. If in doubt: don't use this. Each key used adds additional versioning and testing complexity as runtime code path changes will explode combinatorially and should be treated with as much care as a binary file format change. Keys should be prefixed with `ex.` when experimental indicating that they are not expected to be present forever; all non-experimental keys should be vetted.  Well-known keys: (none yet)  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>category</code>::mlir::StringAttrstring attribute <code>key</code>::mlir::StringAttrstring attribute <code>default_value</code>::mlir::Attributeany attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `ok` | 1-bit signless integer | `value` | any type"},{"location":"reference/mlir-dialects/HALLoader/","title":"HALLoader","text":""},{"location":"reference/mlir-dialects/HALLoader/#hal_loader-dialect","title":"'hal_loader' Dialect","text":"<p>IREE HAL inline executable loader runtime module dialect.</p> <p>Low-level dialect for dynamically loading executables and dispatching work. Only operates synchronously, single-threaded, and on host-local buffers. Use the full HAL for all other cases.</p> <p>This dialect can be used alongside the full HAL but is intended for use in conjunction with the <code>hal_inline</code> dialect which also carries the same usage restrictions.</p> <p>See <code>hal_loader.imports.mlir</code> for the full list of exported functions.</p> <ul> <li>'hal_loader' Dialect<ul> <li>Operation definition<ul> <li>Executable ops<ul> <li>hal_loader.executable.dispatch (HAL::Loader::ExecutableDispatchOp)</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/mlir-dialects/HALLoader/#operation-definition","title":"Operation definition","text":""},{"location":"reference/mlir-dialects/HALLoader/#executable-ops","title":"Executable ops","text":"<p>Ops for <code>!hal.executable</code> / <code>iree_hal_executable_t</code>.</p>"},{"location":"reference/mlir-dialects/HALLoader/#hal_loaderexecutabledispatch-halloaderexecutabledispatchop","title":"<code>hal_loader.executable.dispatch</code> (HAL::Loader::ExecutableDispatchOp)","text":"<p>Inline executable dispatch operation</p> <p>Syntax:</p> <pre><code>operation ::= `hal_loader.executable.dispatch` `executable` `(` $executable `:` type($executable) `)`\n              `` `[` $entry_point `]`\n              `workgroups` `(` `[`\n              $workgroup_x `,`\n              $workgroup_y `,`\n              $workgroup_z\n              `]` `)`\n              (`constants` `(` `[` $push_constants^ `]` `)`)?\n              `bindings` `(` `[`\n              custom&lt;DispatchBindings&gt;($binding_buffers,\n              type($binding_buffers),\n              $binding_offsets,\n              $binding_lengths)\n              `]` `)`\n              attr-dict-with-keyword\n</code></pre> <p>Dispatches execution to an executable entry point with the given parameters.</p> <p>Traits: AttrSizedOperandSegments</p>"},{"location":"reference/mlir-dialects/HALLoader/#attributes","title":"Attributes:","text":"AttributeMLIR TypeDescription <code>entry_point</code>::mlir::IntegerAttrsize_t   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `executable` | executable | `workgroup_x` | index | `workgroup_y` | index | `workgroup_z` | index | `push_constants` | 32-bit signless integer | `binding_buffers` | a reference counted byte buffer | `binding_offsets` | index | `binding_lengths` | index  #### `hal_loader.executable.dispatch.symbol` (HAL::Loader::ExecutableDispatchSymbolOp)  _Inline executable dispatch operation_   Syntax:  <pre><code>operation ::= `hal_loader.executable.dispatch.symbol` `executable` `(` $executable `:` type($executable) `)`\n              `target` `(` $entry_point `)`\n              `workgroups` `(` `[`\n              $workgroup_x `,`\n              $workgroup_y `,`\n              $workgroup_z\n              `]` `)`\n              (`constants` `(` `[` $push_constants^ `]` `)`)?\n              `bindings` `(` `[`\n              custom&lt;DispatchBindings&gt;($binding_buffers,\n              type($binding_buffers),\n              $binding_offsets,\n              $binding_lengths)\n              `]` `)`\n              attr-dict-with-keyword\n</code></pre>  Dispatches execution to an executable entry point with the given parameters. The entry point is a symbolic reference to an exported entry point.  Traits: AttrSizedOperandSegments  Interfaces: SymbolUserOpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>entry_point</code>::mlir::SymbolRefAttrsymbol reference attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `executable` | executable | `workgroup_x` | index | `workgroup_y` | index | `workgroup_z` | index | `push_constants` | 32-bit signless integer | `binding_buffers` | a reference counted byte buffer | `binding_offsets` | index | `binding_lengths` | index  #### `hal_loader.executable.load` (HAL::Loader::ExecutableLoadOp)  _Dynamically loads an executable_   Syntax:  <pre><code>operation ::= `hal_loader.executable.load` `format` `(` $format `)`\n              `data` `(` $data `)`\n              (`constants` `(` `[` $constants^ `]` `)`)?\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Creates, loads, and dynamically links an executable.  Optional constants provide for specialization of the executable based on runtime-derived parameters.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>format</code>::mlir::StringAttrstring attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `data` | a reference counted byte buffer | `constants` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | executable  #### `hal_loader.executable.lookup` (HAL::Loader::ExecutableLookupOp)  _Executable cache lookup pseudo-op_   Syntax:  <pre><code>operation ::= `hal_loader.executable.lookup` `executable` `(` $executable `)`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Used during conversion to provide a placeholder for a globally cached and possibly lazy-initialized executable.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, SymbolUserOpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>executable</code>::mlir::FlatSymbolRefAttrflat symbol reference attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | executable  #### `hal_loader.executable.query_support` (HAL::Loader::ExecutableQuerySupportOp)  _Queries whether an executable format is supported_   Syntax:  <pre><code>operation ::= `hal_loader.executable.query_support` `format` `(` $executable_format `)`\n              `:` type($supported)\n              attr-dict-with-keyword\n</code></pre>  Returns true if the given format is supported by the device loader. This does not guarantee that loading will succeed as the executable may require functionality that cannot be met my the hosting runtime environment.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>executable_format</code>::mlir::StringAttrstring attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `supported` | 1-bit signless integer"},{"location":"reference/mlir-dialects/IREEInput/","title":"IREEInput","text":""},{"location":"reference/mlir-dialects/IREEInput/#iree_input-dialect","title":"'iree_input' Dialect","text":"<p>Public ops/type/attributes legal for input to IREE's compiler.</p> <p>IREE's compiler allows as input a number of common dialects. This dialect contains structural and unique ops that do not exist elsewhere or that IREE has an interest in maintaining as a stable set.</p> <p>The contents of this dialect often mirror various constructs in IREE's internal implementation. The focus here is on simplicity and stability over time. Generally, this dialect does not use \"advanced\" features and should be broadly source compatible over a range of LLVM versions. There are of course, limits, and source-compatibility is not guaranteed, since LLVM/MLIR's API surface is itself unstable.</p> <ul> <li>'iree_input' Dialect<ul> <li>Operation definition<ul> <li>Buffer and buffer view ops<ul> <li>iree_input.buffer.subspan (Input::BufferSubspanOp)</li> <li>iree_input.buffer_view.create (Input::BufferViewCreateOp)</li> <li>iree_input.buffer_view.dim (Input::BufferViewDimOp)</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/mlir-dialects/IREEInput/#operation-definition","title":"Operation definition","text":""},{"location":"reference/mlir-dialects/IREEInput/#buffer-and-buffer-view-ops","title":"Buffer and buffer view ops","text":""},{"location":"reference/mlir-dialects/IREEInput/#iree_inputbuffersubspan-inputbuffersubspanop","title":"<code>iree_input.buffer.subspan</code> (Input::BufferSubspanOp)","text":"<p>Buffer subspan operation</p> <p>Syntax:</p> <pre><code>operation ::= `iree_input.buffer.subspan` `&lt;` $source_buffer `:` type($source_buffer) `&gt;`\n              `` `[` $source_offset `,` $length `]`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre> <p>Returns a reference to a subspan of the buffer.</p> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/IREEInput/#operands","title":"Operands:","text":"Operand Description <code>source_buffer</code> Buffer is an untyped bag of bits with no shape or dtype <code>source_offset</code> index <code>length</code> index"},{"location":"reference/mlir-dialects/IREEInput/#results","title":"Results:","text":"Result Description <code>result</code> Buffer is an untyped bag of bits with no shape or dtype"},{"location":"reference/mlir-dialects/IREEInput/#iree_inputbuffer_viewcreate-inputbufferviewcreateop","title":"<code>iree_input.buffer_view.create</code> (Input::BufferViewCreateOp)","text":"<p>Buffer view reference initializer</p> <p>Syntax:</p> <pre><code>operation ::= `iree_input.buffer_view.create` `buffer` `(` $source_buffer `:` type($source_buffer) `)`\n              `` `[` $source_offset `,` $source_length `]`\n              `shape` `(` `[` $shape `]` `)`\n              `type` `(` $element_type `)`\n              `encoding` `(` $encoding_type `)`\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre> <p>Creates a reference to a buffer with a particular shape and element type. The buffer is not copied and both the original and view references must be synchronized. This makes it easier to associate commonly-carried metadata along with the contents.</p> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface)</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/IREEInput/#operands_1","title":"Operands:","text":"Operand Description <code>source_buffer</code> Buffer is an untyped bag of bits with no shape or dtype <code>source_offset</code> index <code>source_length</code> index <code>element_type</code> 32-bit signless integer <code>encoding_type</code> 32-bit signless integer <code>shape</code> index"},{"location":"reference/mlir-dialects/IREEInput/#results_1","title":"Results:","text":"Result Description <code>result</code> View into a buffer, with runtime shape and element type"},{"location":"reference/mlir-dialects/IREEInput/#iree_inputbuffer_viewdim-inputbufferviewdimop","title":"<code>iree_input.buffer_view.dim</code> (Input::BufferViewDimOp)","text":"<p>Buffer view dimension value query</p> <p>Syntax:</p> <pre><code>operation ::= `iree_input.buffer_view.dim` $buffer_view `,` $index attr-dict `:` type($result)\n</code></pre> <p>Returns the value of the given dimension.</p> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface)</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/IREEInput/#attributes","title":"Attributes:","text":"AttributeMLIR TypeDescription <code>index</code>::mlir::IntegerAttrindex attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `buffer_view` | View into a buffer, with runtime shape and element type  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index  #### `iree_input.buffer_view.rank` (Input::BufferViewRankOp)  _Buffer view rank query_   Syntax:  <pre><code>operation ::= `iree_input.buffer_view.rank` $buffer_view attr-dict `:` type($result)\n</code></pre>  Returns the rank of the buffer view.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `buffer_view` | View into a buffer, with runtime shape and element type  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index   ### Byte buffer ops    #### `iree_input.byte_buffer.constant` (Input::ByteBufferConstantOp)  _Constant host-side byte buffer_   Syntax:  <pre><code>operation ::= `iree_input.byte_buffer.constant` ($name^)? attr-dict `:` type($result) `=` $value\n</code></pre>  Defines a compile-time byte buffer based on the given attribute value. The attribute will be serialized into the canonical IREE format for the chosen host target.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>name</code>::mlir::StringAttrstring attribute <code>value</code>::mlir::StringAttrstring attribute <code>alignment</code>::mlir::IntegerAttrindex attribute <code>mime_type</code>::mlir::StringAttrstring attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | a reference counted byte buffer   ### Compiler hint ops    #### `iree_input.optimization_barrier` (Input::OptimizationBarrierOp)  _Prevents compiler optimizations across a value._   Syntax:  <pre><code>operation ::= `iree_input.optimization_barrier` attr-dict\n              ($operands^ `:` type($operands))?\n</code></pre>  Wraps any operands in an unoptimizable identity to prevent its results from being folded. It will be dropped during the final step in compilation and has no effect at runtime.  Traits: SameOperandsAndResultType  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operands` | any type  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | any type   ### Dispatch ops    #### `iree_input.dispatch` (Input::DispatchOp)  _A dispatch of an executable across a grid_   Syntax:  <pre><code>operation ::= `iree_input.dispatch` $entry_point\n              (`[` $workload^ `]`)? ``\n              `(` $arguments `)` attr-dict `:`\n              custom&lt;ShapedFunctionType&gt;(ref($arguments),\n              type($arguments), $argument_dims,\n              type($results), $result_dims,\n              $tied_operands)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), SymbolUserOpInterface, TiedOpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>entry_point</code>::mlir::SymbolRefAttrsymbol reference attribute <code>tied_operands</code>::mlir::ArrayAttr64-bit integer array attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `workload` | index | `arguments` | any type | `argument_dims` | index | `result_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | any type   ### Executable source ops    #### `iree_input.executable.export` (Input::ExecutableExportOp)  _Executable entry point declaration_   Syntax:  <pre><code>operation ::= `iree_input.executable.export` custom&lt;SymbolVisibility&gt;($sym_visibility)\n              $sym_name\n              `ordinal` `(` $ordinal `)`\n              `layout` `(` $layout `)`\n              attr-dict-with-keyword\n</code></pre>   Traits: HasParent, IsolatedFromAbove  Interfaces: Symbol  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>sym_name</code>::mlir::StringAttrstring attribute <code>ordinal</code>::mlir::IntegerAttrsize_t <code>layout</code>::mlir::iree_compiler::IREE::Input::PipelineLayoutAttrexecutable entry point layout specification <code>workgroup_size</code>::mlir::ArrayAttrindex array attribute <code>subgroup_size</code>::mlir::IntegerAttrsize_t <code>workgroup_local_memory</code>::mlir::IntegerAttrindex attribute   #### `iree_input.executable.source_end` (Input::ExecutableSourceEndOp)  _Terminator pseudo-op for the executable source op_   Syntax:  <pre><code>operation ::= `iree_input.executable.source_end` attr-dict\n</code></pre>   Traits: HasParent, Terminator  #### `iree_input.executable.source` (Input::ExecutableSourceOp)  _Generic source contents of an executable op_   Syntax:  <pre><code>operation ::= `iree_input.executable.source` custom&lt;SymbolVisibility&gt;($sym_visibility)\n              $sym_name\n              attr-dict-with-keyword\n              ``\n              regions\n</code></pre>   Traits: IsolatedFromAbove, SingleBlock, SingleBlockImplicitTerminator, SymbolTable  Interfaces: Symbol  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>sym_name</code>::mlir::StringAttrstring attribute <code>objects</code>::mlir::iree_compiler::IREE::Input::ExecutableObjectsAttrtarget-specific object file references    ### Global variable ops    #### `iree_input.global.address` (Input::GlobalAddressOp)  _Returns an address reference to a global_   Syntax:  <pre><code>operation ::= `iree_input.global.address` $global attr-dict `:` type($result)\n</code></pre>  Returns the address of a global as a typed reference. Can be used with the global load and store indirect ops.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>global</code>FlatSymbolRefAttrsymbol reference attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values or index or signless integer or floating-point or complex-type  #### `iree_input.global.load.indirect` (Input::GlobalLoadIndirectOp)  _Loads a value from a global variable_   Syntax:  <pre><code>operation ::= `iree_input.global.load.indirect` $global attr-dict `:` type($global) `-&gt;` type($result)\n</code></pre>  Returns a copy of the global value.  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `global` | ranked tensor of any type values or index or signless integer or floating-point or complex-type  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | any type  #### `iree_input.global.load` (Input::GlobalLoadOp)  _Loads a value from a global variable_   Syntax:  <pre><code>operation ::= `iree_input.global.load` $global attr-dict `:` type($result)\n</code></pre>  Returns a copy of the global value.  Interfaces: SymbolUserOpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>global</code>FlatSymbolRefAttrsymbol reference attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | any type  #### `iree_input.global` (Input::GlobalOp)  _Stateful global variable declaration_   Syntax:  <pre><code>operation ::= `iree_input.global` custom&lt;SymbolVisibility&gt;($sym_visibility)\n              (`mutable` $is_mutable^)?\n              $sym_name\n              attr-dict\n              (`initializer` `(` $initializer^ `)`)?\n              custom&lt;TypeOrAttr&gt;($type, $initial_value)\n</code></pre>  Declares a global variable that maintains its value across invocations. The value is tied to the execution context of the module and different contexts will have different global storage.  Interfaces: Symbol  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>sym_name</code>::mlir::StringAttrstring attribute <code>type</code>::mlir::TypeAttrany type attribute <code>is_mutable</code>::mlir::UnitAttrunit attribute <code>initializer</code>::mlir::FlatSymbolRefAttrflat symbol reference attribute <code>initial_value</code>::mlir::TypedAttrTypedAttr instance{{% markdown %}}     This interface is used for attributes that have a type. The type of an     attribute is understood to represent the type of the data contained in the     attribute and is often used as the type of a value with this data.   {{% /markdown %}}   #### `iree_input.global.store.indirect` (Input::GlobalStoreIndirectOp)  _Stores a value into a global variable_   Syntax:  <pre><code>operation ::= `iree_input.global.store.indirect` $value `,` $global attr-dict `:` type($value) `-&gt;` type($global)\n</code></pre>  Stores a copy of the value into a global.  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | any type | `global` | ranked tensor of any type values or index or signless integer or floating-point or complex-type  #### `iree_input.global.store` (Input::GlobalStoreOp)  _Stores a value into a global variable_   Syntax:  <pre><code>operation ::= `iree_input.global.store` $value `,` $global attr-dict `:` type($value)\n</code></pre>  Stores a copy of the value into a global.  Interfaces: SymbolUserOpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>global</code>FlatSymbolRefAttrsymbol reference attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | any type   ### Mutable list ops    #### `iree_input.list.create` (Input::ListCreateOp)  _Creates a new empty list_   Syntax:  <pre><code>operation ::= `iree_input.list.create` ($initial_capacity^)? attr-dict `:` type($result)\n</code></pre>  Creates a new empty list with an optional initial capacity.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, MemoryEffectOpInterface (MemoryEffectOpInterface), NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{MemoryEffects::Allocate on ::mlir::SideEffects::DefaultResource}, MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `initial_capacity` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | list  #### `iree_input.list.get` (Input::ListGetOp)  _Element accessor_   Syntax:  <pre><code>operation ::= `iree_input.list.get` $list `[` $index `]` attr-dict `:` type($list) `-&gt;` type($result)\n</code></pre>  Returns the value of the element at the given index. Note that the value may be null if the element is null or the type does not match.  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `list` | list | `index` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | any type  #### `iree_input.list.resize` (Input::ListResizeOp)  _Resizes the list to a new count in elements_   Syntax:  <pre><code>operation ::= `iree_input.list.resize` operands attr-dict `:` type($list)\n</code></pre>  Resizes the list to contain `new_size` elements. This will either truncate the list if the existing size is greater than `new_size` or extend the list with the default list value of the element type.  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `list` | list | `new_size` | index  #### `iree_input.list.set` (Input::ListSetOp)  _Element mutator_   Syntax:  <pre><code>operation ::= `iree_input.list.set` $list `[` $index `]` `,` $value attr-dict `:` type($list) `,` type($value)\n</code></pre>  Sets the element at the given index to the new value.  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `list` | list | `index` | index | `value` | any type  #### `iree_input.list.size` (Input::ListSizeOp)  _The size of the list in elements_   Syntax:  <pre><code>operation ::= `iree_input.list.size` operands attr-dict `:` type($list)\n</code></pre>  Returns the current size of the list in elements.  Interfaces: InferTypeOpInterface, MemoryEffectOpInterface (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `list` | list  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index   ### Pseudo ops for conversion support    #### `iree_input.tensor.export` (Input::TensorExportOp)  _Exports a tensor to a Buffer(View), capturing dynamic dims_   Syntax:  <pre><code>operation ::= `iree_input.tensor.export` $source `:` type($source) (`{` $source_dims^ `}`)? `-&gt;` type($target)\n              attr-dict-with-keyword\n</code></pre>   Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | ranked tensor of any type values | `source_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `target` | Buffer is an untyped bag of bits with no shape or dtype or View into a buffer, with runtime shape and element type  #### `iree_input.tensor.import` (Input::TensorImportOp)  _Imports a Buffer(View) to a tensor, providing dynamic dims_   Syntax:  <pre><code>operation ::= `iree_input.tensor.import` $source `:` type($source) `-&gt;` type($target) (`{` $target_dims^ `}`)?\n              attr-dict-with-keyword\n</code></pre>   Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | Buffer is an untyped bag of bits with no shape or dtype or View into a buffer, with runtime shape and element type | `target_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `target` | ranked tensor of any type values   ### Tensor ops    #### `iree_input.tensor.clone` (Input::TensorCloneOp)  _Performs a full tensor clone operation_   Syntax:  <pre><code>operation ::= `iree_input.tensor.clone` $operand `:` type($result) (`{` $operand_dims^ `}`)?\n              attr-dict-with-keyword\n</code></pre>  Clones the input tensor into an identical output tensor.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | ranked tensor of any type values | `operand_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values  #### `iree_input.tensor.load` (Input::TensorLoadOp)  _Loads a value from a tensor element_   Syntax:  <pre><code>operation ::= `iree_input.tensor.load` $source (`[` $indices^ `]`)? `:`\n              type($source) (`{` $source_dims^ `}`)?\n              attr-dict-with-keyword\n</code></pre>  Returns the element at the given location from within the tensor.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | ranked tensor of any type values | `source_dims` | index | `indices` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index or signless integer or floating-point or complex-type or vector of any type values  #### `iree_input.tensor.reshape` (Input::TensorReshapeOp)  _Reshapes a tensor_   Syntax:  <pre><code>operation ::= `iree_input.tensor.reshape` $source `:`\n              type($source) (`{` $source_dims^ `}`)? `-&gt;`\n              type($result) (`{` $result_dims^ `}`)?\n              attr-dict-with-keyword\n</code></pre>  Reshapes a tensor to a new shape without modifying the contents.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | ranked tensor of any type values | `source_dims` | index | `result_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values  #### `iree_input.tensor.slice` (Input::TensorSliceOp)  _Slices out a subregion of a tensor_   Syntax:  <pre><code>operation ::= `iree_input.tensor.slice` $source `[` $start_indices `for` $lengths `]` `:`\n              type($source) (`{` $source_dims^ `}`)? `-&gt;`\n              type($result) (`{` $result_dims^ `}`)?\n              attr-dict-with-keyword\n</code></pre>  Clones a subregion of a tensor.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | ranked tensor of any type values | `source_dims` | index | `start_indices` | index | `lengths` | index | `result_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values  #### `iree_input.tensor.splat` (Input::TensorSplatOp)  _Splats a value into a shaped tensor_   Syntax:  <pre><code>operation ::= `iree_input.tensor.splat` $value `:` type($result) (`{` $result_dims^ `}`)?\n              attr-dict-with-keyword\n</code></pre>  Returns a tensor initialized to the given primitive value.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | index or signless integer or floating-point or complex-type | `result_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values  #### `iree_input.tensor.store` (Input::TensorStoreOp)  _Stores a value into a tensor element_   Syntax:  <pre><code>operation ::= `iree_input.tensor.store` $value `,` $target (`[` $indices^ `]`)? `:`\n              type($target) (`{` $target_dims^ `}`)?\n              attr-dict-with-keyword\n</code></pre>  Returns a tensor with the element at the given index set to the given value.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | index or signless integer or floating-point or complex-type or vector of any type values | `target` | ranked tensor of any type values | `target_dims` | index | `indices` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values  #### `iree_input.tensor.trace` (Input::TensorTraceOp)  _Trace value(s) operation_   Syntax:  <pre><code>operation ::= `iree_input.tensor.trace` $key attr-dict ($operands^ `:` type($operands))?\n</code></pre>  Traces out to a runtime trace sink (console, log file, etc) the given tensors and titles them with the given key. The key is informational only and useful for titling/marking specific sets of tensors for easier searching.  ##### Attributes:   AttributeMLIR TypeDescription <code>key</code>::mlir::StringAttrstring attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operands` | ranked tensor of any type values  #### `iree_input.tensor.update` (Input::TensorUpdateOp)  _Updates a tensor with the contents of another tensor_   Syntax:  <pre><code>operation ::= `iree_input.tensor.update` $update `,` $target `[` $start_indices `]` `:`\n              type($update) (`{` $update_dims^ `}`)? `-&gt;`\n              custom&lt;ShapedTiedResult&gt;(type($result), $target_dims)\n              attr-dict-with-keyword\n</code></pre>  Updates the target tensor with the contents of the update tensor at the given offset indices.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), TiedOpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `target` | ranked tensor of any type values | `target_dims` | index | `start_indices` | index | `update` | ranked tensor of any type values | `update_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values   ### Utility ops    #### `iree_input.align` (Input::AlignOp)  _Aligns up to a power-of-two alignment if required_   Syntax:  <pre><code>operation ::= `iree_input.align` $value `,` $alignment attr-dict `:` type($result)\n</code></pre>  Aligns |value| up to the given power-of-two |alignment| if required.  Traits: AlwaysSpeculatableImplTrait, SameOperandsAndResultType  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | signless-integer-like | `alignment` | signless-integer-like  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | signless-integer-like  #### `iree_input.null` (Input::NullOp)  _A null value_   Syntax:  <pre><code>operation ::= `iree_input.null` attr-dict `:` type($result)\n</code></pre>  Initializes reference and variant types with a null value.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | any type   ### Workgroup dispatch ops    #### `iree_input.dispatch.workgroup.count` (Input::DispatchWorkgroupCountOp)  _Returns the total workgroup count of the grid_   Syntax:  <pre><code>operation ::= `iree_input.dispatch.workgroup.count` `[` $dimension `]` attr-dict `:` type($result)\n</code></pre>  The total number of workgroups along each dimension in the dispatch grid.  Corresponds to the `NumWorkgroups` SPIR-V built-in and the `gridDim` CUDA built-in variable, only in the iree dialect the number of dimensions is not restricted to 3 (XYZ).  <pre><code>%x = iree_input.dispatch.workgroup.count[0] : index\n%y = iree_input.dispatch.workgroup.count[1] : index\n</code></pre>  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>dimension</code>::mlir::IntegerAttrindex attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index  #### `iree_input.dispatch.workgroup.id` (Input::DispatchWorkgroupIDOp)  _Returns the index of the current workgroup in the grid_   Syntax:  <pre><code>operation ::= `iree_input.dispatch.workgroup.id` `[` $dimension `]` attr-dict `:` type($result)\n</code></pre>  The global workgroup ID of the current workgroup in the range of `[0, iree_input.dispatch.workgroup.count)` along each dimension.  Corresponds to the `WorkgroupId` SPIR-V built-in and the `blockIdx` CUDA built-in variable, only in the iree dialect the number of dimensions is not restricted to 3 (XYZ).  <pre><code>%x = iree_input.dispatch.workgroup.id[0] : index\n%y = iree_input.dispatch.workgroup.id[1] : index\n</code></pre>  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>dimension</code>::mlir::IntegerAttrindex attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index  #### `iree_input.dispatch.workgroup.size` (Input::DispatchWorkgroupSizeOp)  _Returns the size of each workgroup in invocations_   Syntax:  <pre><code>operation ::= `iree_input.dispatch.workgroup.size` `[` $dimension `]` attr-dict `:` type($result)\n</code></pre>  The number of local invocations within the current workgroup along each dimension. Depending on backend this may map to the SIMT thread count or inner loop nest parameters.  Workgroup sizes are not determined at the iree dialect level as they are dependent on the target backend determined when lowering into the HAL. It's still possible to use the symbolic workgroup size inside of dispatch executables as a placeholder for the resolved value once in the HAL.  Corresponds to the `WorkgroupSize` SPIR-V built-in and the `blockDim` CUDA built-in variable, only in the iree dialect the number of dimensions is not restricted to 3 (XYZ).  <pre><code>%x = iree_input.dispatch.workgroup.size[0] : index\n%y = iree_input.dispatch.workgroup.size[1] : index\n</code></pre>  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>dimension</code>::mlir::IntegerAttrindex attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index   ## Attribute definition  ### DescriptorSetBindingAttr  descriptor set binding specification  Syntax:  <pre><code>#iree_input.descriptor_set.binding&lt;\n  int64_t,   # ordinal\n  DescriptorType,   # type\n  std::optional&lt;DescriptorFlags&gt;   # flags\n&gt;\n</code></pre>   ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | ordinal | `int64_t` |  | | type | `DescriptorType` |  | | flags | `std::optional` |  |  ### DescriptorSetLayoutAttr  descriptor set layout specification  Syntax:  <pre><code>#iree_input.descriptor_set.layout&lt;\n  int64_t,   # ordinal\n  ::llvm::ArrayRef&lt;DescriptorSetBindingAttr&gt;,   # bindings\n  std::optional&lt;DescriptorSetLayoutFlags&gt;   # flags\n&gt;\n</code></pre>   ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | ordinal | `int64_t` |  | | bindings | `::llvm::ArrayRef` |  | | flags | `std::optional` |  |  ### DescriptorTypeAttr  valid DescriptorType  Syntax:  <pre><code>#iree_input.descriptor_type&lt;\n  ::mlir::iree_compiler::IREE::Input::DescriptorType   # value\n&gt;\n</code></pre>  Enum cases: * uniform_buffer (`UniformBuffer`) * storage_buffer (`StorageBuffer`) ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | value | `::mlir::iree_compiler::IREE::Input::DescriptorType` | an enum of type DescriptorType |  ### DeviceTargetAttr  generic device target specification   ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | deviceID | `StringAttr` |  | | configuration | `DictionaryAttr` |  |  ### ExecutableObjectAttr  executable object reference   ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | path | `StringAttr` |  | | data | `DenseIntElementsAttr` |  |  ### ExecutableObjectsAttr  target-specific object file references   ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | targets | `ArrayAttr` |  | | targetObjects | `ArrayAttr` |  |  ### ExecutableTargetAttr  generic executable target specification   ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | backend | `StringAttr` |  | | format | `StringAttr` |  | | configuration | `DictionaryAttr` |  |  ### PipelineLayoutAttr  executable entry point layout specification  Syntax:  <pre><code>#iree_input.pipeline.layout&lt;\n  int64_t,   # pushConstants\n  ::llvm::ArrayRef&lt;DescriptorSetLayoutAttr&gt;   # setLayouts\n&gt;\n</code></pre>   ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | pushConstants | `int64_t` |  | | setLayouts | `::llvm::ArrayRef` |  |  ## Type constraint definition  ### list  A mutable, resizable list of some type.   ## Type definition  ### BufferType  Buffer is an untyped bag of bits with no shape or dtype  Syntax: `!iree_input.buffer`  Buffers represent an untyped bag of bits that can be reinterpreted depending on a use case using `buffer_view` operation. Buffers can be used for packing multiple tensors into the same underlying storage. It is left to higher level code to decide how exactly tensors layed out in the buffer.  ### BufferViewType  View into a buffer, with runtime shape and element type  Syntax: `!iree_input.buffer_view`  BufferViews represent views onto backing IREE runtime Buffer objects, adding runtime shape and element type parameters to the backing buffer. BufferViews are typically accepted and returned at boundaries with external code.  In the runtime and lower level compiler, BufferView's are fully modeled; however, as boundary types, not all features are exposed publicly. Since within compiled tensor programs, it is typical to operate in terms of fully typed tensors, the primary mechanism for getting or using a BufferView at the high level is by casting to/from a tensor. It is left to higher level code to ensure that aliasing rules are enforced at such boundaries.  ### ByteBufferType  a reference counted byte buffer  Syntax: `!iree_input.byte_buffer`  A reference counted byte buffer that models a pointer, offset, and length.  ### ListType  A one dimensional list of runtime values  Represents a list of arbitrary type. Primitive types can be expected to be efficiently stored in an unboxed form. Reference types and variants are permitted.  Lists can either be homogenous, with a fixed element type, or heterogenous by parameterizing them with a VariantType.  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | elementType | `::mlir::Type` | A type suitable as an element type of a container |  ### PtrType  Pointer to a concrete type   ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | targetType | `::mlir::Type` | A type suitable as a target type of a pointer |  ### VariantType  Represents any legal or reference type in the IREE runtime  Syntax: `!iree_input.variant`  The variant type is typically used to parameterize container types that can contain any legal primitive, reference or null in the IREE type system."},{"location":"reference/mlir-dialects/IREELinalgExt/","title":"IREELinalgExt","text":""},{"location":"reference/mlir-dialects/IREELinalgExt/#iree_linalg_ext-dialect","title":"'iree_linalg_ext' Dialect","text":"<p>IREE Linalg Extensions.</p> <p>A dialect designed for experimenting with non-structured operations that cannot be represented efficiently/directly by the Linalg dialect.</p> <ul> <li>'iree_linalg_ext' Dialect<ul> <li>Operation definition<ul> <li>Data tiling ops<ul> <li>iree_linalg_ext.pack (LinalgExt::PackOp)</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/mlir-dialects/IREELinalgExt/#operation-definition","title":"Operation definition","text":""},{"location":"reference/mlir-dialects/IREELinalgExt/#data-tiling-ops","title":"Data tiling ops","text":"<p>Operations for working with data layouts, padding, encodings, and other properties useful for tiling computations across iteration space dimensions.</p>"},{"location":"reference/mlir-dialects/IREELinalgExt/#iree_linalg_extpack-linalgextpackop","title":"<code>iree_linalg_ext.pack</code> (LinalgExt::PackOp)","text":"<p>Pack operation</p> <p>Syntax:</p> <pre><code>operation ::= `iree_linalg_ext.pack` attr-dict\n              $inputs\n              (`padding_value` `(` $padding_value^ `:` type($padding_value) `)`)?\n              (`outer_dims_perm` `=` $outer_dims_perm^)?\n              `inner_dims_pos` `=` $inner_dims_pos\n              `inner_tiles` `=`\n              custom&lt;DynamicIndexList&gt;($inner_tiles, $static_inner_tiles)\n              `into` $outputs `:` `(` type($inputs) type($outputs) `)`\n              (`-&gt;` type($results)^)?\n</code></pre> <p>The pack operation converts an <code>input</code> into a tiled and packed layout. The dimensions to be tiled are obtained from <code>inner_dims_pos</code> and the size of the tile is obtained from <code>inner_tiles</code>. The dimensions listed in <code>inner_dims_pos</code> do not need to be contiguous in which case the tile will get transposed.  We handle only full tiles if <code>padding_value</code> is not set; it is UB if the tile does not perfectly divide the dimension. If <code>padding_value</code> is set, it will pad along high dimensions, i.e., it pads at the bottom and on the right if the input has rank 2, and the result type shape, will be dynamic in any dimension if and only if the input shape is. As optional input, the operation takes <code>outer_dims_perm</code> that allows to permute the tiled loops.</p> <p>Example KC_to_KCck:</p> <pre><code>iree_linalg_ext.pack %arg0 inner_dims_pos = [1, 0]\n  inner_tiles = [32, 8] into %arg1 : (memref&lt;128x256xf32&gt; memref&lt;16x8x32x8xf32&gt;)\n</code></pre> <p>Example NC_to_NCnc:</p> <p><pre><code>iree_linalg_ext.pack %arg0 inner_dims_pos = [0, 1]\n  inner_tiles = [8, 32] into %arg1 : (memref&lt;128x256xf32&gt; memref&lt;16x8x8x32xf32&gt;)\n</code></pre> Example KC_to_CKkc</p> <pre><code>iree_linalg_ext.pack %arg0 outer_dims_perm = [1, 0] inner_dims_pos = [0, 1]\n  inner_tiles = [32, 8] into %arg1 : (memref&lt;128x256xf32&gt; memref&lt;32x4x32x8xf32&gt;)\n</code></pre> <p>In all cases, dimension at position 0 in the input memref (128) is tiled with a factor of 8, while dimension at position 1 (256) is tiled with a factor of 32. In the KC_to_KCck example, the point loops are interchanged, while in the KC_to_CKkc example the tiled loops.</p> <p>Example NC_to_NCnc with padding:</p> <pre><code>iree_linalg_ext.pack %arg padding_value(%pad : f32) inner_dims_pos = [0, 1]\n  inner_tiles = [8, 2] into %arg1 : (memref&lt;13x15xf32&gt; memref&lt;2x8x8x2xf32&gt;)\n</code></pre> <p>Traits: AttrSizedOperandSegments, SingleBlock, SingleBlockImplicitTerminator&lt;::mlir::iree_compiler::IREE::LinalgExt::YieldOp&gt;</p> <p>Interfaces: DestinationStyleOpInterface, LinalgExtInterface, LinalgExtOp, MemoryEffectOpInterface, ReifyRankedShapedTypeOpInterface, TilingInterface</p>"},{"location":"reference/mlir-dialects/IREELinalgExt/#attributes","title":"Attributes:","text":"AttributeMLIR TypeDescription <code>outer_dims_perm</code>::mlir::DenseI64ArrayAttri64 dense array attribute <code>inner_dims_pos</code>::mlir::DenseI64ArrayAttri64 dense array attribute <code>static_inner_tiles</code>::mlir::DenseI64ArrayAttri64 dense array attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `inputs` | shaped of any type values | `outputs` | shaped of any type values | `inner_tiles` | index | `padding_value` | any type  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | ranked tensor of any type values  #### `iree_linalg_ext.set_encoding` (LinalgExt::SetEncodingOp)  _Perform pack and pad operation on source_   Syntax:  <pre><code>operation ::= `iree_linalg_ext.set_encoding` attr-dict $source `:` type($source) `-&gt;` type($result)\n</code></pre>  Operation to assign an encoding to a tensor. The operation does not change the rank or extent of a tensor. Instead it adds an encoding attribute to the tensor type to represent a change in layout.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), ReifyRankedShapedTypeOpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | ranked tensor of any type values  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values  #### `iree_linalg_ext.unpack` (LinalgExt::UnPackOp)  _Unpack operation_   Syntax:  <pre><code>operation ::= `iree_linalg_ext.unpack` attr-dict\n              $inputs\n              (`outer_dims_perm` `=` $outer_dims_perm^)?\n              `inner_dims_pos` `=` $inner_dims_pos\n              `inner_tiles` `=`\n              custom&lt;DynamicIndexList&gt;($inner_tiles, $static_inner_tiles)\n              `into` $outputs `:` `(` type($inputs) type($outputs) `)`\n              (`-&gt;` type($results)^)?\n</code></pre>  The unpack operation converts a tiled and packed input to an unpacked output. See `pack` for more details on `inner_tiles` and `dims_pos`; it is UB if the tile does not perfectly divide the dimension. Optionally, the operation also supports permuting the tiled loops.  Example KCck_to_KC:  <pre><code>iree_linalg_ext.unpack %arg0 dims_pos = [1, 0]\n  inner_tiles = [32, 8] into %arg1 : (memref&lt;16x8x32x8xf32&gt; memref&lt;128x256xf32&gt;)\n</code></pre>  Example NCnc_to_NC:  <pre><code>iree_linalg_ext.unpack %arg0 dims_pos = [0, 1]\n  inner_tiles = [8, 32] into %arg1 : (memref&lt;16x8x8x32xf32&gt; memref&lt;128x256xf32&gt;)\n</code></pre>  Example CKkc_to_KC:  <pre><code>iree_linalg_ext.unpack %arg1 outer_dims_perm = [1, 0] inner_dims_pos = [0, 1]\n  inner_tiles = [32, 8] into %arg0 : (memref&lt;32x4x32x8xf32&gt; memref&lt;128x256xf32&gt;)\n</code></pre>  Traits: AttrSizedOperandSegments, SingleBlock, SingleBlockImplicitTerminator&lt;::mlir::iree_compiler::IREE::LinalgExt::YieldOp&gt;  Interfaces: DestinationStyleOpInterface, LinalgExtInterface, LinalgExtOp, MemoryEffectOpInterface, ReifyRankedShapedTypeOpInterface, TilingInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>outer_dims_perm</code>::mlir::DenseI64ArrayAttri64 dense array attribute <code>inner_dims_pos</code>::mlir::DenseI64ArrayAttri64 dense array attribute <code>static_inner_tiles</code>::mlir::DenseI64ArrayAttri64 dense array attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `inputs` | shaped of any type values | `outputs` | shaped of any type values | `inner_tiles` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | ranked tensor of any type values  #### `iree_linalg_ext.unset_encoding` (LinalgExt::UnsetEncodingOp)  _Perfom unpack and extract operation on source_   Syntax:  <pre><code>operation ::= `iree_linalg_ext.unset_encoding` attr-dict $source `:` type($source) `-&gt;` type($result)\n</code></pre>  Operation to convert an tensor with encoding that represents its data layout into a tensor with default layout (i.e. no encoding). For now in IREE the default layout is row-major.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), ReifyRankedShapedTypeOpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | ranked tensor of any type values  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values  #### `iree_linalg_ext.upper_bound_tile_size` (LinalgExt::UpperBoundTileSizeOp)  _Returns an upper bound on tile sizes_   Syntax:  <pre><code>operation ::= `iree_linalg_ext.upper_bound_tile_size` attr-dict $tensorType `-&gt;` type($results)\n</code></pre>  This returns the largest tile sizes that might result from materialization of the given encoding. This can be used outside of target-specific code, so there may be multiple targets, and this will return the maximum tile size from iterating over all of them. The evaluation happens in the MaterializeUpperBoundTileSize pass.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>tensorType</code>::mlir::TypeAttrtype attribute of ranked tensor of any type values   ##### Results:  | Result | Description | | :----: | ----------- | | `results` | index   ### Non-structured ops    #### `iree_linalg_ext.attention` (LinalgExt::AttentionOp)  _Attention operator_   Syntax:  <pre><code>operation ::= `iree_linalg_ext.attention` attr-dict\n              `ins` `(` $inputs `:` type($inputs) `)`\n              `outs` `(` $outputs `:` type($outputs) `)`\n              (`-&gt;` type($result)^)?\n</code></pre>  This operator takes in 3 tensors: query(Q), key(K) and value(V) and computes the attention. For self-attention, all inputs have the same shape BxNxd where B is the of the batch dimension, N is the sequence length and d is head dimension. Typically N &gt;&gt;&gt; d. Mathematically, the attention is defined as matmul(softmax(matmul(Q, transpose(K))), V) and has shape BxNxd. Usually, this operator also performs scaling, masking and dropout, but we leave that out of the current implementation. For cross-attention, the query and output have the same shape (BxNxd), while the key and value differ in sequence length (they have shape BxLxd, where L != N).  Traits: AttrSizedOperandSegments, SingleBlock, SingleBlockImplicitTerminator&lt;::mlir::iree_compiler::IREE::LinalgExt::YieldOp&gt;  Interfaces: DestinationStyleOpInterface, LinalgExtInterface, MemoryEffectOpInterface, ReifyRankedShapedTypeOpInterface, TilingInterface  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `inputs` | shaped of any type values | `outputs` | shaped of any type values  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values  #### `iree_linalg_ext.fft` (LinalgExt::FftOp)  _Fft operator_   Syntax:  <pre><code>operation ::= `iree_linalg_ext.fft` attr-dict (`ins` `(` $inputs^ `:` type($inputs) `)`)?\n              `outs` `(` $outputs `:` type($outputs) `)`\n              (`:` type($results)^)?\n</code></pre>  Apply 1D FFT to innermost dim. This is an iterative FFT, not recurrsive. Thus, the bit reversal is assumed applied on the input. The op carries an input -- stage, which indicates the level of reduction loop in the algorithm. It represents the computation body. For more details, see \"Data reordering, bit reversal, and in-place algorithms\" section in https://en.wikipedia.org/wiki/Cooley%E2%80%93Tukey_FFT_algorithm  The size of innermost dim is expected to be a power of 2.  It is optional to carry coefficient tensors/buffers as inputs. In this context, they will be the second and third inputs.  Traits: AttrSizedOperandSegments, SingleBlock, SingleBlockImplicitTerminator&lt;::mlir::iree_compiler::IREE::LinalgExt::YieldOp&gt;  Interfaces: DestinationStyleOpInterface, LinalgExtInterface, MemoryEffectOpInterface, ReifyRankedShapedTypeOpInterface, TilingInterface  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `inputs` | any type | `outputs` | shaped of any type values  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | ranked tensor of any type values  #### `iree_linalg_ext.reverse` (LinalgExt::ReverseOp)  _Reverse operator_   Syntax:  <pre><code>operation ::= `iree_linalg_ext.reverse` attr-dict `dimensions` `(` $dimensions `)`\n              (`ins` `(` $inputs^ `:` type($inputs) `)`)?\n              (`outs` `(` $outputs^ `:` type($outputs) `)`)?\n              (`:` type($results)^)?\n</code></pre>  A temporary solution for lowering reverse ops into IREE, allowing IREE to tile and distribute them. }  Traits: AttrSizedOperandSegments, SingleBlock, SingleBlockImplicitTerminator&lt;::mlir::iree_compiler::IREE::LinalgExt::YieldOp&gt;  Interfaces: DestinationStyleOpInterface, LinalgExtInterface, LinalgExtOp, MemoryEffectOpInterface, ReifyRankedShapedTypeOpInterface, TilingInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>dimensions</code>::mlir::DenseIntElementsAttr64-bit signless integer elements attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `inputs` | shaped of any type values | `outputs` | shaped of any type values  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | ranked tensor of any type values  #### `iree_linalg_ext.scan` (LinalgExt::ScanOp)  _Scan operator_   Syntax:  <pre><code>operation ::= `iree_linalg_ext.scan` attr-dict\n              `dimension` `(` $dimension `)`\n              `inclusive` `(` $inclusive `)`\n              `ins` `(` $inputs `:` type($inputs) `)`\n              `outs` `(` $outputs `:` type($outputs) `)`\n              $region (`-&gt;` type($results)^)?\n</code></pre>  Computes the inclusive/exclusive scan along a given dimension.  Traits: AttrSizedOperandSegments, SingleBlock, SingleBlockImplicitTerminator&lt;::mlir::iree_compiler::IREE::LinalgExt::YieldOp&gt;  Interfaces: DestinationStyleOpInterface, LinalgExtInterface, MemoryEffectOpInterface, ReifyRankedShapedTypeOpInterface, TilingInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>dimension</code>::mlir::IntegerAttr64-bit signless integer attribute <code>inclusive</code>::mlir::BoolAttrbool attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `inputs` | shaped of any type values | `outputs` | shaped of any type values  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | ranked tensor of any type values  #### `iree_linalg_ext.scatter` (LinalgExt::ScatterOp)  _Scatter operator_   Syntax:  <pre><code>operation ::= `iree_linalg_ext.scatter` attr-dict `dimension_map` `=` $dimension_map\n              `unique_indices` `(` $unique_indices `)`\n              (`ins` `(` $inputs^ `:` type($inputs) `)`)?\n              `outs` `(` $outputs `:` type($outputs) `)`\n              $region (`-&gt;` type($results)^)?\n</code></pre>  Based on XLA operation semantics, takes two `inputs` (`update` and `indices`) and `outputs` value (`original`). The operation updates the value at the slices specified by `indices` by combining the current value with the value in `updates` using the computation specified in `region`. The `region` specifies a binary operation of signature (T, T) -&gt; T, where `T` is the element-type of `updates` (and `original`). The first argument correspond the value to be updated (i.e. from `updates`), and the second the current value (i.e. value from `original`).  The `indices` is a 2D tensor/memref type. The first dim is the number of updates, and the second dim is index depth. The index depth should always be static.  The first dim of `updates` and `indices` is identical, since they represent the number of updates.  The rank of the `original`/`result` is at least `index_depth + rank(%updates) - 1`. The first `index_depth` indices are derived from `indices` and the shape of update value has the last rank(%original) - index_depth values match %(originals) last dimensions, with the previous dims extending from the index offsets.  The dimension_map attributes describes which index value maps to which dimension in the destionation. It cannot contain duplicate values, must have as many entries as index depth, and values must be within the rank of the destination.  The unique_indices attribute carries the information whether all the indices are unique. If there are repeated indices, the first iteration loop will be marked as reduction.  The shapes definition follows tensorflow operations execept that it force batch dims to be 1D. See more information in   https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update  Traits: AttrSizedOperandSegments, SingleBlock, SingleBlockImplicitTerminator&lt;::mlir::iree_compiler::IREE::LinalgExt::YieldOp&gt;  Interfaces: DestinationStyleOpInterface, LinalgExtInterface, MemoryEffectOpInterface, ReifyRankedShapedTypeOpInterface, TilingInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>dimension_map</code>::mlir::DenseI64ArrayAttri64 dense array attribute <code>unique_indices</code>::mlir::BoolAttrbool attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `inputs` | ranked tensor or memref of any type values | `outputs` | ranked tensor or memref of any type values  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | ranked tensor of any type values  #### `iree_linalg_ext.sort` (LinalgExt::SortOp)  _Sort operator_   Syntax:  <pre><code>operation ::= `iree_linalg_ext.sort` attr-dict\n              `dimension` `(` $dimension `)`\n              (`ins` `(` $inputs^ `:` type($inputs) `)`)?\n              `outs` `(` $outputs `:` type($outputs) `)`\n              $region (`-&gt;` type($results)^)?\n</code></pre>  Based on XLA operation semantics, sorts the given `operands` at the given `dimension` with the given `comparator`.  See https://www.tensorflow.org/xla/operation_semantics#sort.  Traits: AttrSizedOperandSegments, SingleBlock, SingleBlockImplicitTerminator&lt;::mlir::iree_compiler::IREE::LinalgExt::YieldOp&gt;  Interfaces: DestinationStyleOpInterface, LinalgExtInterface, MemoryEffectOpInterface, ReifyRankedShapedTypeOpInterface, TilingInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>dimension</code>::mlir::IntegerAttr64-bit signless integer attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `inputs` | any type | `outputs` | shaped of any type values  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | ranked tensor of any type values  #### `iree_linalg_ext.topk` (LinalgExt::TopkOp)  _Top-K operator_   Syntax:  <pre><code>operation ::= `iree_linalg_ext.topk` attr-dict\n              `dimension` `(` $dimension `)`\n              `ins` `(` $inputs `:` type($inputs) `)`\n              `outs` `(` $outputs `:` type($outputs) `)`\n              $region (`-&gt;` type($results)^)?\n</code></pre>  A Top-K operation for N-D tensors. Reduces the target dimension from the input size N down to K elements based on the supplied binary region.  Accepts an N-D tensor input consisting of values and an optioanl N-D tensor for indices of those values (i32 type). If input indices aren't provided, the index mapping is inferred based on the k dim.  Both input values/indices tensors and output values/indicies tensors must have the same shape. Top-K is computed along the target dimension (from dimension()). Returns two output tensors of values and the indicies of Top-K results. The output dimensions must match the input save for the dimension that is reduced to K results.  Region accepts lhs=[next N input] and rhs=[exiting K output] and yeilds an i1. If true, the two values are swapped:   - For Top-K compoarision: &gt;   - For Min-K comparision: &lt; Note: when the two values are equal, the first occurence is always selected.  Traits: AttrSizedOperandSegments, SingleBlock, SingleBlockImplicitTerminator&lt;::mlir::iree_compiler::IREE::LinalgExt::YieldOp&gt;  Interfaces: DestinationStyleOpInterface, LinalgExtInterface, LinalgExtOp, MemoryEffectOpInterface, ReifyRankedShapedTypeOpInterface, TilingInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>dimension</code>::mlir::IntegerAttr64-bit signless integer attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `inputs` | shaped of any type values | `outputs` | shaped of any type values  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | ranked tensor of any type values   ### Utility ops    #### `iree_linalg_ext.transform.do_not_dce_operands` (LinalgExt::DoNotDCEOperandsOp)  _Unfoldable op that just keeps its operands live_   Syntax:  <pre><code>operation ::= `iree_linalg_ext.transform.do_not_dce_operands` attr-dict $operands `:` type($operands)\n</code></pre>  Unfoldable op that just keeps its operands live. This is to use with the transform dialect in case where transforms introduce IR that would be otherwise DCE'd by canonicalizations.  This op should be added to the transform dialect in the fullness of time but it can't be registered dynamically on the IREE side as that triggers errors since the op does not implement any transform interface.  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operands` | any type  #### `iree_linalg_ext.yield` (LinalgExt::YieldOp)  _LinalgExt yield op_   Syntax:  <pre><code>operation ::= `iree_linalg_ext.yield` attr-dict ($operands^ `:` type($operands))?\n</code></pre>  `iree_linalg_ext.yield` is a special terminator operation for blocks inside regions in `iree_linalg_ext` ops.  Traits: AlwaysSpeculatableImplTrait, ReturnLike, Terminator  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), RegionBranchTerminatorOpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operands` | any type   ### Winograd ops    #### `iree_linalg_ext.winograd.input_transform` (LinalgExt::WinogradInputTransformOp)  _Winograd Input Transform operator_   Syntax:  <pre><code>operation ::= `iree_linalg_ext.winograd.input_transform` attr-dict\n              `output_tile_size` `(` $output_tile_size `)`\n              `kernel_size` `(` $kernel_size `)`\n              `image_dimensions` `(` $image_dimensions `)`\n              `ins` `(` $inputs `:` type($inputs) `)`\n              `outs` `(` $outputs `:` type($outputs) `)`\n              (`-&gt;` type($result)^)?\n</code></pre>  This operator is the first step in converting a convolution to its Winograd equivalent. Given a tile of an input image (I), this operator computes matmul(tranpose(B), matmul(I, B)). The input tile is assumed to be square with each side of size m + r - 1, where the convolutional kernel is m x m and the output tile size is r x r. B is a constant 2-d square matrix of the same shape as the input tile I. The input to the operator is an image of shape (N, H, W, C) or (N, C, H, W) and the output is an operator of shape (m + r - 1, m + r - 1, N, H', W', C) where H' = ceil((H - m + 1)/r) and W' = ceil((W - m + 1)/r). The result of this operator is first collapsed and then fed to a batch matmul op.  Traits: AttrSizedOperandSegments, SingleBlock, SingleBlockImplicitTerminator&lt;::mlir::iree_compiler::IREE::LinalgExt::YieldOp&gt;  Interfaces: DestinationStyleOpInterface, LinalgExtInterface, MemoryEffectOpInterface, ReifyRankedShapedTypeOpInterface, TilingInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>output_tile_size</code>::mlir::IntegerAttr64-bit signless integer attribute <code>kernel_size</code>::mlir::IntegerAttr64-bit signless integer attribute <code>image_dimensions</code>::mlir::DenseI64ArrayAttri64 dense array attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `inputs` | shaped of any type values | `outputs` | shaped of any type values  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values  #### `iree_linalg_ext.winograd.output_transform` (LinalgExt::WinogradOutputTransformOp)  _Winograd Output Transform operator_   Syntax:  <pre><code>operation ::= `iree_linalg_ext.winograd.output_transform` attr-dict\n              `output_tile_size` `(` $output_tile_size `)`\n              `kernel_size` `(` $kernel_size `)`\n              `image_dimensions` `(` $image_dimensions `)`\n              `ins` `(` $inputs `:` type($inputs) `)`\n              `outs` `(` $outputs `:` type($outputs) `)`\n              (`-&gt;` type($result)^)?\n</code></pre>  This operator is the last transform in converting a convolution to its Winograd equivalent. After convolution in the Winograd domain (which turns into an elementwise product for a single channel and batch matrix multiplication for many channels), this operator converts the output back into the original domain. Given a tile of the output (O) in the Winograd domain, this operator computes matmul(transpose(A), matmul(O, A)). The output tile is square with each side of size m + r - 1, where the convolutional kernel is m x m and the output tile size is r x r. A is a constant 2-d matrix of shape (m + r - 1) x r. The input to the operator is a tensor of shape (m + r - 1, m + r - 1, N, H', W', C) and the output is a tensor of shape (N, H, W, C) or (N, C, H, W) where H = r H' and W = r W'. This operator is followed by a tensor.extract_slice which extracts only the non-padded part of the output.  Traits: AttrSizedOperandSegments, SingleBlock, SingleBlockImplicitTerminator&lt;::mlir::iree_compiler::IREE::LinalgExt::YieldOp&gt;  Interfaces: DestinationStyleOpInterface, LinalgExtInterface, MemoryEffectOpInterface, ReifyRankedShapedTypeOpInterface, TilingInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>output_tile_size</code>::mlir::IntegerAttr64-bit signless integer attribute <code>kernel_size</code>::mlir::IntegerAttr64-bit signless integer attribute <code>image_dimensions</code>::mlir::DenseI64ArrayAttri64 dense array attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `inputs` | shaped of any type values | `outputs` | shaped of any type values  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ranked tensor of any type values   ## Attribute definition  ### EncodingAttr  information to decide how to data-tile a tensor  Syntax:  <pre><code>#iree_linalg_ext.encoding&lt;\n  EncodingUserAttr,   # user\n  EncodingRoleAttr,   # role\n  TypeAttr   # originalType\n&gt;\n</code></pre>  This attribute describes the change in the layout for a given tensor to execute subsequent operations on the tiled layout. The encoding serves as a way to represent the change in the way the data is laid out in memory without changing the logical rank/extent of the tensor itself. When required, the encoding can be used to explicitly manifest the layout change through operations like pack/unpack.  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | user | `EncodingUserAttr` | kind of operation using this tensor | | role | `EncodingRoleAttr` | role of this tensor as an operand | | originalType | `TypeAttr` | type of the original tensor type before padding |  ### EncodingRoleAttr  Describes the role of the tensor as an operand or a result of an operation.  Syntax:  <pre><code>#iree_linalg_ext.role&lt;\n  ::mlir::iree_compiler::IREE::LinalgExt::EncodingRole   # value\n&gt;\n</code></pre>  Enum cases: * LHS (`LHS`) * RHS (`RHS`) * RESULT (`RESULT`) ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | value | `::mlir::iree_compiler::IREE::LinalgExt::EncodingRole` | an enum of type EncodingRole |  ### EncodingUserAttr  Describes the operation that a tensor is an operand or a result of.  Syntax:  <pre><code>#iree_linalg_ext.user&lt;\n  ::mlir::iree_compiler::IREE::LinalgExt::EncodingUser   # value\n&gt;\n</code></pre>  Enum cases: * MATMUL_F32F32F32 (`MATMUL_F32F32F32`) * MATMUL_I8I8I32 (`MATMUL_I8I8I32`) * MATMUL_F16F16F32 (`MATMUL_F16F16F32`) * MATMUL_F16F16F16 (`MATMUL_F16F16F16`) * MATMUL_BF16BF16F32 (`MATMUL_BF16BF16F32`) * MATMUL_BF16BF16BF16 (`MATMUL_BF16BF16BF16`) * BATCH_MATMUL_F32F32F32 (`BATCH_MATMUL_F32F32F32`) * BATCH_MATMUL_I8I8I32 (`BATCH_MATMUL_I8I8I32`) * BATCH_MATMUL_F16F16F32 (`BATCH_MATMUL_F16F16F32`) * BATCH_MATMUL_F16F16F16 (`BATCH_MATMUL_F16F16F16`) * BATCH_MATMUL_BF16BF16F32 (`BATCH_MATMUL_BF16BF16F32`) * BATCH_MATMUL_BF16BF16BF16 (`BATCH_MATMUL_BF16BF16BF16`) ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | value | `::mlir::iree_compiler::IREE::LinalgExt::EncodingUser` | an enum of type EncodingUser |"},{"location":"reference/mlir-dialects/Stream/","title":"Stream","text":""},{"location":"reference/mlir-dialects/Stream/#stream-dialect","title":"'stream' Dialect","text":"<p>A dialect designed to model execution partitioning and scheduling.</p> <p>The stream dialect is designed to take tensor programs and convert them to explicitly scheduled asynchronous programs. This includes placing ops on specific targets, partitioning the work between the targets, scheduling the work for concurrency, and encoding tensors into target-specific resources.</p> <pre><code>+--------+    +----------+    +-------+\n| flow.* | -&gt; | stream.* | -&gt; | hal.* |\n+--------+    +----------+    +-------+\n</code></pre> <p>This sits in-between the <code>flow</code> and <code>hal</code> dialects.</p> <ul> <li> <p><code>flow</code> models tensor programs by separating work into dispatchable   functions in order to isolate the main host program data flow and the   dense tensor compute operations.</p> </li> <li> <p><code>stream</code> models explicitly scheduled asynchronous programs by partitioning   the dispatchable work, specifying target affinities, encoding tensors into   target-specific forms, and scheduling the work to run concurrently.</p> </li> <li> <p><code>hal</code> models a low-level hardware abstraction layer used to manage   buffers and issue asynchronous work across a variety of device types. The   dialect is largely 1:1 with the IREE HAL C API.</p> </li> </ul> <p>Transforms in the dialect lower tensor values into opaque resources with the goal of ensuring no tensors survive in the IR. At entry <code>stream.tensor.*</code> ops are used to capture the source tensor encoding information (data type, shapes, etc) and then lowered into <code>stream.async.*</code> ops that model the asynchronous workloads on the opaque resources. The asynchronous operations are then partitioned, allocated, and scheduled for execution using the <code>stream.cmd.*</code> ops.</p> <p>It's intended that after transformation through the stream dialect the program is ready for execution on an abstract machine. At this level of representation buffers have still not been allocated and devices are not yet resolved, however the information captured in the <code>stream</code> IR allows such operations to be done trivially. To this end all ops carry the symbolic size of the resources on which they operate as well as the lifetime of the resources they are acting upon. This manifests in the usage of the <code>!stream.resource</code> type:</p> <pre><code>// Unresolved lifetime (resolved during the iree-stream-refine-usage pass):\n!stream.resource&lt;*&gt;\n// An externally managed value (passed in via the program API).\n!stream.resource&lt;external&gt;\n// A staging buffer for uploads/downloads.\n!stream.resource&lt;staging&gt;\n// A short-lived value that is used across streams.\n!stream.resource&lt;transient&gt;\n// A long-lived value that persists across streams in globals.\n!stream.resource&lt;variable&gt;\n// An immutable value that persists for the duration of the program.\n!stream.resource&lt;constant&gt;\n</code></pre> <p>Operations using resources carry the size of all operand result resources:</p> <pre><code>// %update (40 bytes) is being inserted into %target (296 bytes).\n// Can be dynamic values such as those originating from dynamic dimensions.\n%13 = stream.async.update %update, %target[%c256 to %c296] :\n    !stream.resource&lt;transient&gt;{%c40} -&gt;\n    %target as !stream.resource&lt;transient&gt;{%c296}\n</code></pre> <p>Once all <code>stream.async.*</code> work is moved into executable regions (such as <code>stream.async.execute</code>) <code>!stream.timepoint</code> values are used to sequence the execution. These timepoints represent some point in time where all execution up to that timepoint has completed and any results that were produced by the execution are available for use. Attempting to use the resources before their corresponding timepoint has been reached will lead to undefined behavior. The benefit of this is that after timepoints are established in the IR it's possible to induce aliasing of resources without breaking execution correctness.</p> <ul> <li>'stream' Dialect<ul> <li>Operation definition<ul> <li>Async control flow ops<ul> <li>stream.async.call (Stream::AsyncCallOp)</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/mlir-dialects/Stream/#operation-definition","title":"Operation definition","text":""},{"location":"reference/mlir-dialects/Stream/#async-control-flow-ops","title":"Async control flow ops","text":""},{"location":"reference/mlir-dialects/Stream/#streamasynccall-streamasynccallop","title":"<code>stream.async.call</code> (Stream::AsyncCallOp)","text":"<p>Calls a streamable external host function</p> <p>Syntax:</p> <pre><code>operation ::= `stream.async.call` (`on` `(` $affinity^ `)`)?\n              $callee ``\n              custom&lt;DispatchOperands&gt;($resource_operands,\n              $resource_operand_offsets,\n              $resource_operand_ends,\n              $resource_operand_lengths) attr-dict `:`\n              custom&lt;ShapedFunctionType&gt;(ref($resource_operands),\n              type($resource_operands), $resource_operand_sizes,\n              type($results), $result_sizes,\n              $tied_operands)\n</code></pre> <p>Calls a function taking/returning resource values with stream semantics. Asynchronous calls must have no side-effects.</p> <p>Note that returned resources must have their sizes declared prior to the call as this is what allows the call to be made on the stream. If external host logic is required to compute the size (avoid at all costs!) a separate func.call can be used outside of the stream to do so. If sizes are unknownable until the operation is performed it should be made as a normal asynchronous host call with 'coarse-fences' instead.</p> <p>Traits: AttrSizedOperandSegments, Stream_AsyncPhaseOp</p> <p>Interfaces: AsyncAccessOpInterface, CallOpInterface, Stream_AffinityOp, Stream_StreamableOp, SymbolUserOpInterface, TiedOpInterface, Util_SizeAwareOp</p>"},{"location":"reference/mlir-dialects/Stream/#attributes","title":"Attributes:","text":"AttributeMLIR TypeDescription <code>callee</code>::mlir::FlatSymbolRefAttrflat symbol reference attribute <code>tied_operands</code>::mlir::ArrayAttr64-bit integer array attribute <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `resource_operands` | resource or external resource or transient resource or variable resource or constant resource or index or integer or floating-point or complex-type or any type | `resource_operand_sizes` | index | `resource_operand_offsets` | index | `resource_operand_ends` | index | `resource_operand_lengths` | index | `result_sizes` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | resource or external resource or transient resource or variable resource or constant resource or index or integer or floating-point or complex-type  #### `stream.async.concurrent` (Stream::AsyncConcurrentOp)  _Executes all ops concurrently_   Syntax:  <pre><code>operation ::= `stream.async.concurrent` (`on` `(` $affinity^ `)`)?\n              `with` ``\n              custom&lt;ResourceRegion&gt;($resource_operands,\n              type($resource_operands), $resource_operand_sizes,\n              type($results), $result_sizes,\n              $tied_operands, $body)\n              attr-dict-with-keyword\n</code></pre>  Represents a wave of work scheduled concurrently (each op executing at the same time). All resource inputs must be captured explicitly. All results are only ready once all nested ops complete execution.  Waves can be nested to create a DAG. For example, take the following graph: <pre><code>                  |\n        v---------+---------v\n+-------|-------+   +-------|-------+\n|    v--+--v    |   |    v--+--v    |\n| +----+ +----+ |   | +----+ +----+ |\n| | %a | | %b | |   | | %c | | %d | |\n| +----+ +----+ |   | +----+ +----+ |\n|    +--v--+    |   |    +--v--+    |\n+-------|-------+   +-------|-------+\n        +---------v---------+\n                  |\n</code></pre>  Represented with nested waves: <pre><code>  %0 = stream.async.concurrent with(%arg) -&gt; ... {\n    %1 = stream.async.concurrent with(%arg as %arg0) -&gt; ... {\n      %a = ...\n      %b = ...\n      stream.yield %a, %b\n    }\n    %2 = stream.async.concurrent with(%arg as %arg1) -&gt; ... {\n      %c = ...\n      %d = ...\n      stream.yield %c, %d\n    }\n    stream.yield %1, %2\n  }\n</code></pre>  Traits: AttrSizedOperandSegments, HasParent, RecursiveMemoryEffects, SingleBlock, SingleBlockImplicitTerminator, Stream_AsyncPhaseOp  Interfaces: AsyncAccessOpInterface, ClosureOpInterface, RegionBranchOpInterface, Stream_AffinityOp, Stream_StreamableOp, TiedOpInterface, Util_SizeAwareOp  ##### Attributes:   AttributeMLIR TypeDescription <code>tied_operands</code>::mlir::ArrayAttr64-bit integer array attribute <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `resource_operands` | resource or external resource or transient resource or variable resource or constant resource or staging resource | `resource_operand_sizes` | index | `result_sizes` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | resource or external resource or transient resource or variable resource or constant resource or staging resource  #### `stream.async.execute` (Stream::AsyncExecuteOp)  _Executes a dependency-aware sequence of streamable ops_   Syntax:  <pre><code>operation ::= `stream.async.execute` (`on` `(` $affinity^ `)`)?\n              (`await` `(` $await_timepoint^ `)` `=` `` `&gt;`)?\n              `with` ``\n              custom&lt;ResourceRegion&gt;($resource_operands,\n              type($resource_operands), $resource_operand_sizes,\n              type($results), $result_sizes,\n              $tied_operands, $body)\n              `=` `` `&gt;` type($result_timepoint)\n              attr-dict-with-keyword\n</code></pre>  Evaluates the operations within the region by dependency order while obeying ties when present. Nested ops execute serially in block order and nested `stream.async.concurrent` ops can be used to run multiple ops concurrently within the stream. All resource inputs must be captured explicitly. All results are only ready once all nested ops complete execution and the returned timepoint is reached. Zero or more timepoints may be provided to block execution until they are all reached; zero timepoints indicates that execution may begin immediately.  Traits: AttrSizedOperandSegments, RecursiveMemoryEffects, SingleBlock, SingleBlockImplicitTerminator, Stream_AsyncPhaseOp  Interfaces: AsyncAccessOpInterface, ClosureOpInterface, RegionBranchOpInterface, Stream_AffinityOp, Stream_TimelineOp, TiedOpInterface, Util_SizeAwareOp  ##### Attributes:   AttributeMLIR TypeDescription <code>tied_operands</code>::mlir::ArrayAttr64-bit integer array attribute <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `resource_operands` | resource or external resource or transient resource or variable resource or constant resource or staging resource | `resource_operand_sizes` | index | `result_sizes` | index | `await_timepoint` | a timepoint indicating execution availability  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | resource or external resource or transient resource or variable resource or constant resource or staging resource | `result_timepoint` | a timepoint indicating execution availability  #### `stream.async.func` (Stream::AsyncFuncOp)  _Streamable function declaration_   Syntax:  <pre><code>operation ::= `stream.async.func` custom&lt;SymbolVisibility&gt;($sym_visibility)\n              $sym_name\n              ``\n              custom&lt;ShapedFunctionSignature&gt;($function_type,\n              $tied_operands,\n              $arg_attrs,\n              $res_attrs)\n              attr-dict-with-keyword\n              ($body^)?\n</code></pre>  Declares a function that can be called as an asynchronous streaming operation via `stream.async.call`. Today only external functions are allowed.  Traits: IsolatedFromAbove, Stream_AsyncPhaseOp  Interfaces: CallableOpInterface, FunctionOpInterface, Symbol  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_name</code>::mlir::StringAttrstring attribute <code>function_type</code>::mlir::TypeAttrtype attribute of function type <code>tied_operands</code>::mlir::ArrayAttr64-bit integer array attribute <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>arg_attrs</code>::mlir::ArrayAttrArray of dictionary attributes <code>res_attrs</code>::mlir::ArrayAttrArray of dictionary attributes    ### Channel ops    #### `stream.channel.count` (Stream::ChannelCountOp)  _Returns the total number of participants in the group_   Syntax:  <pre><code>operation ::= `stream.channel.count` $channel `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns the total participant count in the collective communicator group.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `channel` | a collective communication channel  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index  #### `stream.channel.create` (Stream::ChannelCreateOp)  _Creates a new channel for collective communication_   Syntax:  <pre><code>operation ::= `stream.channel.create` (`on` `(` $affinity^ `)`)?\n              (`id` `(` $id^ `)`)?\n              (`group` `(` $group^ `)`)?\n              (`rank` `(` $rank^ `)`)?\n              (`count` `(` $count^ `)`)?\n              `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns a new channel with the given rank associated with the specified affinity. Collective operations using this channel must only be submitted on compatible affinities.  The group and ID are optional and may be null. The rank and count can be omitted to indicate a default inherited from the environment or device configuration at runtime.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, Stream_AffinityOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>group</code>::mlir::StringAttrstring attribute <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `id` | a reference counted byte buffer | `rank` | index | `count` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | a collective communication channel  #### `stream.channel.rank` (Stream::ChannelRankOp)  _Returns the rank of the local participant in the group_   Syntax:  <pre><code>operation ::= `stream.channel.rank` $channel `:` type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns the rank the channel represents as a participant in a collective group in `[0, count)`.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `channel` | a collective communication channel  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index  #### `stream.channel.split` (Stream::ChannelSplitOp)  _Splits a collective communication channel_   Syntax:  <pre><code>operation ::= `stream.channel.split` $channel `,` $color `,` $key\n              `:` type($channel) `-&gt;` type($result)\n              attr-dict-with-keyword\n</code></pre>  Partitions the group associated with the given channel into disjoint subgroups for each unique value of color. Each new subgroup contains all participants of the same color and within each subgroup the key argument is used to define the rank order. When multiple participants in a group use the same key the tie will be broken using their rank in the parent group. A color of -1 indicates that the rank does not participate in any subgroup and will return a null channel.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `channel` | a collective communication channel | `color` | index | `key` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | a collective communication channel   ### Executable ops    #### `stream.binding.subspan` (Stream::BindingSubspanOp)  _Returns an alias to a subspan of interface binding data_   Syntax:  <pre><code>operation ::= `stream.binding.subspan` $binding `` `[` $byte_offset `]`\n              attr-dict `:` type($binding) `-&gt;` type($result) (`{` $dynamic_dims^ `}`)?\n</code></pre>  Returns a subview to a tensor or memref-like type from a binding. The same binding may have multiple subviews at different byte offsets.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), Util_ShapeAwareOp  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `binding` | a managed resource binding into an executable scope | `byte_offset` | index | `dynamic_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | any type  #### `stream.executable.end` (Stream::ExecutableEndOp)  _Terminator pseudo-op for the executable op_   Syntax:  <pre><code>operation ::= `stream.executable.end` attr-dict\n</code></pre>   Traits: HasParent, Terminator  #### `stream.executable.export` (Stream::ExecutableExportOp)  _Defines an executable entry point for dispatch operations_   Syntax:  <pre><code>operation ::= `stream.executable.export` custom&lt;SymbolVisibility&gt;($sym_visibility)\n              custom&lt;SymbolAlias&gt;($sym_name, $function_ref)\n              custom&lt;WorkgroupCountRegion&gt;($workgroup_count)\n              attr-dict-with-keyword\n</code></pre>  Specifies an exported function with an externally-visible alias. Multiple exports can reference the same internal function.  Each entry point can have a unique workgroup count calculation region. This region takes the workload parameters passed to each flow.dispatch and produces an XYZ workgroup count for the 3D grid dispatch.  Traits: HasParent, IsolatedFromAbove  Interfaces: Symbol  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>sym_name</code>::mlir::StringAttrstring attribute <code>function_ref</code>::mlir::FlatSymbolRefAttrflat symbol reference attribute   #### `stream.executable` (Stream::ExecutableOp)  _Generic executable module_   Syntax:  <pre><code>operation ::= `stream.executable` custom&lt;SymbolVisibility&gt;($sym_visibility)\n              $sym_name\n              attr-dict-with-keyword\n              regions\n</code></pre>  An executable module containing one or more public functions. The contents of the functions are safe to dispatch and can be lowered further to target-specific backend IR representations.  Traits: IsolatedFromAbove, SingleBlock, SingleBlockImplicitTerminator, SymbolTable  Interfaces: Symbol  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>sym_name</code>::mlir::StringAttrstring attribute    ### Explicit command ops    #### `stream.cmd.call` (Stream::CmdCallOp)  _Calls a streamable external host function_   Syntax:  <pre><code>operation ::= `stream.cmd.call` $callee ``\n              custom&lt;CmdCallOperands&gt;($resource_operands,\n              $resource_operand_offsets,\n              $resource_operand_lengths,\n              $resource_operand_accesses) attr-dict `:`\n              custom&lt;ShapedFunctionType&gt;(ref($resource_operands),\n              type($resource_operands),\n              $resource_operand_sizes,\n              type($results),\n              $result_sizes,\n              $tied_operands)\n</code></pre>  Calls a function operating on resource values with stream semantics. Asynchronous calls must have no side-effects.  Traits: AttrSizedOperandSegments, Stream_CmdPhaseOp  Interfaces: CallOpInterface, Stream_StreamableOp, Stream_SubviewEffectOp, SymbolUserOpInterface, Util_SizeAwareOp  ##### Attributes:   AttributeMLIR TypeDescription <code>callee</code>::mlir::FlatSymbolRefAttrflat symbol reference attribute <code>tied_operands</code>::mlir::ArrayAttr64-bit integer array attribute <code>resource_operand_accesses</code>::mlir::ArrayAttraccess array attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `resource_operands` | index or integer or floating-point or complex-type or resource or external resource or transient resource or variable resource or constant resource or any type | `resource_operand_sizes` | index | `resource_operand_offsets` | index | `resource_operand_lengths` | index | `result_sizes` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | index or integer or floating-point or complex-type  #### `stream.cmd.collective` (Stream::CmdCollectiveOp)  _Dispatches a collective operation_   Syntax:  <pre><code>operation ::= `stream.cmd.collective` `` $op `` `[` $element_count `]`\n              `channel` `(` $channel `)`\n              (`param` `(` $param^ `:` type($param) `)`)? `{`\n              custom&lt;DispatchResources&gt;($resources, type($resources), $resource_sizes,\n              $resource_offsets, $resource_lengths,\n              $resource_accesses)\n              `\\n` `}`\n              attr-dict-with-keyword\n</code></pre>  Dispatches a collective operation specified against the device. If grouped with other collectives in a `stream.cmd.concurrent` region the collective operations may fuse and execute more efficiently.  Traits: AttrSizedOperandSegments, Stream_CmdPhaseOp  Interfaces: Stream_StreamableOp, Stream_SubviewEffectOp, Util_SizeAwareOp  ##### Attributes:   AttributeMLIR TypeDescription <code>op</code>::mlir::iree_compiler::IREE::Stream::CollectiveAttrcollective operation and specification{{% markdown %}}     Specifies the collective operation to perform and any mode bits required.   {{% /markdown %}} <code>resource_accesses</code>::mlir::ArrayAttraccess array attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `channel` | a collective communication channel | `element_count` | index | `param` | 32-bit signless integer | `resources` | resource or external resource or transient resource or variable resource or constant resource | `resource_sizes` | index | `resource_offsets` | index | `resource_lengths` | index  #### `stream.cmd.concurrent` (Stream::CmdConcurrentOp)  _Executes all ops concurrently_   Syntax:  <pre><code>operation ::= `stream.cmd.concurrent` $body\n              attr-dict-with-keyword\n</code></pre>  Represents a wave of work scheduled concurrently (each op executing at the same time).  Waves can be nested to create a DAG. For example, take the following graph: <pre><code>                  |\n        v---------+---------v\n+-------|-------+   +-------|-------+\n|    v--+--v    |   |    v--+--v    |\n| +----+ +----+ |   | +----+ +----+ |\n| | @a | | @b | |   | | @c | | @d | |\n| +----+ +----+ |   | +----+ +----+ |\n|    +--v--+    |   |    +--v--+    |\n+-------|-------+   +-------|-------+\n        +---------v---------+\n                  |\n</code></pre>  Represented with nested waves: <pre><code>  stream.cmd.concurrent {\n    stream.cmd.concurrent {\n      stream.cmd.dispatch @a\n      stream.cmd.dispatch @b\n    }\n    stream.cmd.concurrent {\n      stream.cmd.dispatch @c\n      stream.cmd.dispatch @d\n    }\n  }\n</code></pre>  Traits: HasParent, RecursiveMemoryEffects, SingleBlock, SingleBlockImplicitTerminator, Stream_CmdPhaseOp  Interfaces: RegionBranchOpInterface, Stream_StreamableOp  #### `stream.cmd.copy` (Stream::CmdCopyOp)  _Copies a subview of a stream resource to another_   Syntax:  <pre><code>operation ::= `stream.cmd.copy` $source `[` $source_offset `]` `,`\n              $target `[` $target_offset `]` `,`\n              $length `:`\n              type($source) `` `{` $source_size `}` `-&gt;`\n              type($target) `` `{` $target_size `}`\n              attr-dict-with-keyword\n</code></pre>  Copies a subview of a resource into a subview of another. As with memcpy this does not support overlapping updates into the same resource.  Traits: Stream_CmdPhaseOp  Interfaces: Stream_StreamableOp, Stream_SubviewEffectOp, Util_SizeAwareOp  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | any stream-compatible type | `source_size` | index | `source_offset` | index | `target` | any stream-compatible type | `target_size` | index | `target_offset` | index | `length` | index  #### `stream.cmd.discard` (Stream::CmdDiscardOp)  _Discards a subview of a resource_   Syntax:  <pre><code>operation ::= `stream.cmd.discard` $target `[` $target_offset `for` $target_length `]` `:`\n              type($target) `` `{` $target_size `}`\n              attr-dict-with-keyword\n</code></pre>  Discards a subview of a resource, indicating that after this command the specified contents are no longer needed. This can be used to trim memory or invalidate caches.  Traits: Stream_CmdPhaseOp  Interfaces: Stream_StreamableOp, Stream_SubviewEffectOp, Util_SizeAwareOp  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `target` | any stream-compatible type | `target_size` | index | `target_offset` | index | `target_length` | index  #### `stream.cmd.dispatch` (Stream::CmdDispatchOp)  _Dispatches a parallelized grid of work_   Syntax:  <pre><code>operation ::= `stream.cmd.dispatch` custom&lt;DispatchEntryPoints&gt;($entry_points)\n              (`[` $workload^ `]`)? ``\n              (`(` $uniform_operands^ `:` type($uniform_operands) `)`)? `{`\n              custom&lt;DispatchResources&gt;($resources, type($resources), $resource_sizes,\n              $resource_offsets, $resource_lengths,\n              $resource_accesses)\n              `\\n` `}`\n              attr-dict-with-keyword\n</code></pre>  Calls the specified entry point function once for each element in the specified workgroup count. Each workgroup has access to the same operands and results and is able to load/store at will.  Traits: AttrSizedOperandSegments, Stream_CmdPhaseOp  Interfaces: Stream_StreamableOp, Stream_SubviewEffectOp, SymbolUserOpInterface, Util_SizeAwareOp  ##### Attributes:   AttributeMLIR TypeDescription <code>entry_points</code>::mlir::ArrayAttrsymbol ref array attribute <code>resource_accesses</code>::mlir::ArrayAttraccess array attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `workload` | index | `uniform_operands` | index or integer or floating-point or complex-type | `resources` | resource or external resource or transient resource or variable resource or constant resource | `resource_sizes` | index | `resource_offsets` | index | `resource_lengths` | index  #### `stream.cmd.execute` (Stream::CmdExecuteOp)  _Executes a dependency-aware sequence of streamable ops_   Syntax:  <pre><code>operation ::= `stream.cmd.execute` (`on` `(` $affinity^ `)`)?\n              (`await` `(` $await_timepoint^ `)` `=` `` `&gt;`)?\n              `with` ``\n              custom&lt;ExplicitResourceRegion&gt;($resource_operands,\n              type($resource_operands), $resource_operand_sizes,\n              $body)\n              `=` `` `&gt;` type($result_timepoint)\n              attr-dict-with-keyword\n</code></pre>  Evaluates the operations within the region by dependency order while obeying ties when present. Nested ops execute serially in block order and nested `stream.cmd.concurrent` ops can be used to run multiple ops concurrently within the stream. All resource inputs must be captured explicitly. All results are only ready once all nested ops complete execution and the returned timepoint is reached. Zero or more timepoints may be provided to block execution until they are all reached; zero timepoints indicates that execution may begin immediately.  Traits: AttrSizedOperandSegments, RecursiveMemoryEffects, SingleBlock, SingleBlockImplicitTerminator, Stream_CmdPhaseOp  Interfaces: ClosureOpInterface, InferTypeOpInterface, RegionBranchOpInterface, Stream_AffinityOp, Stream_TimelineOp, Util_SizeAwareOp  ##### Attributes:   AttributeMLIR TypeDescription <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `resource_operands` | resource or external resource or transient resource or variable resource or constant resource or staging resource | `resource_operand_sizes` | index | `await_timepoint` | a timepoint indicating execution availability  ##### Results:  | Result | Description | | :----: | ----------- | | `result_timepoint` | a timepoint indicating execution availability  #### `stream.cmd.fill` (Stream::CmdFillOp)  _Fills a subview of a stream resource with a value_   Syntax:  <pre><code>operation ::= `stream.cmd.fill` $value `,`\n              $target `[` $target_offset `for` $target_length `]` `:`\n              type($value) `-&gt;`\n              type($target) `` `{` $target_size `}`\n              attr-dict-with-keyword\n</code></pre>  Splats a value into a subview of the given stream resource and returns the resource with the update applied.  Traits: Stream_CmdPhaseOp  Interfaces: Stream_StreamableOp, Stream_SubviewEffectOp, Util_SizeAwareOp  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `target` | resource or external resource or transient resource or variable resource or constant resource | `target_size` | index | `target_offset` | index | `target_length` | index | `value` | 8-bit signless integer or 16-bit signless integer or 32-bit signless integer  #### `stream.cmd.flush` (Stream::CmdFlushOp)  _Flushes a subview of a resource_   Syntax:  <pre><code>operation ::= `stream.cmd.flush` (`to` `(` $source_affinity^ `)`)?\n              $target `[` $target_offset `for` $target_length `]` `:`\n              type($target) `` `{` $target_size `}`\n              attr-dict-with-keyword\n</code></pre>  Transfers a resource to an external target. The resource memory is made available to the target and can be made visible there using `stream.cmd.invalidate`.  Traits: Stream_CmdPhaseOp  Interfaces: Stream_StreamableOp, Stream_SubviewEffectOp, Util_SizeAwareOp  ##### Attributes:   AttributeMLIR TypeDescription <code>source_affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `target` | any stream-compatible type | `target_size` | index | `target_offset` | index | `target_length` | index  #### `stream.cmd.func` (Stream::CmdFuncOp)  _Streamable function declaration_   Syntax:  <pre><code>operation ::= `stream.cmd.func` custom&lt;SymbolVisibility&gt;($sym_visibility)\n              $sym_name ``\n              custom&lt;DispatchFunctionSignature&gt;($function_type,\n              $arg_attrs,\n              $res_attrs)\n              attr-dict-with-keyword\n              ($body^)?\n</code></pre>  Declares a function that can be called as an asynchronous streaming operation via `stream.cmd.call`. Today only external functions are allowed.  Traits: IsolatedFromAbove, Stream_CmdPhaseOp  Interfaces: CallableOpInterface, FunctionOpInterface, Symbol  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_name</code>::mlir::StringAttrstring attribute <code>function_type</code>::mlir::TypeAttrtype attribute of function type <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>arg_attrs</code>::mlir::ArrayAttrArray of dictionary attributes <code>res_attrs</code>::mlir::ArrayAttrArray of dictionary attributes   #### `stream.cmd.invalidate` (Stream::CmdInvalidateOp)  _Invalidates a subview of a resource_   Syntax:  <pre><code>operation ::= `stream.cmd.invalidate` (`from` `(` $source_affinity^ `)`)?\n              $target `[` $target_offset `for` $target_length `]` `:`\n              type($target) `` `{` $target_size `}`\n              attr-dict-with-keyword\n</code></pre>  Transfers a resource from an external source into the current target. The resource memory is assumed to have been made available at the source via `stream.cmd.flush`.  Traits: Stream_CmdPhaseOp  Interfaces: Stream_StreamableOp, Stream_SubviewEffectOp, Util_SizeAwareOp  ##### Attributes:   AttributeMLIR TypeDescription <code>source_affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `target` | any stream-compatible type | `target_size` | index | `target_offset` | index | `target_length` | index  #### `stream.cmd.serial` (Stream::CmdSerialOp)  _Executes all ops serially (in-order)_   Syntax:  <pre><code>operation ::= `stream.cmd.serial` $body\n              attr-dict-with-keyword\n</code></pre>  Represents a sequence of work scheduled serially (each op executing one after the other).  Regions can be nested to create a DAG. For example, take the following graph: <pre><code>                  |\n        v---------+-----v\n+-------|-------+   +---|----+\n|    v--+--v    |   |   v    |\n| +----+ +----+ |   | +----+ |\n| | @a | | @b | |   | | @c | |\n| +----+ +----+ |   | +----+ |\n|    |     |    |   |   |    |\n|    |     |    |   | +-v--+ |\n|    |     |    |   | | @d | |\n|    |     |    |   | +----+ |\n|    +--v--+    |   |   |    |\n+-------|-------+   +---|----+\n        +---------v-----+\n                  |\n</code></pre>  Represented with nested regions: <pre><code>  stream.cmd.concurrent {\n    stream.cmd.concurrent {\n      stream.cmd.dispatch @a\n      stream.cmd.dispatch @b\n    }\n    stream.cmd.serial {\n      stream.cmd.dispatch @c\n      stream.cmd.dispatch @d\n    }\n  }\n</code></pre>  Traits: HasParent, RecursiveMemoryEffects, SingleBlock, SingleBlockImplicitTerminator, Stream_CmdPhaseOp  Interfaces: RegionBranchOpInterface, Stream_StreamableOp   ### File ops  File ops.  #### `stream.file.constant` (Stream::FileConstantOp)  _Creates a file backed by the provided constant host memory_   Syntax:  <pre><code>operation ::= `stream.file.constant` (`on` `(` $affinity^ `)`)?\n              $source `[` $source_offset `for` $source_length `]` `:`\n              type($source) `` `{` $source_size `}`\n              `-&gt;`\n              type($result)\n              attr-dict-with-keyword\n</code></pre>  Synchronously wraps a host heap buffer into a stream-accessible file handle. Changing the source buffer after definition has undefined behavior.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, MemoryEffectOpInterface (MemoryEffectOpInterface), NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, Stream_AffinityOp, SubrangeOperandOpInterface, Util_SizeAwareOp  Effects: MemoryEffects::Effect{MemoryEffects::Allocate on ::mlir::SideEffects::DefaultResource}, MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | a reference counted byte buffer | `source_size` | index | `source_offset` | index | `source_length` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | a file handle used for I/O operations  #### `stream.file.read` (Stream::FileReadOp)  _Reads a segment of a file into a resource_   Syntax:  <pre><code>operation ::= `stream.file.read` (`on` `(` $affinity^ `)`)?\n              (`await` `(` $await_timepoint^ `)` `=` `` `&gt;`):(`:`)?\n              $source `[` $source_offset `]` `,`\n              $target `[` $target_offset `]` `,`\n              $length `:`\n              type($source) `-&gt;`\n              type($target) `` `{` $target_size `}`\n              `=` `` `&gt;`\n              type($result_timepoint)\n              attr-dict-with-keyword\n</code></pre>  Asynchronously reads a segment of a file into a resource.  Some implementations this can stream directly from the file into device-local memory and should be preferred to manually staging memory through host buffers.  Traits: Stream_CmdPhaseOp  Interfaces: AffinityOpInterface, InferTypeOpInterface, Stream_TimelineOp, Util_SizeAwareOp  ##### Attributes:   AttributeMLIR TypeDescription <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | a file handle used for I/O operations | `source_offset` | 64-bit signless integer | `target` | resource or external resource or transient resource or variable resource or constant resource | `target_size` | index | `target_offset` | index | `length` | index | `await_timepoint` | a timepoint indicating execution availability  ##### Results:  | Result | Description | | :----: | ----------- | | `result_timepoint` | a timepoint indicating execution availability  #### `stream.file.write` (Stream::FileWriteOp)  _Writes a segment of a file from a resource_   Syntax:  <pre><code>operation ::= `stream.file.write` (`on` `(` $affinity^ `)`)?\n              (`await` `(` $await_timepoint^ `)` `=` `` `&gt;`):(`:`)?\n              $source `[` $source_offset `]` `,`\n              $target `[` $target_offset `]` `,`\n              $length `:`\n              type($source) `` `{` $source_size `}` `-&gt;`\n              type($target)\n              `=` `` `&gt;`\n              type($result_timepoint)\n              attr-dict-with-keyword\n</code></pre>  Asynchronously writes a segment of a resource into a file. The file range must be valid within the file as this operation cannot grow the underlying file storage.  Some implementations this can stream directly from device-local memory into the file and should be preferred to manually staging memory through host buffers.  Traits: Stream_CmdPhaseOp  Interfaces: AffinityOpInterface, InferTypeOpInterface, Stream_TimelineOp, Util_SizeAwareOp  ##### Attributes:   AttributeMLIR TypeDescription <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | resource or external resource or transient resource or variable resource or constant resource | `source_size` | index | `source_offset` | index | `target` | a file handle used for I/O operations | `target_offset` | 64-bit signless integer | `length` | index | `await_timepoint` | a timepoint indicating execution availability  ##### Results:  | Result | Description | | :----: | ----------- | | `result_timepoint` | a timepoint indicating execution availability   ### Miscellaneous ops    #### `stream.return` (Stream::ReturnOp)  _Returns results from a region_   Syntax:  <pre><code>operation ::= `stream.return` attr-dict\n              $operands `:` type($operands)\n</code></pre>  The values returned are copied by-value.  Traits: AlwaysSpeculatableImplTrait, HasParent, ReturnLike, Terminator  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), RegionBranchTerminatorOpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operands` | any type  #### `stream.yield` (Stream::YieldOp)  _Yields stream values from an execution region_   Syntax:  <pre><code>operation ::= `stream.yield` attr-dict\n              ($resource_operands^ `:`\n              custom&lt;SizeAwareTypeList&gt;(type($resource_operands),\n              $resource_operand_sizes))?\n</code></pre>  The values returned represent the asynchronous value at the point in time the SSA value is defined (or tied).  Traits: AlwaysSpeculatableImplTrait, HasParent, SameVariadicOperandSize, Terminator  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), RegionBranchTerminatorOpInterface, Util_SizeAwareOp  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `resource_operands` | resource or external resource or transient resource or variable resource or constant resource or staging resource | `resource_operand_sizes` | index   ### Pseudo Ops  Pseudo ops for conversion support.  #### `stream.tensor.export` (Stream::TensorExportOp)  _Conversion placeholder for stream-&gt;other type conversion_   Syntax:  <pre><code>operation ::= `stream.tensor.export` (`on` `(` $affinity^ `)`)?\n              $source `:`\n              $source_encoding (`` `{` $source_encoding_dims^ `}`)?\n              `in`\n              type($source) `` `{` $source_size `}`\n              `-&gt;`\n              type($result)\n              attr-dict-with-keyword\n</code></pre>  Defines a conversion to a higher-level dialect type such as `tensor` that is resolved during lowering into the stream dialect. This can be used to interoperate between levels of the stack that require specifying stream types and those that prior to lowering do not handle them.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), Stream_AffinityOp, TiedOpInterface, Util_ShapeAwareOp, Util_SizeAwareOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>source_encoding</code>::mlir::TypeAttrany type attribute <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | external resource | `source_encoding_dims` | index | `source_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | any type  #### `stream.tensor.import` (Stream::TensorImportOp)  _Conversion placeholder for other-&gt;stream type conversion_   Syntax:  <pre><code>operation ::= `stream.tensor.import` (`on` `(` $affinity^ `)`)?\n              $source `:`\n              type($source)\n              `-&gt;`\n              $result_encoding (`` `{` $result_encoding_dims^ `}`)?\n              `in`\n              type($result) `{` $result_size `}`\n              attr-dict-with-keyword\n</code></pre>  Defines a conversion from a higher-level dialect type such as `tensor` that is resolved during lowering into the stream dialect. This can be used to interoperate between levels of the stack that require specifying stream types and those that prior to lowering do not handle them.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), Stream_AffinityOp, TiedOpInterface, Util_ShapeAwareOp, Util_SizeAwareOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>result_encoding</code>::mlir::TypeAttrany type attribute <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | any type | `result_encoding_dims` | index | `result_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | external resource   ### Resource ops  Generic resource ops.  #### `stream.resource.alloc` (Stream::ResourceAllocOp)  _Allocates a persistent value with undefined contents_   Syntax:  <pre><code>operation ::= `stream.resource.alloc` (`on` `(` $affinity^ `)`)?\n              (`uninitialized` $uninitialized^)?\n              attr-dict `:` custom&lt;SizeAwareTypeList&gt;(type($results), $storage_sizes)\n</code></pre>  Allocates a persistent value (one that is long-lived and possibly external to the program) with undefined contents. Consumers of the allocated result must assume nothing of the contents and use `discard` access.  Uninitialized allocations will have undefined contents and must only be used when all bytes are discarded prior to any reads. Runtimes decide what \"undefined contents\" means and here it only indicates that execution will be correct even if the memory starts with non-zero values.  If multiple values are allocated from the same operation it implies that they have matching lifetimes. When lowering to execution environments the separate allocations may be fused into one or more slab allocations in order to reduce overheads. How many allocations can be fused is based on the size of the individual resources and the target constraints (how large any single buffer may be, etc). At the stream dialect level treat a multi-result alloc as a way to indicate similar lifetimes.  Traits: AlwaysSpeculatableImplTrait  Interfaces: AffinityOpInterface, ConditionallySpeculatable, MemoryEffectOpInterface (MemoryEffectOpInterface), Util_SizeAwareOp  Effects: MemoryEffects::Effect{MemoryEffects::Allocate on ::mlir::SideEffects::DefaultResource}  ##### Attributes:   AttributeMLIR TypeDescription <code>uninitialized</code>::mlir::UnitAttrunit attribute <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `storage_sizes` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | any stream-compatible type  #### `stream.resource.alloca` (Stream::ResourceAllocaOp)  _Allocates a transient value with undefined contents_   Syntax:  <pre><code>operation ::= `stream.resource.alloca` `uninitialized`\n              (`on` `(` $affinity^ `)`)?\n              (`await` `(` $await_timepoint^ `)` `=` `` `&gt;`):(`:`)?\n              attr-dict\n              type($result) `{` $storage_size `}`\n              `=` `` `&gt;`\n              type($result_timepoint)\n</code></pre>  Allocates a transient value (one that is short-lived and local to the current computation) with undefined contents. Consumers of the allocated result must assume nothing of the contents and use `discard` access.  The resource returned is not valid for use until the timepoint is reached; execution using this resource must await on the timepoint.  Traits: AlwaysSpeculatableImplTrait  Interfaces: AffinityOpInterface, ConditionallySpeculatable, MemoryEffectOpInterface (MemoryEffectOpInterface), Stream_TimelineOp, Util_SizeAwareOp  Effects: MemoryEffects::Effect{MemoryEffects::Allocate on ::mlir::SideEffects::DefaultResource}  ##### Attributes:   AttributeMLIR TypeDescription <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `storage_size` | index | `await_timepoint` | a timepoint indicating execution availability  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | staging resource or transient resource | `result_timepoint` | a timepoint indicating execution availability  #### `stream.resource.constants` (Stream::ResourceConstantsOp)  _Asynchronously uploads or maps constant values_   Syntax:  <pre><code>operation ::= `stream.resource.constants` (`on` `(` $affinity^ `)`)?\n              attr-dict `:`\n              custom&lt;ConstantValueList&gt;(type($results),\n              $result_sizes,\n              $values)\n              `\\n` ` ` ` ` `=` `` `&gt;` type($result_timepoint)\n</code></pre>  Represents an upload of constant resources that may be packed, suballocated, and mapped depending on the final lowering target.  In runtime environments where memory is shared between host and device this turns into a mapping operation that avoids additional memory allocation and copies. When memory cannot be shared an asynchronous stream will be created to allocate and copy all of the constant values.  Though this op returns a unique resource for each constant value it's expected that almost all end up aliasing into the same storage. The exact packing and number of storage resources that are needed are not known until lowering to a particular backend, though, so they are separate here for proper usage tracking.  Both constant and variable resources can be produced; a constant is immutable while a variable will be treated as a constant-value initializer for a mutable resource. By modeling these together it's not required that variable initializers first be allocated, copied to the target, and then copied into the variable storage if the target is capable of doing a direct upload or mapping.  Traits: AlwaysSpeculatableImplTrait, SameVariadicResultSize  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), Stream_AffinityOp, Stream_TimelineOp, Util_SizeAwareOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>values</code>::mlir::ArrayAttrconstant value array attribute <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `result_sizes` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | constant resource or variable resource | `result_timepoint` | a timepoint indicating execution availability  #### `stream.resource.dealloca` (Stream::ResourceDeallocaOp)  _Frees a transient value when available_   Syntax:  <pre><code>operation ::= `stream.resource.dealloca` (`on` `(` $affinity^ `)`)?\n              (`await` `(` $await_timepoint^ `)` `=` `` `&gt;`)?\n              $operand `:` type($operand) `{` $operand_size `}`\n              `=` `` `&gt;` type($result_timepoint)\n              attr-dict\n</code></pre>  Deallocates a transient value (one that is short-lived and local to the current computation) previously allocated using `stream.resource.alloca`.  The resource is considered live and valid until the provided timepoint is reached and the memory is only made available for future requests after the result timepoint is reached.  Interfaces: AffinityOpInterface, InferTypeOpInterface, MemoryEffectOpInterface (MemoryEffectOpInterface), Stream_TimelineOp, Util_SizeAwareOp  Effects: MemoryEffects::Effect{MemoryEffects::Free on ::mlir::SideEffects::DefaultResource}  ##### Attributes:   AttributeMLIR TypeDescription <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | staging resource or transient resource | `operand_size` | index | `await_timepoint` | a timepoint indicating execution availability  ##### Results:  | Result | Description | | :----: | ----------- | | `result_timepoint` | a timepoint indicating execution availability  #### `stream.resource.load` (Stream::ResourceLoadOp)  _Loads a value from a staging resource_   Syntax:  <pre><code>operation ::= `stream.resource.load` $source `[` $source_offset `]` `:`\n              type($source) `` `{` $source_size `}`\n              `-&gt;`\n              type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns the element(s) at the given offset in the staging resource. The operation will complete synchronously against the resource though it may introduce a yield point if the staging resource needs to be transferred.  Interfaces: Util_SizeAwareOp  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | staging resource | `source_size` | index | `source_offset` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index or integer or floating-point or complex-type or vector of any type values  #### `stream.resource.pack` (Stream::ResourcePackOp)  _Packs variable-sized slices into a single slab_   Syntax:  <pre><code>operation ::= `stream.resource.pack` (`on` `(` $affinity^ `)`)?\n              (`offset` `(` $offset^ `)`)?\n              `slices` `(` `{`\n              custom&lt;PackSliceRanges&gt;($lifetime_intervals,\n              $dynamic_slice_sizes,\n              type($packed_offsets))\n              `}` `)`\n              `:` type($total_length)\n              attr-dict-with-keyword\n</code></pre>  Performs a greedy packing of one or more sized slices with specified lifetimes and returns their relative offsets in an aliased linear space.  Slices are `[start, end] = %slice_byte_size`, where the start and end values define an inclusive lifetime range and the size is the total number of bytes required to be live for that range.  <pre><code>// Computes the total length required for the packed values and the offsets\n// of the 3 slices requested relative to the base of the packed memory:\n%total_length, %offset_0, %offset_1, %offset_2 =\n    stream.resource.pack\n        // Each slice gets one result offset:\n        slices({\n          // 3 slices where A and B overlap and will get unique offsets\n          // while B and C do not overlap and are allowed to alias.\n          [0, 10] = %size_0,  // A =&gt; %offset_0\n          [3,  8] = %size_1,  // B =&gt; %offset_1\n          [9, 10] = %size_2,  // C =&gt; %offset_2\n          ...\n        }) : index\n</code></pre>  The lifetime start and end points (inclusive) are only used for relative comparisons and may originate with any meaning (op order in block, epoch, phase of the moon, etc). The packing algorithm uses the intervals to determine slice liveness and when aliasing is safe.  The size of each slice may either be a constant or runtime-computed dynamic value. Constant slices can achieve more dense packing than the dynamic values and CSE/canonicalization should be applied to ensure that as many of the dynamic values are equivalent if possible.  The total length required to pack all slices is returned and can be used to acquire storage. The individual slice offsets are 0-based and as such if are directly used as buffer offsets may need additional offsetting. This can either be applied via the optional `offset` operand or slicing of the underlying allocation buffer.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, Stream_AffinityOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>lifetime_intervals</code>::mlir::ArrayAttrindex array attribute <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `offset` | index | `dynamic_slice_sizes` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `total_length` | index | `packed_offsets` | index  #### `stream.resource.size` (Stream::ResourceSizeOp)  _Returns the size of the resource storage in bytes_   Syntax:  <pre><code>operation ::= `stream.resource.size` (`on` `(` $affinity^ `)`)?\n              $operand\n              attr-dict `:` type($operand)\n</code></pre>  Returns a possibly runtime-dynamic byte size of the resource backing storage. This may differ from the logical storage size of a value based on the alignment requirements of the target as well as encoding of higher level values such as sparse tensor formats.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), Stream_AffinityOp, Util_SizeAwareOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | any stream-compatible type  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index  #### `stream.resource.store` (Stream::ResourceStoreOp)  _Stores a value into a staging resource_   Syntax:  <pre><code>operation ::= `stream.resource.store` $value `,`\n              $target `[` $target_offset `]` `:`\n              type($value)\n              `-&gt;`\n              type($target) `{` $target_size `}`\n              attr-dict-with-keyword\n</code></pre>  The operation will complete synchronously against the resource though it may introduce a yield point if the staging resource needs to be acquired.  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), Util_SizeAwareOp  Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `target` | staging resource | `target_size` | index | `target_offset` | index | `value` | index or integer or floating-point or complex-type or vector of any type values  #### `stream.resource.subview` (Stream::ResourceSubviewOp)  _Slices out a cloned subview of a value_   Syntax:  <pre><code>operation ::= `stream.resource.subview` $source `[` $source_offset `]` `:`\n              type($source) `` `{` $source_size `}` `-&gt;`\n              type($result) `` `{` $result_size `}`\n              attr-dict-with-keyword\n</code></pre>  Aliases a byte subrange of a resource.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), StreamableOpInterface, TiedOpInterface, Util_SizeAwareOp, Util_SubrangeOp, ViewLikeOpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | any stream-compatible type | `source_size` | index | `source_offset` | index | `result_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | any stream-compatible type  #### `stream.resource.try_map` (Stream::ResourceTryMapOp)  _Maps read-only memory into a resource_   Syntax:  <pre><code>operation ::= `stream.resource.try_map` (`on` `(` $affinity^ `)`)?\n              $source `[` $source_offset `]` `:`\n              type($source)\n              `-&gt;`\n              type($did_map) `,` type($result) `` `{` $result_size `}`\n              attr-dict-with-keyword\n</code></pre>  Synchronously maps a host heap buffer into a stream-accessible resource with the requested lifetime. If the given source cannot be mapped the `did_map` result will be 0 and users must find another route into memory (such as file I/O). The resulting resource is not coherent with the source and behavior is undefined if the underlying contents change.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, MemoryEffectOpInterface (MemoryEffectOpInterface), NoMemoryEffect (MemoryEffectOpInterface), Stream_AffinityOp, Util_SizeAwareOp  Effects: MemoryEffects::Effect{MemoryEffects::Allocate on ::mlir::SideEffects::DefaultResource}, MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | a reference counted byte buffer | `source_offset` | index | `result_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `did_map` | 1-bit signless integer | `result` | resource or external resource or transient resource or variable resource or constant resource   ### Resource transfer ops    #### `stream.async.alloca` (Stream::AsyncAllocaOp)  _Allocates a transient value with undefined contents_   Syntax:  <pre><code>operation ::= `stream.async.alloca` (`on` `(` $affinity^ `)`)?\n              attr-dict `:` type($result) `{` $storage_size `}`\n</code></pre>  Allocates a transient value (one that is short-lived and local to the current computation) with undefined contents. Consumers of the allocated result must assume nothing of the contents and use `discard` access.  Traits: AlwaysSpeculatableImplTrait, Stream_AsyncPhaseOp  Interfaces: AffinityOpInterface, ConditionallySpeculatable, MemoryEffectOpInterface (MemoryEffectOpInterface), StreamableOpInterface, Util_SizeAwareOp  Effects: MemoryEffects::Effect{MemoryEffects::Allocate on ::mlir::SideEffects::DefaultResource}  ##### Attributes:   AttributeMLIR TypeDescription <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `storage_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | resource or external resource or transient resource or variable resource or constant resource  #### `stream.async.clone` (Stream::AsyncCloneOp)  _Clones the contents of a value_   Syntax:  <pre><code>operation ::= `stream.async.clone` (`on` `(` $affinity^ `)`)?\n              $source `:`\n              type($source) `` `{` $source_size `}` `-&gt;`\n              type($result) `` `{` $result_size `}`\n              attr-dict-with-keyword\n</code></pre>  Clones the contents of a value at a snapshot in time. Future changes to the cloned value will not effect the result. Acts as a copy-on-write operation.  Traits: Stream_AsyncPhaseOp  Interfaces: AsyncAccessOpInterface, Stream_AffinityOp, StreamableOpInterface, Util_SizeAwareOp  ##### Attributes:   AttributeMLIR TypeDescription <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | resource or external resource or transient resource or variable resource or constant resource | `source_size` | index | `result_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | resource or external resource or transient resource or variable resource or constant resource  #### `stream.async.collective` (Stream::AsyncCollectiveOp)  _Performs a collective operation_   Syntax:  <pre><code>operation ::= `stream.async.collective` `` $op `` `[` $element_count `]`\n              (`on` `(` $affinity^ `)`)?\n              `channel` `(` $channel `)`\n              custom&lt;CollectiveParam&gt;(ref($op), $param) ``\n              $source `[` $source_offset `to` $source_end `for` $source_length `]` `,`\n              $target `[` $target_offset `to` $target_end `for` $target_length `]` `:`\n              type($source) `` `{` $source_size `}` `-&gt;`\n              custom&lt;ShapedTiedResult&gt;(type($target), $target_size)\n              attr-dict-with-keyword\n</code></pre>  TODO: document different usage. For now this should be considered a prototype and that modeling of collective operations may change in the future to better ensure in-place operations (where send/recv is a subset of recv/send). We may have dedicated operations for the send and recv verbs as they have sequencing implications - or we could add optional sequencing to this base op.  Traits: Stream_AsyncPhaseOp  Interfaces: AsyncAccessOpInterface, InferTypeOpInterface, Stream_AffinityOp, Stream_StreamableOp, TiedOpInterface, Util_SizeAwareOp  ##### Attributes:   AttributeMLIR TypeDescription <code>op</code>::mlir::iree_compiler::IREE::Stream::CollectiveAttrcollective operation and specification{{% markdown %}}     Specifies the collective operation to perform and any mode bits required.   {{% /markdown %}} <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `target` | resource or external resource or transient resource or variable resource or constant resource | `target_size` | index | `target_offset` | index | `target_end` | index | `target_length` | index | `source` | resource or external resource or transient resource or variable resource or constant resource | `source_size` | index | `source_offset` | index | `source_end` | index | `source_length` | index | `element_count` | index | `channel` | a collective communication channel | `param` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | resource or external resource or transient resource or variable resource or constant resource  #### `stream.async.constant` (Stream::AsyncConstantOp)  _Defines a constant resource_   Syntax:  <pre><code>operation ::= `stream.async.constant` (`on` `(` $affinity^ `)`)?\n              `:`\n              type($result) `` `{` $result_size `}`\n              `=`\n              $value\n              attr-dict-with-keyword\n</code></pre>  Returns a new resource with the given constant value.  Traits: AlwaysSpeculatableImplTrait, Stream_AsyncPhaseOp  Interfaces: AsyncAccessOpInterface, ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, Stream_AffinityOp, StreamableOpInterface, Util_SizeAwareOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>value</code>::mlir::Attributeany attribute <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `result_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | resource or external resource or transient resource or variable resource or constant resource  #### `stream.async.copy` (Stream::AsyncCopyOp)  _Copies a subview of a stream resource to another_   Syntax:  <pre><code>operation ::= `stream.async.copy` (`on` `(` $affinity^ `)`)?\n              $source `[` $source_offset `to` $source_end `]` `,`\n              $target `[` $target_offset `to` $target_end `]` `,`\n              $length `:`\n              type($source) `` `{` $source_size `}` `-&gt;`\n              custom&lt;ShapedTiedResult&gt;(type($target), $target_size)\n              attr-dict-with-keyword\n</code></pre>  Copies a subview of a resource into a subview of another. As with memcpy this does not support overlapping updates into the same resource. Unlike `stream.async.update` copy sources cannot be allocated in-place.  Equivalent to a stream.async.slice + stream.async.update.  Traits: Stream_AsyncPhaseOp  Interfaces: AsyncAccessOpInterface, InferTypeOpInterface, Stream_AffinityOp, Stream_StreamableOp, TiedOpInterface, Util_SizeAwareOp  ##### Attributes:   AttributeMLIR TypeDescription <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `target` | resource or external resource or transient resource or variable resource or constant resource | `target_size` | index | `target_offset` | index | `target_end` | index | `source` | resource or external resource or transient resource or variable resource or constant resource | `source_size` | index | `source_offset` | index | `source_end` | index | `length` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | resource or external resource or transient resource or variable resource or constant resource  #### `stream.async.dispatch` (Stream::AsyncDispatchOp)  _Dispatches a parallelized grid of work_   Syntax:  <pre><code>operation ::= `stream.async.dispatch` (`on` `(` $affinity^ `)`)?\n              $entry_point\n              (`[` $workload^ `]`)? ``\n              custom&lt;DispatchOperands&gt;($resource_operands,\n              $resource_operand_offsets,\n              $resource_operand_ends,\n              $resource_operand_lengths) attr-dict `:`\n              custom&lt;ShapedFunctionType&gt;(ref($resource_operands),\n              type($resource_operands), $resource_operand_sizes,\n              type($results), $result_sizes,\n              $tied_operands)\n</code></pre>  Calls the specified entry point function once for each element in the specified workgroup count. Each workgroup has access to the same operands and results and is able to load/store at will.  Traits: AttrSizedOperandSegments, Stream_AsyncPhaseOp  Interfaces: AsyncAccessOpInterface, Stream_AffinityOp, Stream_StreamableOp, SymbolUserOpInterface, TiedOpInterface, Util_SizeAwareOp  ##### Attributes:   AttributeMLIR TypeDescription <code>entry_point</code>::mlir::SymbolRefAttrsymbol reference attribute <code>tied_operands</code>::mlir::ArrayAttr64-bit integer array attribute <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `workload` | index | `resource_operands` | resource or external resource or transient resource or variable resource or constant resource or index or integer or floating-point or complex-type | `resource_operand_sizes` | index | `resource_operand_offsets` | index | `resource_operand_ends` | index | `resource_operand_lengths` | index | `result_sizes` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | resource or external resource or transient resource or variable resource or constant resource  #### `stream.async.fill` (Stream::AsyncFillOp)  _Fills a subview of a stream resource with a value_   Syntax:  <pre><code>operation ::= `stream.async.fill` (`on` `(` $affinity^ `)`)?\n              $value `,`\n              $target `[` $target_offset `to` $target_end `for` $target_length `]` `:`\n              type($value) `-&gt;`\n              custom&lt;ShapedTiedResult&gt;(type($target), $target_size)\n              attr-dict-with-keyword\n</code></pre>  Splats a value into a subview of the given stream resource and returns the resource with the update applied.  Equivalent to a stream.async.splat + stream.async.update.  Traits: Stream_AsyncPhaseOp  Interfaces: AsyncAccessOpInterface, InferTypeOpInterface, Stream_AffinityOp, Stream_StreamableOp, TiedOpInterface, Util_SizeAwareOp  ##### Attributes:   AttributeMLIR TypeDescription <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `target` | resource or external resource or transient resource or variable resource or constant resource | `target_size` | index | `target_offset` | index | `target_end` | index | `target_length` | index | `value` | 8-bit signless integer or 16-bit signless integer or 32-bit signless integer or 64-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | resource or external resource or transient resource or variable resource or constant resource  #### `stream.async.load` (Stream::AsyncLoadOp)  _Loads a value from a resource_   Syntax:  <pre><code>operation ::= `stream.async.load` $source `[` $source_offset `]` `:`\n              type($source) `` `{` $source_size `}`\n              `-&gt;`\n              type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns the element at the given location from within the resource.  Traits: AlwaysSpeculatableImplTrait, Stream_AsyncPhaseOp  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), Util_SizeAwareOp  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | staging resource | `source_size` | index | `source_offset` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index or integer or floating-point or complex-type or vector of any type values  #### `stream.async.slice` (Stream::AsyncSliceOp)  _Slices out a cloned subview of a value_   Syntax:  <pre><code>operation ::= `stream.async.slice` (`on` `(` $affinity^ `)`)?\n              $source `[` $source_offset `to` $source_end `]` `:`\n              type($source) `` `{` $source_size `}` `-&gt;`\n              type($result) `` `{` $result_size `}`\n              attr-dict-with-keyword\n</code></pre>  Slices a subrange of a stream resource based on a byte range. Acts as a copy-on-write operation.  Traits: AlwaysSpeculatableImplTrait, Stream_AsyncPhaseOp  Interfaces: AsyncAccessOpInterface, ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), Stream_AffinityOp, Stream_StreamableOp, Util_SizeAwareOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | resource or external resource or transient resource or variable resource or constant resource | `source_size` | index | `source_offset` | index | `source_end` | index | `result_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | resource or external resource or transient resource or variable resource or constant resource  #### `stream.async.splat` (Stream::AsyncSplatOp)  _Splats a value into a resource_   Syntax:  <pre><code>operation ::= `stream.async.splat` (`on` `(` $affinity^ `)`)?\n              $value `:` type($value) `-&gt;` type($result) `` `{` $result_size `}`\n              attr-dict-with-keyword\n</code></pre>  Returns a new resource with the given primitive value splatted out to fill the entire contents.  Traits: Stream_AsyncPhaseOp  Interfaces: AsyncAccessOpInterface, Stream_AffinityOp, StreamableOpInterface, Util_SizeAwareOp  ##### Attributes:   AttributeMLIR TypeDescription <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | 8-bit signless integer or 16-bit signless integer or 32-bit signless integer or 64-bit signless integer | `result_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | resource or external resource or transient resource or variable resource or constant resource  #### `stream.async.store` (Stream::AsyncStoreOp)  _Stores a value into a resource_   Syntax:  <pre><code>operation ::= `stream.async.store` $value `,`\n              $target `[` $target_offset `]` `:`\n              type($value)\n              `-&gt;`\n              custom&lt;ShapedTiedResult&gt;(type($target), $target_size)\n              attr-dict-with-keyword\n</code></pre>  Returns a resource with the element at the given offset set to the given value.  Traits: AlwaysSpeculatableImplTrait, Stream_AsyncPhaseOp  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), TiedOpInterface, Util_SizeAwareOp  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `target` | staging resource | `target_size` | index | `target_offset` | index | `value` | index or integer or floating-point or complex-type or vector of any type values  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | staging resource  #### `stream.async.transfer` (Stream::AsyncTransferOp)  _Transfers a resource from one location/state to another_   Syntax:  <pre><code>operation ::= `stream.async.transfer` (`from` `(` $source_affinity^ `)`)?\n              $source `:`\n              type($source) `` `{` $source_size `}` `-&gt;`\n              (`to` `(` $result_affinity^ `)`)?\n              type($result) `` `{` $result_size `}`\n              attr-dict-with-keyword\n</code></pre>  Transfers a resource between different states (such as a `staging` lifetime to a `local` lifetime) or different affinities. This is roughly equivalent to a cast but may have special semantics when later lowered to one or more devices with discrete memory spaces or pools.  Traits: Stream_AsyncPhaseOp  Interfaces: AsyncAccessOpInterface, Stream_AffinityOp, Stream_StreamableOp, Util_SizeAwareOp  ##### Attributes:   AttributeMLIR TypeDescription <code>source_affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}} <code>result_affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | resource or external resource or transient resource or variable resource or constant resource or staging resource | `source_size` | index | `result_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | resource or external resource or transient resource or variable resource or constant resource or staging resource  #### `stream.async.update` (Stream::AsyncUpdateOp)  _Updates a slice of a subview of a resource in-place_   Syntax:  <pre><code>operation ::= `stream.async.update` (`on` `(` $affinity^ `)`)?\n              $update `,`\n              $target `[` $target_offset `to` $target_end `]` `:`\n              type($update) `` `{` $update_size `}` `-&gt;`\n              custom&lt;ShapedTiedResult&gt;(type($target), $target_size)\n              attr-dict-with-keyword\n</code></pre>  Copies a value into a resource based on a byte range. The returned value is the entire updated target value. Updates can be turned into placement allocations and avoid copies.  Traits: Stream_AsyncPhaseOp  Interfaces: AsyncAccessOpInterface, InferTypeOpInterface, Stream_AffinityOp, Stream_StreamableOp, TiedOpInterface, Util_SizeAwareOp  ##### Attributes:   AttributeMLIR TypeDescription <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `target` | resource or external resource or transient resource or variable resource or constant resource | `target_size` | index | `target_offset` | index | `target_end` | index | `update` | resource or external resource or transient resource or variable resource or constant resource | `update_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | resource or external resource or transient resource or variable resource or constant resource   ### Synchronization ops    #### `stream.timepoint.await` (Stream::TimepointAwaitOp)  _Awaits a timepoint before returning a set of resources_   Syntax:  <pre><code>operation ::= `stream.timepoint.await` (`on` `(` $affinity^ `)`)?\n              $await_timepoint `=` `` `&gt;`\n              $resource_operands `:`\n              custom&lt;SizeAwareTypeList&gt;(type($resource_operands),\n              type($results), $resource_operand_sizes)\n              attr-dict-with-keyword\n</code></pre>  After asynchronous execution scheduling resources may exist in different states at different points in the execution timeline. This op enables resolving the version of a resource after a particular point in the timeline. As timepoints transitively chain the timepoint must only cover the resource availability but not be limited to its original production timepoint.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), Stream_AffinityOp, Stream_TimelineOp, TiedOpInterface, Util_SizeAwareOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `resource_operands` | resource or external resource or transient resource or variable resource or constant resource or staging resource | `resource_operand_sizes` | index | `await_timepoint` | a timepoint indicating execution availability  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | resource or external resource or transient resource or variable resource or constant resource or staging resource  #### `stream.timepoint.barrier` (Stream::TimepointBarrierOp)  _Returns a timepoint indicating when a resource is available_   Syntax:  <pre><code>operation ::= `stream.timepoint.barrier` (`on` `(` $affinity^ `)`)?\n              $resource `:` type($resource) `` `{` $resource_size `}`\n              `=` `` `&gt;`\n              type($result_timepoint)\n              attr-dict-with-keyword\n</code></pre>  After asynchronous execution scheduling resources may exist in different states at different points in the execution timeline. This op enables identifying when the version of a resource after a particular point in the timeline is available. As timepoints transitively chain the timepoint must only cover the resource availability but not be limited to its original production timepoint.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), Stream_AffinityOp, Stream_TimelineOp, TiedOpInterface, Util_SizeAwareOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `resource` | resource or external resource or transient resource or variable resource or constant resource or staging resource | `resource_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | resource or external resource or transient resource or variable resource or constant resource or staging resource | `result_timepoint` | a timepoint indicating execution availability  #### `stream.timepoint.chain_external` (Stream::TimepointChainExternalOp)  _Exports a timepoint to an external dialect type_   Syntax:  <pre><code>operation ::= `stream.timepoint.chain_external` (`on` `(` $affinity^ `)`)?\n              $await_timepoint\n              `=` `` `&gt;`\n              `(` $external_values `:` type($external_values) `)`\n              attr-dict-with-keyword\n</code></pre>  Defines a conversion to an external dialect type such as `hal.fence` that is resolved during lowering into the stream dialect. This can be used to interoperate between levels of the stack that require specifying stream types and those that prior to lowering do not handle them.  Interfaces: Stream_AffinityOp  ##### Attributes:   AttributeMLIR TypeDescription <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `await_timepoint` | a timepoint indicating execution availability | `external_values` | any type  #### `stream.timepoint.export` (Stream::TimepointExportOp)  _Exports a timepoint to an external dialect type_   Syntax:  <pre><code>operation ::= `stream.timepoint.export` (`on` `(` $affinity^ `)`)?\n              $await_timepoint\n              `=` `` `&gt;`\n              `(` type($results) `)`\n              attr-dict-with-keyword\n</code></pre>  Defines a conversion to an external dialect type such as `hal.fence` that is resolved during lowering into the stream dialect. This can be used to interoperate between levels of the stack that require specifying stream types and those that prior to lowering do not handle them.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), Stream_AffinityOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `await_timepoint` | a timepoint indicating execution availability  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | any type  #### `stream.timepoint.immediate` (Stream::TimepointImmediateOp)  _Results an immediately-available timepoint_   Syntax:  <pre><code>operation ::= `stream.timepoint.immediate` attr-dict\n              `=` `` `&gt;` type($result_timepoint)\n</code></pre>  Timepoints indicate a point in the execution timeline and this op can be used to get a placeholder representing the start of the timeline. Any waits on the returned timepoint will resolve immediately. This generally folds away but can be useful if needing to initialize globals or branch args.  Traits: AlwaysSpeculatableImplTrait, ConstantLike  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), Stream_TimelineOp  Effects: MemoryEffects::Effect{}  ##### Results:  | Result | Description | | :----: | ----------- | | `result_timepoint` | a timepoint indicating execution availability  #### `stream.timepoint.import` (Stream::TimepointImportOp)  _Imports a timepoint from an external dialect type_   Syntax:  <pre><code>operation ::= `stream.timepoint.import` (`on` `(` $affinity^ `)`)?\n              $operands `:` `(` type($operands) `)`\n              `=` `` `&gt;`\n              type($result_timepoint)\n              attr-dict-with-keyword\n</code></pre>  Defines a conversion from an external dialect type such as `hal.semaphore` that is resolved during lowering into the stream dialect. This can be used to interoperate between levels of the stack that require specifying stream types and those that prior to lowering do not handle them.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), Stream_AffinityOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operands` | any type  ##### Results:  | Result | Description | | :----: | ----------- | | `result_timepoint` | a timepoint indicating execution availability  #### `stream.timepoint.join` (Stream::TimepointJoinOp)  _Joins one or more timepoints into the max of all of them_   Syntax:  <pre><code>operation ::= `stream.timepoint.join` `max` `(` $await_timepoints `)` `=` `` `&gt;` type($result_timepoint)\n              attr-dict-with-keyword\n</code></pre>  Returns a timepoint that indicates that all of the input timepoints have been reached.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), Stream_TimelineOp  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `await_timepoints` | a timepoint indicating execution availability  ##### Results:  | Result | Description | | :----: | ----------- | | `result_timepoint` | a timepoint indicating execution availability   ### Tensor ops    #### `stream.tensor.clone` (Stream::TensorCloneOp)  _Clones the contents of a value_   Syntax:  <pre><code>operation ::= `stream.tensor.clone` (`on` `(` $affinity^ `)`)?\n              $source `:`\n              $source_encoding (`` `{` $source_encoding_dims^ `}`)?\n              `in`\n              type($source) `` `{` $source_size `}`\n              `-&gt;`\n              $result_encoding (`` `{` $result_encoding_dims^ `}`)?\n              `in`\n              type($result) `` `{` $result_size `}`\n              attr-dict-with-keyword\n</code></pre>  Clones the contents of a value at a snapshot in time. Future changes to the cloned value will not effect the result. Acts as a copy-on-write operation.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments, Stream_TensorPhaseOp  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), Stream_AffinityOp, Stream_StreamableOp, Util_ShapeAwareOp, Util_SizeAwareOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>source_encoding</code>::mlir::TypeAttrany type attribute <code>result_encoding</code>::mlir::TypeAttrany type attribute <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | resource or external resource or transient resource or variable resource or constant resource | `source_encoding_dims` | index | `source_size` | index | `result_encoding_dims` | index | `result_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | resource or external resource or transient resource or variable resource or constant resource  #### `stream.tensor.constant` (Stream::TensorConstantOp)  _Defines a constant tensor value_   Syntax:  <pre><code>operation ::= `stream.tensor.constant` (`on` `(` $affinity^ `)`)?\n              `:`\n              $result_encoding (`` `{` $result_encoding_dims^ `}`)?\n              `in`\n              type($result)\n              `=`\n              $value\n              attr-dict-with-keyword\n</code></pre>  Returns a typed resource initialized to the given constant value.  Traits: AlwaysSpeculatableImplTrait, Stream_TensorPhaseOp  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, Stream_AffinityOp, Stream_StreamableOp, Util_ShapeAwareOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>value</code>::mlir::Attributeany attribute <code>result_encoding</code>::mlir::TypeAttrany type attribute <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `result_encoding_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | resource or external resource or transient resource or variable resource or constant resource  #### `stream.tensor.empty` (Stream::TensorEmptyOp)  _Defines an empty tensor value_   Syntax:  <pre><code>operation ::= `stream.tensor.empty` (`on` `(` $affinity^ `)`)?\n              `:`\n              $result_encoding (`` `{` $result_encoding_dims^ `}`)?\n              `in`\n              type($result) `` `{` $result_size `}`\n              attr-dict-with-keyword\n</code></pre>  Returns a typed resource initialized with no contents. This still carries shape metadata and may encode to a non-empty resource such as in cases where the empty representation still has data (e.g. sparse tensors). Subsequent writes must populate any ranges of the tensor that are later read.  Traits: AlwaysSpeculatableImplTrait, Stream_TensorPhaseOp  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, Stream_AffinityOp, StreamableOpInterface, Util_ShapeAwareOp, Util_SizeAwareOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>result_encoding</code>::mlir::TypeAttrany type attribute <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `result_encoding_dims` | index | `result_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | resource or external resource or transient resource or variable resource or constant resource  #### `stream.tensor.fill` (Stream::TensorFillOp)  _Fills a subview of a stream resource with a value_   Syntax:  <pre><code>operation ::= `stream.tensor.fill` (`on` `(` $affinity^ `)`)?\n              $value `,` $target `[` $start_indices `for` $lengths `]` `:`\n              type($value)\n              `-&gt;`\n              $target_encoding (`` `{` $target_encoding_dims^ `}`)?\n              `in`\n              custom&lt;ShapedTiedResult&gt;(type($target), $target_size)\n              attr-dict-with-keyword\n</code></pre>  Splats a value into a subview of the given stream resource and returns the resource with the update applied.  Equivalent to a stream.tensor.splat + stream.tensor.update.  Traits: AttrSizedOperandSegments, Stream_TensorPhaseOp  Interfaces: InferTypeOpInterface, Stream_AffinityOp, Stream_StreamableOp, TiedOpInterface, Util_ShapeAwareOp, Util_SizeAwareOp  ##### Attributes:   AttributeMLIR TypeDescription <code>target_encoding</code>::mlir::TypeAttrany type attribute <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `target` | resource or external resource or transient resource or variable resource or constant resource | `target_encoding_dims` | index | `target_size` | index | `start_indices` | index | `lengths` | index | `value` | index or integer or floating-point or complex-type  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | resource or external resource or transient resource or variable resource or constant resource  #### `stream.tensor.load` (Stream::TensorLoadOp)  _Loads a value from a tensor element_   Syntax:  <pre><code>operation ::= `stream.tensor.load` $source (`[` $indices^ `]`)? `:`\n              $source_encoding (`` `{` $source_encoding_dims^ `}`)?\n              `in`\n              type($source) `` `{` $source_size `}`\n              `-&gt;`\n              type($result)\n              attr-dict-with-keyword\n</code></pre>  Returns the element at the given location from within the tensor.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments, Stream_TensorPhaseOp  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), Util_ShapeAwareOp, Util_SizeAwareOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>source_encoding</code>::mlir::TypeAttrany type attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | staging resource | `source_encoding_dims` | index | `source_size` | index | `indices` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index or integer or floating-point or complex-type or vector of any type values  #### `stream.tensor.sizeof` (Stream::TensorSizeOfOp)  _Calculates the storage size of a given high-level type_   Syntax:  <pre><code>operation ::= `stream.tensor.sizeof` (`on` `(` $affinity^ `)`)?\n              $encoding (`{` $encoding_dims^ `}`)?\n              attr-dict `:` type($storage_size)\n</code></pre>  Target-dependent storage size calculation using a high-level annotated type. While within the stream dialect the storage size of a value is left as a placeholder using this op. The requisite target-specific parameters for expanding the size calculation are only available after affinities have been assigned.  Traits: AlwaysSpeculatableImplTrait, Stream_TensorPhaseOp  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), Stream_AffinityOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>encoding</code>::mlir::TypeAttrany type attribute <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `encoding_dims` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `storage_size` | index  #### `stream.tensor.slice` (Stream::TensorSliceOp)  _Slices out a cloned subview of a value_   Syntax:  <pre><code>operation ::= `stream.tensor.slice` (`on` `(` $affinity^ `)`)?\n              $source `[` $start_indices `for` $lengths `]` `:`\n              $source_encoding (`` `{` $source_encoding_dims^ `}`)?\n              `in`\n              type($source) `` `{` $source_size `}`\n              `-&gt;`\n              $result_encoding (`` `{` $result_encoding_dims^ `}`)?\n              `in`\n              type($result) `` `{` $result_size `}`\n              attr-dict-with-keyword\n</code></pre>  Slices a subrange of a stream resource based on a tensor encoding. Acts as a copy-on-write operation.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments, Stream_TensorPhaseOp  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), Stream_AffinityOp, Stream_StreamableOp, Util_ShapeAwareOp, Util_SizeAwareOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>source_encoding</code>::mlir::TypeAttrany type attribute <code>result_encoding</code>::mlir::TypeAttrany type attribute <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | resource or external resource or transient resource or variable resource or constant resource | `source_encoding_dims` | index | `source_size` | index | `start_indices` | index | `lengths` | index | `result_encoding_dims` | index | `result_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | resource or external resource or transient resource or variable resource or constant resource  #### `stream.tensor.splat` (Stream::TensorSplatOp)  _Splats a value into a shaped tensor_   Syntax:  <pre><code>operation ::= `stream.tensor.splat` (`on` `(` $affinity^ `)`)?\n              $value\n              `:` type($value)\n              `-&gt;`\n              $result_encoding (`` `{` $result_encoding_dims^ `}`)?\n              `in`\n              type($result) `` `{` $result_size `}`\n              attr-dict-with-keyword\n</code></pre>  Returns a typed resource initialized to the given primitive value.  Traits: AlwaysSpeculatableImplTrait, Stream_TensorPhaseOp  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), Stream_AffinityOp, StreamableOpInterface, Util_ShapeAwareOp, Util_SizeAwareOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>result_encoding</code>::mlir::TypeAttrany type attribute <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | index or integer or floating-point or complex-type | `result_encoding_dims` | index | `result_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | resource or external resource or transient resource or variable resource or constant resource  #### `stream.tensor.store` (Stream::TensorStoreOp)  _Stores a value into a tensor element_   Syntax:  <pre><code>operation ::= `stream.tensor.store` $value `,`\n              $target (`[` $indices^ `]`)? `:`\n              type($value)\n              `-&gt;`\n              $target_encoding (`` `{` $target_encoding_dims^ `}`)?\n              `in`\n              custom&lt;ShapedTiedResult&gt;(type($target), $target_size)\n              attr-dict-with-keyword\n</code></pre>  Returns a tensor with the element at the given index set to the given value.  Traits: AlwaysSpeculatableImplTrait, AttrSizedOperandSegments, Stream_TensorPhaseOp  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), TiedOpInterface, Util_ShapeAwareOp, Util_SizeAwareOp  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>target_encoding</code>::mlir::TypeAttrany type attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `target` | staging resource | `target_encoding_dims` | index | `target_size` | index | `indices` | index | `value` | index or integer or floating-point or complex-type or vector of any type values  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | staging resource  #### `stream.tensor.trace` (Stream::TensorTraceOp)  _Trace value(s) operation_   Syntax:  <pre><code>operation ::= `stream.tensor.trace` attr-dict ($operands^ `:` type($operands))?\n</code></pre>  Traces out to a runtime trace sink (console, log file, etc) the given tensors and titles them with the given key. The key is informational only and useful for titling/marking specific sets of tensors for easier searching.  ##### Attributes:   AttributeMLIR TypeDescription <code>key</code>::mlir::StringAttrstring attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operands` | tensor of any type values  #### `stream.tensor.update` (Stream::TensorUpdateOp)  _Updates a slice of a subview of a resource in-place_   Syntax:  <pre><code>operation ::= `stream.tensor.update` (`on` `(` $affinity^ `)`)?\n              $update `,` $target `[` $start_indices `]` `:`\n              $update_encoding (`` `{` $update_encoding_dims^ `}`)?\n              `in`\n              type($update) `` `{` $update_size `}`\n              `-&gt;`\n              $target_encoding (`` `{` $target_encoding_dims^ `}`)?\n              `in`\n              custom&lt;ShapedTiedResult&gt;(type($target), $target_size)\n              attr-dict-with-keyword\n</code></pre>  Copies a value into a resource based on tensor encodings. The returned value is the entire updated target value.  Traits: AttrSizedOperandSegments, Stream_TensorPhaseOp  Interfaces: InferTypeOpInterface, Stream_AffinityOp, Stream_StreamableOp, TiedOpInterface, Util_ShapeAwareOp, Util_SizeAwareOp  ##### Attributes:   AttributeMLIR TypeDescription <code>target_encoding</code>::mlir::TypeAttrany type attribute <code>update_encoding</code>::mlir::TypeAttrany type attribute <code>affinity</code>::mlir::iree_compiler::IREE::Stream::AffinityAttrdefines execution context affinity{{% markdown %}}     WIP; see [#10765](https://github.com/openxla/iree/issues/10765).      TBD. The intent is that this can specify host, device, and queue affinity.     Scopes can be annotated with an affinity to ensure execution within happens     in a particular location. Arrays of affinities or wildcard specifiers will     allow for refinement (\"do it on this device but auto select a queue\"). It     will also allow us to indicate host affinity such that device&lt;-&gt;device and     host&lt;-&gt;device can be identified in the IR structure. Today all affinities     are no-op'ed and assumed to be 'current device'.   {{% /markdown %}}   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `target` | resource or external resource or transient resource or variable resource or constant resource | `target_encoding_dims` | index | `target_size` | index | `start_indices` | index | `update` | resource or external resource or transient resource or variable resource or constant resource | `update_encoding_dims` | index | `update_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | resource or external resource or transient resource or variable resource or constant resource   ## Attribute definition  ### CollectiveAttr  collective operation and specification  Syntax:  <pre><code>#stream.collective&lt;\n  CollectiveKind,   # kind\n  std::optional&lt;CollectiveReductionOp&gt;,   # reduction\n  CollectiveElementType   # element_type\n&gt;\n</code></pre>  Specifies the collective operation to perform and any mode bits required.  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | kind | `CollectiveKind` |  | | reduction | `std::optional` |  | | element_type | `CollectiveElementType` |  |  ### PartitioningConfigAttr  defines partitioning configuration  Configures the partitioning algorithm to use and its configuration. Partitioning is useful to adjust when scheduling behavior of targets is radically different - such as single-threaded vs. multi-threaded CPUs or bespoke ML accelerators vs. general purpose GPUs. This mechanism controls the amount of concurrency, parallelism, memory consumption, and latency.  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | favor | `IREE::Stream::FavorAttr` |  |  ### ResourceConfigAttr  defines resource constraints configuration  Defines resource storage constraints. These allow for packing and layout algorithms to ensure they are producing usable results on target devices.  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | maxAllocationSize | `int64_t` |  | | minBufferOffsetAlignment | `int64_t` |  | | maxBufferRange | `int64_t` |  | | minBufferRangeAlignment | `int64_t` |  | | indexBits | `int64_t` |  | | aliasMutableBindings | `bool` |  |  ### TimepointAttr  an immediately-resolved timepoint   ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | type | `::mlir::Type` |  |  ## Type constraint definition  ### constant resource  Stream constants are immutable values that are available for the lifetime of the program once initialized.   ### external resource  Stream external values represent asynchronously-available and sequenced values that are owned and managed by external code - such as those passed in or out of the program entry points. Though external values are managed during an invocation the same as other stream values the visibility into them does not extend outside of the invocation they are provided to.  Stream values are not usable directly outside of a stream execution or transfer operation. If the contents of the value are needed they must first be transferred via `stream.transfer` - which may incur a copy.   ### staging resource  Stream upload/download staging resource. These are used outside of streams and then transferred to other stream resources such as variables or transients for use inside of streams. Dispatches and several other operations cannot directly operate on these resources.   ### transient resource  Stream transients represent asynchronously-available and sequenced values that have a short lifetime - often only passed between stream executions. It is expected that transient values are not stored in global state and have minimal lifetime as they may be heavily pooled or suballocated.  Stream values are not usable directly outside of a stream execution or transfer operation. If the contents of the value are needed they must first be transferred via `stream.transfer` - which may incur a copy.   ### resource  A stream resource that has not yet had its lifetime calculated.   ### variable resource  Stream variables represent asynchronously-available and sequenced values that have a long lifetime relative to the work being performed on them. These variables are often stored in global state and may live for the entire duration of the program.  Stream values are not usable directly outside of a stream execution or transfer operation. If the contents of the value are needed they must first be transferred via `stream.transfer` - which may incur a copy.   ## Type definition  ### BindingType  a managed resource binding into an executable scope  Syntax: `!stream.binding`  A resource binding available within an executable dispatch function. The bindings map 1:1 with the resources bound during dispatch operations.  ### ChannelType  a collective communication channel  Syntax: `!stream.channel`  Represents a single participant in a collective clique. Multiple channels may exist within the same program to allow for partial operations or hierarchical operations.  In programs that model SPMD behavior internally channels can be created or provided by hosting applications. For example, the program could expose a `@set_channels(!util.list)` method that stores the channels in globals for use throughout the program allowing for application-controlled channel configuration.  ### FileType  a file handle used for I/O operations  Syntax: `!stream.file`  A file handle that can be asynchronously read and written into/from stream resources.  ### ResourceType  a managed resource  Stream external values represent asynchronously-available and sequenced values that are owned and managed by external code - such as those passed in or out of the program entry points. Though external values are managed during an invocation the same as other stream values the visibility into them does not extend outside of the invocation they are provided to.  Stream values are not usable directly outside of a stream execution or transfer operation. If the contents of the value are needed they must first be transferred via `stream.transfer` - which may incur a copy.  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | lifetime | `IREE::Stream::Lifetime` |  |  ### TimepointType  a timepoint indicating execution availability  Syntax: `!stream.timepoint`  Represents a point in the execution timeline that when resolved indicates that all of the execution prior to this timepoint has completed and the results of the execution are available for use. This includes transitive dependencies as well; if timepoint B is dependent on timepoint A then when B is available so too must be A."},{"location":"reference/mlir-dialects/Util/","title":"Util","text":""},{"location":"reference/mlir-dialects/Util/#util-dialect","title":"'util' Dialect","text":"<p>A dialect used for types common across IREE subdialects.</p> <ul> <li>'util' Dialect<ul> <li>Operation definition<ul> <li>Address/offset arithmetic ops<ul> <li>util.align (Util::AlignOp)</li> <li>util.sizeof (Util::SizeOfOp)</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/mlir-dialects/Util/#operation-definition","title":"Operation definition","text":""},{"location":"reference/mlir-dialects/Util/#addressoffset-arithmetic-ops","title":"Address/offset arithmetic ops","text":""},{"location":"reference/mlir-dialects/Util/#utilalign-utilalignop","title":"<code>util.align</code> (Util::AlignOp)","text":"<p>Aligns up to a power-of-two alignment if required</p> <p>Syntax:</p> <pre><code>operation ::= `util.align` $value `,` $alignment attr-dict `:` type($result)\n</code></pre> <p>Aligns |value| up to the given power-of-two |alignment| if required.</p> <p>Traits: AlwaysSpeculatableImplTrait, SameOperandsAndResultType</p> <p>Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface)</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/Util/#operands","title":"Operands:","text":"Operand Description <code>value</code> signless-integer-like <code>alignment</code> signless-integer-like"},{"location":"reference/mlir-dialects/Util/#results","title":"Results:","text":"Result Description <code>result</code> signless-integer-like"},{"location":"reference/mlir-dialects/Util/#utilsizeof-utilsizeofop","title":"<code>util.sizeof</code> (Util::SizeOfOp)","text":"<p>Returns the size in bytes of a datatype</p> <p>Syntax:</p> <pre><code>operation ::= `util.sizeof` $sizedType attr-dict-with-keyword\n</code></pre> <p>Most datatypes have a static size at all layers of the compilation stack. However, those that only have a size for certain lowering flows can be challenging. This op represents such sizes in a way that can be specialized later.</p> <p>Returns the size in bytes, rounded up to the next whole byte of the specified type. This op will fold to a constant index value for IntegerType and FloatType. All others are not folded.</p> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface)</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/Util/#attributes","title":"Attributes:","text":"AttributeMLIR TypeDescription <code>sizedType</code>::mlir::TypeAttrany type attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `size` | index   ### Buffer ops    #### `util.buffer.alloc` (Util::BufferAllocOp)  _Allocates a buffer with undefined contents_   Syntax:  <pre><code>operation ::= `util.buffer.alloc` `uninitialized`\n              attr-dict\n              `:`\n              type($result) `` `{` $storage_size `}`\n</code></pre>  Allocates a buffer with undefined contents. Consumers of the allocated result must assume nothing of the contents.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, MemoryEffectOpInterface (MemoryEffectOpInterface), NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, Util_SizeAwareOp  Effects: MemoryEffects::Effect{MemoryEffects::Allocate on ::mlir::SideEffects::DefaultResource}, MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>alignment</code>::mlir::IntegerAttrindex attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `storage_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | a reference counted byte buffer  #### `util.buffer.compare` (Util::BufferCompareOp)  _Compares a range of two buffers_   Syntax:  <pre><code>operation ::= `util.buffer.compare` $lhs `[` $lhs_offset `]` `,`\n              $rhs `[` $rhs_offset `]` `,`\n              $length `:`\n              type($lhs) `` `{` $lhs_size `}` `,`\n              type($rhs) `` `{` $rhs_size `}`\n              attr-dict-with-keyword\n</code></pre>  Returns true if the two ranges are bitwise equivalent, somewhat like memcmp.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, MemoryEffectOpInterface (MemoryEffectOpInterface), NoMemoryEffect (MemoryEffectOpInterface), SubrangeOperandOpInterface, Util_SizeAwareOp  Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}, MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | a reference counted byte buffer | `lhs_size` | index | `lhs_offset` | index | `rhs` | a reference counted byte buffer | `rhs_size` | index | `rhs_offset` | index | `length` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 1-bit signless integer  #### `util.buffer.constant` (Util::BufferConstantOp)  _Constant host-side byte buffer_   Syntax:  <pre><code>operation ::= `util.buffer.constant` ($name^)? attr-dict `:` type($result) `=` $value\n</code></pre>  Defines a compile-time byte buffer based on the given attribute value. The attribute will be serialized into the canonical IREE format for the chosen host target.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>name</code>::mlir::StringAttrstring attribute <code>value</code>::mlir::Attributebuffer-like constant attribute values <code>alignment</code>::mlir::IntegerAttrindex attribute <code>mime_type</code>::mlir::StringAttrstring attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | a reference counted byte buffer  #### `util.buffer.copy` (Util::BufferCopyOp)  _Copies a range of bytes between buffers_   Syntax:  <pre><code>operation ::= `util.buffer.copy` $source `[` $source_offset `]` `,`\n              $target `[` $target_offset `]` `,`\n              $length `:`\n              type($source) `` `{` $source_size `}` `-&gt;`\n              type($target) `` `{` $target_size `}`\n              attr-dict-with-keyword\n</code></pre>  Copies a range of bytes as with memcpy (no overlapping).  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), SubrangeOperandOpInterface, Util_SizeAwareOp  Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource, MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | a reference counted byte buffer | `source_size` | index | `source_offset` | index | `target` | a reference counted byte buffer | `target_size` | index | `target_offset` | index | `length` | index  #### `util.buffer.dealloc` (Util::BufferDeallocOp)  _Deallocates a buffer_   Syntax:  <pre><code>operation ::= `util.buffer.dealloc` $operand `:` type($operand) `{` $operand_size `}`\n              attr-dict-with-keyword\n</code></pre>  Hints that the buffer contents can be discarded. Buffers are reference counted and other owners may keep it live beyond the dealloc.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, MemoryEffectOpInterface (MemoryEffectOpInterface), NoMemoryEffect (MemoryEffectOpInterface), Util_SizeAwareOp  Effects: MemoryEffects::Effect{MemoryEffects::Free on ::mlir::SideEffects::DefaultResource}, MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | a reference counted byte buffer | `operand_size` | index  #### `util.buffer.fill` (Util::BufferFillOp)  _Fills a range of bytes with a value_   Syntax:  <pre><code>operation ::= `util.buffer.fill` $pattern `,`\n              $target `[` $target_offset `for` $length `]` `:`\n              type($pattern) `-&gt;`\n              type($target) `` `{` $target_size `}`\n              attr-dict-with-keyword\n</code></pre>  Fills the contents of the buffer in the given byte range with a pattern. The offset and length must match the natural alignment of the pattern type.  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), SubrangeOperandOpInterface, Util_SizeAwareOp  Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `pattern` | integer or floating-point or index | `target` | a reference counted byte buffer | `target_size` | index | `target_offset` | index | `length` | index  #### `util.buffer.load` (Util::BufferLoadOp)  _Loads a value from a buffer_   Syntax:  <pre><code>operation ::= `util.buffer.load` $source `[` $source_offset `for` $length `]`\n              `:` type($source) `` `{` $source_size `}` `-&gt;` type($result)\n              attr-dict-with-keyword\n</code></pre>  Loads a value at a byte offset. Must be aligned to the natural size of the result type.  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), SubrangeOperandOpInterface, Util_SizeAwareOp  Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | a reference counted byte buffer | `source_size` | index | `source_offset` | index | `length` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index or integer or floating-point  #### `util.buffer.size` (Util::BufferSizeOp)  _Returns the total buffer storage size in bytes_   Syntax:  <pre><code>operation ::= `util.buffer.size` $operand\n              `:` type($operand)\n              attr-dict-with-keyword\n</code></pre>  Returns the total length of the buffer in bytes from its base offset.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, Util_SizeAwareOp  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | a reference counted byte buffer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index  #### `util.buffer.slice` (Util::BufferSliceOp)  _Clones a subregion of a buffer_   Syntax:  <pre><code>operation ::= `util.buffer.slice` $source `[` $source_offset `]` attr-dict `:`\n              type($source) `` `{` $source_size `}` `-&gt;`\n              type($result) `` `{` $result_size `}`\n</code></pre>  Returns a copy of the contents from the source buffer.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, MemoryEffectOpInterface (MemoryEffectOpInterface), NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, SubrangeOperandOpInterface, Util_SizeAwareOp  Effects: MemoryEffects::Effect{MemoryEffects::Allocate on ::mlir::SideEffects::DefaultResource, MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}, MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>alignment</code>::mlir::IntegerAttrindex attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | a reference counted byte buffer | `source_size` | index | `source_offset` | index | `result_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | a reference counted byte buffer  #### `util.buffer.storage` (Util::BufferStorageOp)  _Returns the underlying buffer storage range_   Syntax:  <pre><code>operation ::= `util.buffer.storage` $operand\n              `:` type($operand) `` `{` $operand_size `}` `-&gt;` `(` type($result) `,` type($offset) `)`\n              attr-dict-with-keyword\n</code></pre>  Returns the buffer storage as a memref that must be offset and restricted to the returned range. The memref may be of any type and the user is responsible for ensuring that the reinterpret_cast-like behavior makes sense for the data they are accessing.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, Util_SizeAwareOp  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | a reference counted byte buffer | `operand_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | memref of any type values | `offset` | index  #### `util.buffer.store` (Util::BufferStoreOp)  _Stores a value into a buffer_   Syntax:  <pre><code>operation ::= `util.buffer.store` $source `,`\n              $target `[` $target_offset `for` $length `]`\n              `:` type($source) `-&gt;` type($target) `` `{` $target_size `}`\n              attr-dict-with-keyword\n</code></pre>  Stores a value at a byte offset. Must be aligned to the natural size of the source type.  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), SubrangeOperandOpInterface, Util_SizeAwareOp  Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | index or integer or floating-point | `target` | a reference counted byte buffer | `target_size` | index | `target_offset` | index | `length` | index  #### `util.buffer.subspan` (Util::BufferSubspanOp)  _Returns a reference to a subrange of a buffer_   Syntax:  <pre><code>operation ::= `util.buffer.subspan` $source `[` $source_offset `]` `:`\n              type($source) `` `{` $source_size `}` `-&gt;`\n              type($result) `` `{` $result_size `}`\n              attr-dict-with-keyword\n</code></pre>  Returns a logical view into an underlying source buffer. This induces aliasing and multiple SSA values may allow access to the same underlying buffer storage.  Subspans are a compiler-only concept and are propagated by an analysis pass to result in absolute offsets on accesses any place the subrange would have been used.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, SubrangeOperandOpInterface, TiedOpInterface, Util_SizeAwareOp, Util_SubrangeOp, ViewLikeOpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | a reference counted byte buffer | `source_size` | index | `source_offset` | index | `result_size` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | a reference counted byte buffer   ### Compiler hint ops    #### `util.optimization_barrier` (Util::OptimizationBarrierOp)  _Prevents compiler optimizations across a value._   Syntax:  <pre><code>operation ::= `util.optimization_barrier` attr-dict\n              ($operands^ `:` type($operands))?\n</code></pre>  Wraps any operands in an unoptimizable identity to prevent its results from being folded. It will be dropped during the final step in compilation and has no effect at runtime.  Traits: SameOperandsAndResultType  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operands` | any type  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | any type  #### `util.unfoldable_constant` (Util::UnfoldableConstantOp)  _A constant that cannot be folded by the compiler._  Similar to a std.constant, but is declared as having a side effect and has no folder. This is really just syntactic sugar as it is canonicalized to a std.constant wrapped in an util.optimization_barrier.  ##### Attributes:   AttributeMLIR TypeDescription <code>value</code>::mlir::Attributeany attribute   ##### Results:  | Result | Description | | :----: | ----------- | \u00abunnamed\u00bb | any type  #### `util.unreachable` (Util::UnreachableOp)  _Unreachable assertion op_   Syntax:  <pre><code>operation ::= `util.unreachable` $message attr-dict\n</code></pre>  Signals to the compiler that the parent block should not be reachable. This may be converted into a runtime assertion, though ideally they are stripped during translation.  <pre><code>^bb0:\n  %true = arith.constant true\n  cond_br %true, ^bb2, ^bb1\n^bb1:\n  // Indicates that this branch should never be taken.\n  util.unreachable \"shouldn't be here\"\n^bb2:\n  ...\n</code></pre>  Traits: ReturnLike, Terminator  Interfaces: NoMemoryEffect (MemoryEffectOpInterface), RegionBranchTerminatorOpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>message</code>::mlir::StringAttrstring attribute    ### Data type conversion ops    #### `util.numeric.optional_narrow` (Util::NumericOptionalNarrowOp)  _Memorializes an optional numeric narrowing that is valid_   Syntax:  <pre><code>operation ::= `util.numeric.optional_narrow` $operand `:` type($operand) `as` $semantic_type attr-dict\n</code></pre>  Serves as a placeholder for points in the computation where an optional numeric narrowing can be performed without loss of information. Such ops can guide optimization passes wishing to perform precision reduction.  In addition to the operand and result type, this op takes an additional `semantic_type` attribute representing the semantic target type which can be:   * FloatType   * Signed IntegerType   * Unsigned IntegerType  Note that this `semantic_type` must be a sign-carrying integer if using an integer type and cannot be IndexType (i.e. it can be used to indicate a possible narrowing of an IndexType to a specific integer).  If the operand is a TensorType, then the result must be a TensorType. The `semantic_type` constrains the element type.  Optionally, the minimum and maximum integer values (for integer semantic types) are tracked if known.  Traits: AlwaysSpeculatableImplTrait, SameOperandsAndResultType  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>semantic_type</code>::mlir::TypeAttrany type attribute <code>min_value</code>::mlir::IntegerAttrarbitrary integer attribute <code>max_value</code>::mlir::IntegerAttrarbitrary integer attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | signless integer or floating-point or tensor of signless integer or floating-point values  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | signless integer or floating-point or tensor of signless integer or floating-point values   ### Global ops    #### `util.global.address` (Util::GlobalAddressOp)  _Returns an address reference to a global_   Syntax:  <pre><code>operation ::= `util.global.address` $global attr-dict `:` qualified(type($result))\n</code></pre>  Returns the address of a global as a typed reference. Can be used with the global load and store indirect ops.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, SymbolUserOpInterface, Util_GlobalAddressOpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>global</code>::mlir::FlatSymbolRefAttrflat symbol reference attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | a pointer-like reference  #### `util.global.load.indirect` (Util::GlobalLoadIndirectOp)  _Loads a value from a global variable_   Syntax:  <pre><code>operation ::= `util.global.load.indirect` $global attr-dict `:` qualified(type($global)) `-&gt;` type($result)\n</code></pre>  Returns a copy of the global variable value.  Interfaces: Util_GlobalLoadIndirectOpInterface  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `global` | a pointer-like reference  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | any type  #### `util.global.load` (Util::GlobalLoadOp)  _Loads a value from a global variable_   Syntax:  <pre><code>operation ::= `util.global.load` $global attr-dict `:` type($result)\n</code></pre>  Returns a global variable value.  Interfaces: MemoryEffectOpInterface, OpAsmOpInterface, SymbolUserOpInterface, Util_GlobalLoadOpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>global</code>::mlir::FlatSymbolRefAttrflat symbol reference attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | any type  #### `util.global` (Util::GlobalOp)  _Stateful global variable declaration_   Syntax:  <pre><code>operation ::= `util.global` custom&lt;SymbolVisibility&gt;($sym_visibility)\n              (`mutable` $is_mutable^)?\n              $sym_name\n              attr-dict\n              custom&lt;TypeOrAttr&gt;($type, $initial_value)\n</code></pre>  Declares a global variable that maintains its value across invocations. The value is tied to the execution context of the module and different contexts will have different variable storage.  Interfaces: Symbol, Util_GlobalOpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>sym_name</code>::mlir::StringAttrstring attribute <code>type</code>::mlir::TypeAttrany type attribute <code>is_mutable</code>::mlir::UnitAttrunit attribute <code>initial_value</code>::mlir::TypedAttrTypedAttr instance{{% markdown %}}     This interface is used for attributes that have a type. The type of an     attribute is understood to represent the type of the data contained in the     attribute and is often used as the type of a value with this data.   {{% /markdown %}}   #### `util.global.store.indirect` (Util::GlobalStoreIndirectOp)  _Stores a value into a global variable_   Syntax:  <pre><code>operation ::= `util.global.store.indirect` $value `,` $global attr-dict `:` type($value) `-&gt;` qualified(type($global))\n</code></pre>  Stores a copy of the value into a global variable.  Interfaces: Util_GlobalStoreIndirectOpInterface  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | any type | `global` | a pointer-like reference  #### `util.global.store` (Util::GlobalStoreOp)  _Stores a value into a global variable_   Syntax:  <pre><code>operation ::= `util.global.store` $value `,` $global attr-dict `:` type($value)\n</code></pre>  Stores a copy of the value into a global variable.  Interfaces: SymbolUserOpInterface, Util_GlobalStoreOpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>global</code>::mlir::FlatSymbolRefAttrflat symbol reference attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | any type   ### List ops  Ops for `!util.list` (mostly just a placeholder for now).   #### `util.list.create` (Util::ListCreateOp)  _Creates a new empty list_   Syntax:  <pre><code>operation ::= `util.list.create` ($initial_capacity^)? attr-dict `:` qualified(type($result))\n</code></pre>  Creates a new empty list with an optional initial capacity.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, MemoryEffectOpInterface (MemoryEffectOpInterface), NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{MemoryEffects::Allocate on ::mlir::SideEffects::DefaultResource}, MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `initial_capacity` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | dense list container type  #### `util.list.get` (Util::ListGetOp)  _Element accessor_   Syntax:  <pre><code>operation ::= `util.list.get` $list `[` $index `]` attr-dict `:` custom&lt;ListTypeGet&gt;(type($list), type($result))\n</code></pre>  Returns the value of the element at the given index. Note that the value may be null if the element is null or the type does not match.  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `list` | dense list container type | `index` | index  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | any type  #### `util.list.resize` (Util::ListResizeOp)  _Resizes the list to a new count in elements_   Syntax:  <pre><code>operation ::= `util.list.resize` operands attr-dict `:` qualified(type($list))\n</code></pre>  Resizes the list to contain `new_size` elements. This will either truncate the list if the existing size is greater than `new_size` or extend the list with the default list value of the element type.  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `list` | dense list container type | `new_size` | index  #### `util.list.set` (Util::ListSetOp)  _Element mutator_   Syntax:  <pre><code>operation ::= `util.list.set` $list `[` $index `]` `,` $value attr-dict `:` custom&lt;ListTypeSet&gt;(type($list), type($value))\n</code></pre>  Sets the element at the given index to the new value.  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `list` | dense list container type | `index` | index | `value` | any type  #### `util.list.size` (Util::ListSizeOp)  _The size of the list in elements_   Syntax:  <pre><code>operation ::= `util.list.size` operands attr-dict `:` qualified(type($list))\n</code></pre>  Returns the current size of the list in elements.  Interfaces: InferTypeOpInterface, MemoryEffectOpInterface (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `list` | dense list container type  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index   ### Range arithmetic ops    #### `util.range.extents` (Util::RangeExtentsOp)  _Returns the min/max of a union of a set of ranges_   Syntax:  <pre><code>operation ::= `util.range.extents` custom&lt;RangeList&gt;($offsets, $lengths) attr-dict `:` type($min)\n</code></pre>  Computes min(offsets) and max(offsets + lengths). Though it's possible to express this with standard arithmetic this op enables more semantically meaningful folding/optimizations.  Traits: AlwaysSpeculatableImplTrait, SameOperandsAndResultType, SameVariadicOperandSize  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `offsets` | index or integer | `lengths` | index or integer  ##### Results:  | Result | Description | | :----: | ----------- | | `min` | index or integer | `max` | index or integer  #### `util.range.max` (Util::RangeMaxOp)  _Returns the max of all values_   Syntax:  <pre><code>operation ::= `util.range.max` $operands attr-dict `:` type($result)\n</code></pre>  Computes the max of a variadic list of operands. Though it's possible to express this with standard arithmetic this op enables more semantically meaningful folding/optimizations.  Traits: AlwaysSpeculatableImplTrait, SameOperandsAndResultType, SameVariadicOperandSize  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operands` | index or integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index or integer  #### `util.range.min` (Util::RangeMinOp)  _Returns the min of all values_   Syntax:  <pre><code>operation ::= `util.range.min` $operands attr-dict `:` type($result)\n</code></pre>  Computes the min of a variadic list of operands. Though it's possible to express this with standard arithmetic this op enables more semantically meaningful folding/optimizations.  Traits: AlwaysSpeculatableImplTrait, SameOperandsAndResultType, SameVariadicOperandSize  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operands` | index or integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index or integer   ### Status ops    #### `util.status.check_ok` (Util::StatusCheckOkOp)  _Raises a global failure if a status is not 'ok'_   Syntax:  <pre><code>operation ::= `util.status.check_ok` $status (`,` $message^)? attr-dict\n</code></pre>  When the status is not 'ok' this signals a runtime failure that causes the entire active invocation - and possibly *all* in-flight and pending invocations - to fail with the given status. The status will be propagated back via the available runtime error handling mechanisms such as semaphores or synchronous invocation results.  As the IREE execution model is deeply pipelined it's possible that failures have a latency between when they are emitted and when the application can observe the failure. It's also possible that other work that is in-flight or pending when the failure occurs will complete.  ##### Attributes:   AttributeMLIR TypeDescription <code>message</code>::mlir::StringAttrstring attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `status` | 32-bit signless integer   ### Structural ops    #### `util.initializer` (Util::InitializerOp)  _Global initialization function_  A function that is called in definition order upon module initialization. Must not load any globals that are defined or initialized after it in the module.  Traits: IsolatedFromAbove  Interfaces: CallableOpInterface, FunctionOpInterface, Symbol, Util_InitializerOpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>function_type</code>::mlir::TypeAttrtype attribute of function type <code>arg_attrs</code>::mlir::ArrayAttrArray of dictionary attributes <code>res_attrs</code>::mlir::ArrayAttrArray of dictionary attributes   #### `util.initializer.return` (Util::InitializerReturnOp)  _Return from a util.initializer_   Syntax:  <pre><code>operation ::= `util.initializer.return` attr-dict\n</code></pre>  Returns control from an initializer function.  Traits: AlwaysSpeculatableImplTrait, HasParent, ReturnLike, Terminator  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), RegionBranchTerminatorOpInterface  Effects: MemoryEffects::Effect{}   ### Type manipulation ops    #### `util.cast` (Util::CastOp)  _Casts one util type to another ala static_cast/dynamic_cast_   Syntax:  <pre><code>operation ::= `util.cast` $operand attr-dict `:` type($operand) `to` type($result)\n</code></pre>  Performs a type cast between object types known to the util dialect.  Traits: AlwaysSpeculatableImplTrait  Interfaces: CastOpInterface, ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), TiedOpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | any type  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | any type  #### `util.cmp.eq` (Util::CmpEQOp)  _Compares two values for equality_   Syntax:  <pre><code>operation ::= `util.cmp.eq` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands for equality. This is intended for comparing IREE reference types (like !util.buffer) that cannot be used with std.cmpi.  Traits: AlwaysSpeculatableImplTrait, Commutative  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | any type | `rhs` | any type  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 1-bit signless integer  #### `util.null` (Util::NullOp)  _Returns a null type value_   Syntax:  <pre><code>operation ::= `util.null` attr-dict `:` type($result)\n</code></pre>  Defines an SSA value that is lowered into dialects supporting null/undefined/optional/etc values.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | any type   ### Value utility ops    #### `util.switch` (Util::SwitchOp)  _Primitive switch operation_   Syntax:  <pre><code>operation ::= `util.switch` type($default_value) `from`\n              custom&lt;TypedValueList&gt;(ref(type($default_value)), $values, type($values))\n              `at` $index\n              `else` $default_value\n              attr-dict\n              `:` type($result)\n</code></pre>  Returns the value with the given `index` in `values` or `default_value` if the index is out of bounds.  <pre><code>// Switch %index to cases of %c100/%c200/%c300 if index==0, ==1, ==2.\n// If %index is out of range (&lt;0 or &gt;2) then default to %c5.\n%0 = util.switch %index[%c100, %c200, %c300] else %c5 : i32\n</code></pre>  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, InferTypeOpInterface, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `index` | index | `default_value` | index or integer or floating-point | `values` | index or integer or floating-point  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | index or integer or floating-point   ## Type definition  ### BufferType  a reference counted byte buffer  Syntax: `!util.buffer`  A reference counted byte buffer that models a pointer, offset, and length.  ### ListType  dense list container type  Syntax:  <pre><code>!util.list&lt;\n  Type   # element_type\n&gt;\n</code></pre>  Typed container supporting variant storage.  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | element_type | `Type` |  |  ### ObjectType  a placeholder for an unspecified object type  Syntax: `!util.object`  Describes a runtime object type. These may be reference counted or garbage collected at runtime.  ### PtrType  a pointer-like reference  Syntax:  <pre><code>!util.ptr&lt;\n  Type   # target_type\n&gt;\n</code></pre>  A typed indirect reference to a value. These define a runtime addressable value that is strongly referenced.  ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | target_type | `Type` |  |  ### VariantType  a placeholder for a variant type (`?`)  Syntax: `!util.variant`  Describes a runtime variant type. These may be primitives (i32, f32, etc) or object types."},{"location":"reference/mlir-dialects/VM/","title":"VM","text":""},{"location":"reference/mlir-dialects/VM/#vm-dialect","title":"'vm' Dialect","text":"<p>A dialect representing operations against an abstract virtual machine.</p> <p>The virtual machine ops are designed to be either serialized to a bytecode representation that can be interpreted at runtime or lowered further to static representations such as LLVM IR, C, etc. The idea is that the types and operations performed are generally just encoding resource ownership rules and control flow that can be represented in many different ways by target runtimes. For example, it should be possible to lower the VM dialect to SPIR-V and run the VM entirely within a persistent Vulkan kernel.</p> <p>With this scalable runtime approach we make some limiting assumptions to keep the required implementations simple. As we assume all real math is happening within dispatch regions the only math we provide is scalar operations used for offset and shape calculations. This also enables simple flow control such as fixed-range loops.</p> <p>Besides integer values the only other storage type is a variant reference modeling an abstract iree_vm_ref_t. This allows automated reference counting to be relied upon by other dialects built on top of the VM dialect and avoids the need for more verbose manual reference counting logic (that may be difficult or impossible to manage given the coroutine-like nature of the VM). Lowering targets can insert the reference counting as needed.</p> <p>The types in the VM dialect correspond to the storage rather than value type, with the interpretation of the type encoded on the op.</p> <ul> <li>'vm' Dialect<ul> <li>Operation definition<ul> <li>Async/fiber ops<ul> <li>vm.yield (VM::YieldOp)</li> </ul> </li> <li>Bitwise shift and rotate ops<ul> <li>vm.shl.i32 (VM::ShlI32Op)</li> <li>vm.shl.i64 (VM::ShlI64Op)</li> <li>vm.shr.i32.s (VM::ShrI32SOp)</li> <li>vm.shr.i32.u (VM::ShrI32UOp)</li> <li>vm.shr.i64.s (VM::ShrI64SOp)</li> <li>vm.shr.i64.u (VM::ShrI64UOp)</li> </ul> </li> <li>Buffer ops<ul> <li>vm.buffer.alloc (VM::BufferAllocOp)</li> <li>vm.buffer.clone (VM::BufferCloneOp)</li> <li>vm.buffer.compare (VM::BufferCompareOp)</li> <li>vm.buffer.copy (VM::BufferCopyOp)</li> <li>vm.buffer.fill.f32 (VM::BufferFillF32Op)</li> <li>vm.buffer.fill.f64 (VM::BufferFillF64Op)</li> <li>vm.buffer.fill.i16 (VM::BufferFillI16Op)</li> <li>vm.buffer.fill.i32 (VM::BufferFillI32Op)</li> <li>vm.buffer.fill.i64 (VM::BufferFillI64Op)</li> <li>vm.buffer.fill.i8 (VM::BufferFillI8Op)</li> <li>vm.buffer.length (VM::BufferLengthOp)</li> <li>vm.buffer.load.f32 (VM::BufferLoadF32Op)</li> <li>vm.buffer.load.f64 (VM::BufferLoadF64Op)</li> <li>vm.buffer.load.i16.s (VM::BufferLoadI16SOp)</li> <li>vm.buffer.load.i16.u (VM::BufferLoadI16UOp)</li> <li>vm.buffer.load.i32 (VM::BufferLoadI32Op)</li> <li>vm.buffer.load.i64 (VM::BufferLoadI64Op)</li> <li>vm.buffer.load.i8.s (VM::BufferLoadI8SOp)</li> <li>vm.buffer.load.i8.u (VM::BufferLoadI8UOp)</li> <li>vm.buffer.store.f32 (VM::BufferStoreF32Op)</li> <li>vm.buffer.store.f64 (VM::BufferStoreF64Op)</li> <li>vm.buffer.store.i16 (VM::BufferStoreI16Op)</li> <li>vm.buffer.store.i32 (VM::BufferStoreI32Op)</li> <li>vm.buffer.store.i64 (VM::BufferStoreI64Op)</li> <li>vm.buffer.store.i8 (VM::BufferStoreI8Op)</li> </ul> </li> <li>Casting and conversion ops<ul> <li>vm.bitcast.f32.i32 (VM::BitcastF32I32Op)</li> <li>vm.bitcast.f64.i64 (VM::BitcastF64I64Op)</li> <li>vm.bitcast.i32.f32 (VM::BitcastI32F32Op)</li> <li>vm.bitcast.i64.f64 (VM::BitcastI64F64Op)</li> <li>vm.cast.any.ref (VM::CastAnyRefOp)</li> <li>vm.cast.f32.si32 (VM::CastF32SI32Op)</li> <li>vm.cast.f32.ui32 (VM::CastF32UI32Op)</li> <li>vm.cast.ref.any (VM::CastRefAnyOp)</li> <li>vm.cast.si32.f32 (VM::CastSI32F32Op)</li> <li>vm.cast.ui32.f32 (VM::CastUI32F32Op)</li> <li>vm.ext.f32.f64 (VM::ExtF32F64Op)</li> <li>vm.ext.i16.i32.s (VM::ExtI16I32SOp)</li> <li>vm.ext.i16.i32.u (VM::ExtI16I32UOp)</li> <li>vm.ext.i16.i64.s (VM::ExtI16I64SOp)</li> <li>vm.ext.i16.i64.u (VM::ExtI16I64UOp)</li> <li>vm.ext.i32.i64.s (VM::ExtI32I64SOp)</li> <li>vm.ext.i32.i64.u (VM::ExtI32I64UOp)</li> <li>vm.ext.i8.i32.s (VM::ExtI8I32SOp)</li> <li>vm.ext.i8.i32.u (VM::ExtI8I32UOp)</li> <li>vm.ext.i8.i64.s (VM::ExtI8I64SOp)</li> <li>vm.ext.i8.i64.u (VM::ExtI8I64UOp)</li> <li>vm.trunc.f64.f32 (VM::TruncF64F32Op)</li> <li>vm.trunc.i16.i8 (VM::TruncI16I8Op)</li> <li>vm.trunc.i32.i16 (VM::TruncI32I16Op)</li> <li>vm.trunc.i32.i8 (VM::TruncI32I8Op)</li> <li>vm.trunc.i64.i16 (VM::TruncI64I16Op)</li> <li>vm.trunc.i64.i32 (VM::TruncI64I32Op)</li> <li>vm.trunc.i64.i8 (VM::TruncI64I8Op)</li> </ul> </li> <li>Comparison ops<ul> <li>vm.cmp.eq.i32 (VM::CmpEQI32Op)</li> <li>vm.cmp.eq.i64 (VM::CmpEQI64Op)</li> <li>vm.cmp.gte.i32.s (VM::CmpGTEI32SOp)</li> <li>vm.cmp.gte.i32.u (VM::CmpGTEI32UOp)</li> <li>vm.cmp.gte.i64.s (VM::CmpGTEI64SOp)</li> <li>vm.cmp.gte.i64.u (VM::CmpGTEI64UOp)</li> <li>vm.cmp.gt.i32.s (VM::CmpGTI32SOp)</li> <li>vm.cmp.gt.i32.u (VM::CmpGTI32UOp)</li> <li>vm.cmp.gt.i64.s (VM::CmpGTI64SOp)</li> <li>vm.cmp.gt.i64.u (VM::CmpGTI64UOp)</li> <li>vm.cmp.lte.i32.s (VM::CmpLTEI32SOp)</li> <li>vm.cmp.lte.i32.u (VM::CmpLTEI32UOp)</li> <li>vm.cmp.lte.i64.s (VM::CmpLTEI64SOp)</li> <li>vm.cmp.lte.i64.u (VM::CmpLTEI64UOp)</li> <li>vm.cmp.lt.i32.s (VM::CmpLTI32SOp)</li> <li>vm.cmp.lt.i32.u (VM::CmpLTI32UOp)</li> <li>vm.cmp.lt.i64.s (VM::CmpLTI64SOp)</li> <li>vm.cmp.lt.i64.u (VM::CmpLTI64UOp)</li> <li>vm.cmp.ne.i32 (VM::CmpNEI32Op)</li> <li>vm.cmp.ne.i64 (VM::CmpNEI64Op)</li> <li>vm.cmp.nz.i32 (VM::CmpNZI32Op)</li> <li>vm.cmp.nz.i64 (VM::CmpNZI64Op)</li> </ul> </li> <li>Conditional assignment ops<ul> <li>vm.select.f32 (VM::SelectF32Op)</li> <li>vm.select.f64 (VM::SelectF64Op)</li> <li>vm.select.i32 (VM::SelectI32Op)</li> <li>vm.select.i64 (VM::SelectI64Op)</li> <li>vm.select.ref (VM::SelectRefOp)</li> <li>vm.switch.f32 (VM::SwitchF32Op)</li> <li>vm.switch.f64 (VM::SwitchF64Op)</li> <li>vm.switch.i32 (VM::SwitchI32Op)</li> <li>vm.switch.i64 (VM::SwitchI64Op)</li> <li>vm.switch.ref (VM::SwitchRefOp)</li> </ul> </li> <li>Constant ops<ul> <li>vm.const.f32 (VM::ConstF32Op)</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/mlir-dialects/VM/#operation-definition","title":"Operation definition","text":""},{"location":"reference/mlir-dialects/VM/#asyncfiber-ops","title":"Async/fiber ops","text":""},{"location":"reference/mlir-dialects/VM/#vmyield-vmyieldop","title":"<code>vm.yield</code> (VM::YieldOp)","text":"<p>Unconditional fiber yield operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.yield` $dest (`(` $destOperands^ `:` type($destOperands) `)`)? attr-dict\n</code></pre> <p>Yields the fiber for some (likely short) amount of time. This can be used to  perform cooperative scheduling and ensure fair (enough) execution. Execution  resumes at the specified target branch.</p> <p><code>^bb0:    vm.yield ^on_resume  ^on_resume:    ...</code></p> <p>Traits: HasParent, Terminator, Util_YieldPoint <p>Interfaces: BranchOpInterface, VMSerializableOp, VM_OpInterface</p>"},{"location":"reference/mlir-dialects/VM/#operands","title":"Operands:","text":"Operand Description <code>destOperands</code> 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float or 32-bit signless integer or ref"},{"location":"reference/mlir-dialects/VM/#successors","title":"Successors:","text":"Successor Description <code>dest</code> any successor"},{"location":"reference/mlir-dialects/VM/#bitwise-shift-and-rotate-ops","title":"Bitwise shift and rotate ops","text":""},{"location":"reference/mlir-dialects/VM/#vmshli32-vmshli32op","title":"<code>vm.shl.i32</code> (VM::ShlI32Op)","text":"<p>Integer shift left operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.shl.i32` $operand `,` $amount attr-dict `:` type($operand)\n</code></pre> <p>Shifts the operand in a direction by the number of bits specified.</p> <p>Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_1","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit signless integer <code>amount</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmshli64-vmshli64op","title":"<code>vm.shl.i64</code> (VM::ShlI64Op)","text":"<p>Integer shift left operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.shl.i64` $operand `,` $amount attr-dict `:` type($operand)\n</code></pre> <p>Shifts the operand in a direction by the number of bits specified.</p> <p>Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_2","title":"Operands:","text":"Operand Description <code>operand</code> 64-bit signless integer <code>amount</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_1","title":"Results:","text":"Result Description <code>result</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmshri32s-vmshri32sop","title":"<code>vm.shr.i32.s</code> (VM::ShrI32SOp)","text":"<p>Signed integer (arithmetic) shift right operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.shr.i32.s` $operand `,` $amount attr-dict `:` type($operand)\n</code></pre> <p>Shifts the operand in a direction by the number of bits specified.</p> <p>Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_3","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit signless integer <code>amount</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_2","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmshri32u-vmshri32uop","title":"<code>vm.shr.i32.u</code> (VM::ShrI32UOp)","text":"<p>Unsigned integer (logical) shift right operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.shr.i32.u` $operand `,` $amount attr-dict `:` type($operand)\n</code></pre> <p>Shifts the operand in a direction by the number of bits specified.</p> <p>Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_4","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit signless integer <code>amount</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_3","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmshri64s-vmshri64sop","title":"<code>vm.shr.i64.s</code> (VM::ShrI64SOp)","text":"<p>Signed integer (arithmetic) shift right operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.shr.i64.s` $operand `,` $amount attr-dict `:` type($operand)\n</code></pre> <p>Shifts the operand in a direction by the number of bits specified.</p> <p>Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_5","title":"Operands:","text":"Operand Description <code>operand</code> 64-bit signless integer <code>amount</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_4","title":"Results:","text":"Result Description <code>result</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmshri64u-vmshri64uop","title":"<code>vm.shr.i64.u</code> (VM::ShrI64UOp)","text":"<p>Unsigned integer (logical) shift right operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.shr.i64.u` $operand `,` $amount attr-dict `:` type($operand)\n</code></pre> <p>Shifts the operand in a direction by the number of bits specified.</p> <p>Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_6","title":"Operands:","text":"Operand Description <code>operand</code> 64-bit signless integer <code>amount</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_5","title":"Results:","text":"Result Description <code>result</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#buffer-ops","title":"Buffer ops","text":""},{"location":"reference/mlir-dialects/VM/#vmbufferalloc-vmbufferallocop","title":"<code>vm.buffer.alloc</code> (VM::BufferAllocOp)","text":"<p>Allocates a new zero-initialized buffer</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.alloc` operands attr-dict `:` type($result)\n</code></pre> <p>Allocates a new zero-initialized buffer with the given size in bytes.</p> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Allocate on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_7","title":"Operands:","text":"Operand Description <code>length</code> 64-bit signless integer <code>alignment</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_6","title":"Results:","text":"Result Description <code>result</code> ref"},{"location":"reference/mlir-dialects/VM/#vmbufferclone-vmbuffercloneop","title":"<code>vm.buffer.clone</code> (VM::BufferCloneOp)","text":"<p>Clones a buffer</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.clone` operands attr-dict `:` type($source_buffer) `-&gt;` type($result)\n</code></pre> <p>Clones a range of the source buffer to produce a mutable buffer with the same contents.</p> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Allocate on ::mlir::SideEffects::DefaultResource, MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_8","title":"Operands:","text":"Operand Description <code>source_buffer</code> ref <code>source_offset</code> 64-bit signless integer <code>length</code> 64-bit signless integer <code>alignment</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_7","title":"Results:","text":"Result Description <code>result</code> ref"},{"location":"reference/mlir-dialects/VM/#vmbuffercompare-vmbuffercompareop","title":"<code>vm.buffer.compare</code> (VM::BufferCompareOp)","text":"<p>Compares a range of a buffer to another</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.compare` operands attr-dict `:` type($lhs_buffer) `,` type($rhs_buffer)\n</code></pre> <p>Returns 1 if the two ranges are bitwise equivalent, somewhat like memcmp.</p> <p>Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource, MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_9","title":"Operands:","text":"Operand Description <code>lhs_buffer</code> ref <code>lhs_offset</code> 64-bit signless integer <code>rhs_buffer</code> ref <code>rhs_offset</code> 64-bit signless integer <code>length</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_8","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbuffercopy-vmbuffercopyop","title":"<code>vm.buffer.copy</code> (VM::BufferCopyOp)","text":"<p>Copies a range of a buffer to another</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.copy` operands attr-dict `:` type($source_buffer) `-&gt;` type($target_buffer)\n</code></pre> <p>Copies a range of one buffer to another, like memcpy.</p> <p>Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource, MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_10","title":"Operands:","text":"Operand Description <code>source_buffer</code> ref <code>source_offset</code> 64-bit signless integer <code>target_buffer</code> ref <code>target_offset</code> 64-bit signless integer <code>length</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbufferfillf32-vmbufferfillf32op","title":"<code>vm.buffer.fill.f32</code> (VM::BufferFillF32Op)","text":"<p>Fills the buffer with the given repeating 32-bit value</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.fill.f32` $target_buffer `,` $target_offset `,` $length `,` $value\n              attr-dict `:` type($value) `-&gt;` type($target_buffer)\n</code></pre> <p>Fills an element range of the buffer with the given value, like memset.</p> <p>Traits: VM_ExtF32</p> <p>Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_11","title":"Operands:","text":"Operand Description <code>target_buffer</code> ref <code>target_offset</code> 64-bit signless integer <code>length</code> 64-bit signless integer <code>value</code> 32-bit float or 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbufferfillf64-vmbufferfillf64op","title":"<code>vm.buffer.fill.f64</code> (VM::BufferFillF64Op)","text":"<p>Fills the buffer with the given repeating 64-bit value</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.fill.f64` $target_buffer `,` $target_offset `,` $length `,` $value\n              attr-dict `:` type($value) `-&gt;` type($target_buffer)\n</code></pre> <p>Fills an element range of the buffer with the given value, like memset.</p> <p>Traits: VM_ExtF64</p> <p>Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_12","title":"Operands:","text":"Operand Description <code>target_buffer</code> ref <code>target_offset</code> 64-bit signless integer <code>length</code> 64-bit signless integer <code>value</code> 64-bit float or 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbufferfilli16-vmbufferfilli16op","title":"<code>vm.buffer.fill.i16</code> (VM::BufferFillI16Op)","text":"<p>Fills the buffer with the given repeating 16-bit value</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.fill.i16` $target_buffer `,` $target_offset `,` $length `,` $value\n              attr-dict `:` type($value) `-&gt;` type($target_buffer)\n</code></pre> <p>Fills an element range of the buffer with the given value, like memset.</p> <p>Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_13","title":"Operands:","text":"Operand Description <code>target_buffer</code> ref <code>target_offset</code> 64-bit signless integer <code>length</code> 64-bit signless integer <code>value</code> 16-bit signless integer or 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbufferfilli32-vmbufferfilli32op","title":"<code>vm.buffer.fill.i32</code> (VM::BufferFillI32Op)","text":"<p>Fills the buffer with the given repeating 32-bit value</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.fill.i32` $target_buffer `,` $target_offset `,` $length `,` $value\n              attr-dict `:` type($value) `-&gt;` type($target_buffer)\n</code></pre> <p>Fills an element range of the buffer with the given value, like memset.</p> <p>Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_14","title":"Operands:","text":"Operand Description <code>target_buffer</code> ref <code>target_offset</code> 64-bit signless integer <code>length</code> 64-bit signless integer <code>value</code> 32-bit signless integer or 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbufferfilli64-vmbufferfilli64op","title":"<code>vm.buffer.fill.i64</code> (VM::BufferFillI64Op)","text":"<p>Fills the buffer with the given repeating 64-bit value</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.fill.i64` $target_buffer `,` $target_offset `,` $length `,` $value\n              attr-dict `:` type($value) `-&gt;` type($target_buffer)\n</code></pre> <p>Fills an element range of the buffer with the given value, like memset.</p> <p>Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_15","title":"Operands:","text":"Operand Description <code>target_buffer</code> ref <code>target_offset</code> 64-bit signless integer <code>length</code> 64-bit signless integer <code>value</code> 64-bit signless integer or 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbufferfilli8-vmbufferfilli8op","title":"<code>vm.buffer.fill.i8</code> (VM::BufferFillI8Op)","text":"<p>Fills the buffer with the given repeating 8-bit value</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.fill.i8` $target_buffer `,` $target_offset `,` $length `,` $value\n              attr-dict `:` type($value) `-&gt;` type($target_buffer)\n</code></pre> <p>Fills an element range of the buffer with the given value, like memset.</p> <p>Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_16","title":"Operands:","text":"Operand Description <code>target_buffer</code> ref <code>target_offset</code> 64-bit signless integer <code>length</code> 64-bit signless integer <code>value</code> 8-bit signless integer or 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbufferlength-vmbufferlengthop","title":"<code>vm.buffer.length</code> (VM::BufferLengthOp)","text":"<p>Returns the byte length of a buffer</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.length` operands attr-dict `:` type($buffer) `-&gt;` type($result)\n</code></pre> <p>Returns the total byte length of the given buffer. This is the exact value as specified during buffer allocation though the underlying system buffer may have additional padding.</p> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_17","title":"Operands:","text":"Operand Description <code>buffer</code> ref"},{"location":"reference/mlir-dialects/VM/#results_9","title":"Results:","text":"Result Description <code>result</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbufferloadf32-vmbufferloadf32op","title":"<code>vm.buffer.load.f32</code> (VM::BufferLoadF32Op)","text":"<p>32-bit floating-point load</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.load.f32` $source_buffer `[` $source_offset `]`\n              attr-dict `:` type($source_buffer) `-&gt;` type($result)\n</code></pre> <p>Loads a value from the buffer at the given element offset.</p> <p>Traits: VM_ExtF32</p> <p>Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_18","title":"Operands:","text":"Operand Description <code>source_buffer</code> ref <code>source_offset</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_10","title":"Results:","text":"Result Description <code>result</code> 32-bit float or 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbufferloadf64-vmbufferloadf64op","title":"<code>vm.buffer.load.f64</code> (VM::BufferLoadF64Op)","text":"<p>64-bit floating-point load</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.load.f64` $source_buffer `[` $source_offset `]`\n              attr-dict `:` type($source_buffer) `-&gt;` type($result)\n</code></pre> <p>Loads a value from the buffer at the given element offset.</p> <p>Traits: VM_ExtF64</p> <p>Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_19","title":"Operands:","text":"Operand Description <code>source_buffer</code> ref <code>source_offset</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_11","title":"Results:","text":"Result Description <code>result</code> 64-bit float or 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbufferloadi16s-vmbufferloadi16sop","title":"<code>vm.buffer.load.i16.s</code> (VM::BufferLoadI16SOp)","text":"<p>Signed 16-bit integer load</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.load.i16.s` $source_buffer `[` $source_offset `]`\n              attr-dict `:` type($source_buffer) `-&gt;` type($result)\n</code></pre> <p>Loads a value from the buffer at the given element offset.</p> <p>Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_20","title":"Operands:","text":"Operand Description <code>source_buffer</code> ref <code>source_offset</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_12","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer or 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbufferloadi16u-vmbufferloadi16uop","title":"<code>vm.buffer.load.i16.u</code> (VM::BufferLoadI16UOp)","text":"<p>Unsigned 16-bit integer load</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.load.i16.u` $source_buffer `[` $source_offset `]`\n              attr-dict `:` type($source_buffer) `-&gt;` type($result)\n</code></pre> <p>Loads a value from the buffer at the given element offset.</p> <p>Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_21","title":"Operands:","text":"Operand Description <code>source_buffer</code> ref <code>source_offset</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_13","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer or 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbufferloadi32-vmbufferloadi32op","title":"<code>vm.buffer.load.i32</code> (VM::BufferLoadI32Op)","text":"<p>32-bit integer load</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.load.i32` $source_buffer `[` $source_offset `]`\n              attr-dict `:` type($source_buffer) `-&gt;` type($result)\n</code></pre> <p>Loads a value from the buffer at the given element offset.</p> <p>Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_22","title":"Operands:","text":"Operand Description <code>source_buffer</code> ref <code>source_offset</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_14","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer or 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbufferloadi64-vmbufferloadi64op","title":"<code>vm.buffer.load.i64</code> (VM::BufferLoadI64Op)","text":"<p>64-bit integer load</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.load.i64` $source_buffer `[` $source_offset `]`\n              attr-dict `:` type($source_buffer) `-&gt;` type($result)\n</code></pre> <p>Loads a value from the buffer at the given element offset.</p> <p>Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_23","title":"Operands:","text":"Operand Description <code>source_buffer</code> ref <code>source_offset</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_15","title":"Results:","text":"Result Description <code>result</code> 64-bit signless integer or 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbufferloadi8s-vmbufferloadi8sop","title":"<code>vm.buffer.load.i8.s</code> (VM::BufferLoadI8SOp)","text":"<p>Signed 8-bit integer load</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.load.i8.s` $source_buffer `[` $source_offset `]`\n              attr-dict `:` type($source_buffer) `-&gt;` type($result)\n</code></pre> <p>Loads a value from the buffer at the given element offset.</p> <p>Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_24","title":"Operands:","text":"Operand Description <code>source_buffer</code> ref <code>source_offset</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_16","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer or 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbufferloadi8u-vmbufferloadi8uop","title":"<code>vm.buffer.load.i8.u</code> (VM::BufferLoadI8UOp)","text":"<p>Unsigned 8-bit integer load</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.load.i8.u` $source_buffer `[` $source_offset `]`\n              attr-dict `:` type($source_buffer) `-&gt;` type($result)\n</code></pre> <p>Loads a value from the buffer at the given element offset.</p> <p>Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_25","title":"Operands:","text":"Operand Description <code>source_buffer</code> ref <code>source_offset</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_17","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer or 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbufferstoref32-vmbufferstoref32op","title":"<code>vm.buffer.store.f32</code> (VM::BufferStoreF32Op)","text":"<p>32-bit floating-point store</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.store.f32` $value `,` $target_buffer `[` $target_offset `]`\n              attr-dict `:` type($value) `-&gt;` type($target_buffer)\n</code></pre> <p>Stores a value to the buffer at the given element offset.</p> <p>Traits: VM_ExtF32</p> <p>Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_26","title":"Operands:","text":"Operand Description <code>target_buffer</code> ref <code>target_offset</code> 64-bit signless integer <code>value</code> 32-bit float or 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbufferstoref64-vmbufferstoref64op","title":"<code>vm.buffer.store.f64</code> (VM::BufferStoreF64Op)","text":"<p>64-bit floating-point store</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.store.f64` $value `,` $target_buffer `[` $target_offset `]`\n              attr-dict `:` type($value) `-&gt;` type($target_buffer)\n</code></pre> <p>Stores a value to the buffer at the given element offset.</p> <p>Traits: VM_ExtF64</p> <p>Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_27","title":"Operands:","text":"Operand Description <code>target_buffer</code> ref <code>target_offset</code> 64-bit signless integer <code>value</code> 64-bit float or 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbufferstorei16-vmbufferstorei16op","title":"<code>vm.buffer.store.i16</code> (VM::BufferStoreI16Op)","text":"<p>Unsigned 16-bit integer store</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.store.i16` $value `,` $target_buffer `[` $target_offset `]`\n              attr-dict `:` type($value) `-&gt;` type($target_buffer)\n</code></pre> <p>Stores a value to the buffer at the given element offset.</p> <p>Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_28","title":"Operands:","text":"Operand Description <code>target_buffer</code> ref <code>target_offset</code> 64-bit signless integer <code>value</code> 32-bit signless integer or 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbufferstorei32-vmbufferstorei32op","title":"<code>vm.buffer.store.i32</code> (VM::BufferStoreI32Op)","text":"<p>32-bit integer store</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.store.i32` $value `,` $target_buffer `[` $target_offset `]`\n              attr-dict `:` type($value) `-&gt;` type($target_buffer)\n</code></pre> <p>Stores a value to the buffer at the given element offset.</p> <p>Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_29","title":"Operands:","text":"Operand Description <code>target_buffer</code> ref <code>target_offset</code> 64-bit signless integer <code>value</code> 32-bit signless integer or 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbufferstorei64-vmbufferstorei64op","title":"<code>vm.buffer.store.i64</code> (VM::BufferStoreI64Op)","text":"<p>64-bit integer store</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.store.i64` $value `,` $target_buffer `[` $target_offset `]`\n              attr-dict `:` type($value) `-&gt;` type($target_buffer)\n</code></pre> <p>Stores a value to the buffer at the given element offset.</p> <p>Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_30","title":"Operands:","text":"Operand Description <code>target_buffer</code> ref <code>target_offset</code> 64-bit signless integer <code>value</code> 64-bit signless integer or 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbufferstorei8-vmbufferstorei8op","title":"<code>vm.buffer.store.i8</code> (VM::BufferStoreI8Op)","text":"<p>Unsigned 8-bit integer store</p> <p>Syntax:</p> <pre><code>operation ::= `vm.buffer.store.i8` $value `,` $target_buffer `[` $target_offset `]`\n              attr-dict `:` type($value) `-&gt;` type($target_buffer)\n</code></pre> <p>Stores a value to the buffer at the given element offset.</p> <p>Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}</p>"},{"location":"reference/mlir-dialects/VM/#operands_31","title":"Operands:","text":"Operand Description <code>target_buffer</code> ref <code>target_offset</code> 64-bit signless integer <code>value</code> 32-bit signless integer or 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#casting-and-conversion-ops","title":"Casting and conversion ops","text":"<p>Casting and type conversion/emulation.</p>"},{"location":"reference/mlir-dialects/VM/#vmbitcastf32i32-vmbitcastf32i32op","title":"<code>vm.bitcast.f32.i32</code> (VM::BitcastF32I32Op)","text":"<p>Bitcast from a 32-bit float-point value to a 32-bit integer</p> <p>Syntax:</p> <pre><code>operation ::= `vm.bitcast.f32.i32` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait, VM_ExtF32</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_32","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit float"},{"location":"reference/mlir-dialects/VM/#results_18","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbitcastf64i64-vmbitcastf64i64op","title":"<code>vm.bitcast.f64.i64</code> (VM::BitcastF64I64Op)","text":"<p>Bitcast from a 64-bit float-point value to a 64-bit integer</p> <p>Syntax:</p> <pre><code>operation ::= `vm.bitcast.f64.i64` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait, VM_ExtF64</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_33","title":"Operands:","text":"Operand Description <code>operand</code> 64-bit float"},{"location":"reference/mlir-dialects/VM/#results_19","title":"Results:","text":"Result Description <code>result</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmbitcasti32f32-vmbitcasti32f32op","title":"<code>vm.bitcast.i32.f32</code> (VM::BitcastI32F32Op)","text":"<p>Bitcast from a 32-bit integer to a 32-bit float-point value</p> <p>Syntax:</p> <pre><code>operation ::= `vm.bitcast.i32.f32` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait, VM_ExtF32</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_34","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_20","title":"Results:","text":"Result Description <code>result</code> 32-bit float"},{"location":"reference/mlir-dialects/VM/#vmbitcasti64f64-vmbitcasti64f64op","title":"<code>vm.bitcast.i64.f64</code> (VM::BitcastI64F64Op)","text":"<p>Bitcast from a 64-bit integer to a 64-bit float-point value</p> <p>Syntax:</p> <pre><code>operation ::= `vm.bitcast.i64.f64` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait, VM_ExtF64</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_35","title":"Operands:","text":"Operand Description <code>operand</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_21","title":"Results:","text":"Result Description <code>result</code> 64-bit float"},{"location":"reference/mlir-dialects/VM/#vmcastanyref-vmcastanyrefop","title":"<code>vm.cast.any.ref</code> (VM::CastAnyRefOp)","text":"<p>Casts from any ref to a specific ref type</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cast.any.ref` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Performs a runtime cast of an opaque <code>!vm.ref&lt;?&gt;</code> to a specific <code>!vm.ref&lt;T&gt;</code> and raises an error if the operand does not match the expected type. Null refs can always be cast between types.</p> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_36","title":"Operands:","text":"Operand Description <code>operand</code> ref"},{"location":"reference/mlir-dialects/VM/#results_22","title":"Results:","text":"Result Description <code>result</code> ref"},{"location":"reference/mlir-dialects/VM/#vmcastf32si32-vmcastf32si32op","title":"<code>vm.cast.f32.si32</code> (VM::CastF32SI32Op)","text":"<p>Cast from a float-point value to a signed integer</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cast.f32.si32` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait, VM_ExtF32</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_37","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit float"},{"location":"reference/mlir-dialects/VM/#results_23","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcastf32ui32-vmcastf32ui32op","title":"<code>vm.cast.f32.ui32</code> (VM::CastF32UI32Op)","text":"<p>Cast from an float-point value to an unsigned integer</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cast.f32.ui32` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait, VM_ExtF32</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_38","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit float"},{"location":"reference/mlir-dialects/VM/#results_24","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcastrefany-vmcastrefanyop","title":"<code>vm.cast.ref.any</code> (VM::CastRefAnyOp)","text":"<p>Casts from a specific ref to any ref type</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cast.ref.any` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Performs a compile-time widening cast of a specific <code>!vm.ref&lt;T&gt;</code> to an opaque <code>!vm.ref&lt;?&gt;</code>.</p> <p>Traits: AlwaysSpeculatableImplTrait, VM_AssignmentOp</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_39","title":"Operands:","text":"Operand Description <code>operand</code> ref"},{"location":"reference/mlir-dialects/VM/#results_25","title":"Results:","text":"Result Description <code>result</code> ref"},{"location":"reference/mlir-dialects/VM/#vmcastsi32f32-vmcastsi32f32op","title":"<code>vm.cast.si32.f32</code> (VM::CastSI32F32Op)","text":"<p>Cast from a signed integer to a float-point value</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cast.si32.f32` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait, VM_ExtF32</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_40","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_26","title":"Results:","text":"Result Description <code>result</code> 32-bit float"},{"location":"reference/mlir-dialects/VM/#vmcastui32f32-vmcastui32f32op","title":"<code>vm.cast.ui32.f32</code> (VM::CastUI32F32Op)","text":"<p>Cast from an unsigned integer to a float-point value</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cast.ui32.f32` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait, VM_ExtF32</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_41","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_27","title":"Results:","text":"Result Description <code>result</code> 32-bit float"},{"location":"reference/mlir-dialects/VM/#vmextf32f64-vmextf32f64op","title":"<code>vm.ext.f32.f64</code> (VM::ExtF32F64Op)","text":"<p>Floating-point zero extend 32 bits to 64 bits</p> <p>Syntax:</p> <pre><code>operation ::= `vm.ext.f32.f64` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait, VM_ExtF64</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_42","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit float"},{"location":"reference/mlir-dialects/VM/#results_28","title":"Results:","text":"Result Description <code>result</code> 64-bit float"},{"location":"reference/mlir-dialects/VM/#vmexti16i32s-vmexti16i32sop","title":"<code>vm.ext.i16.i32.s</code> (VM::ExtI16I32SOp)","text":"<p>Integer sign extend 16 bits to 32 bits</p> <p>Syntax:</p> <pre><code>operation ::= `vm.ext.i16.i32.s` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_43","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_29","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmexti16i32u-vmexti16i32uop","title":"<code>vm.ext.i16.i32.u</code> (VM::ExtI16I32UOp)","text":"<p>Integer zero extend 16 bits to 32 bits</p> <p>Syntax:</p> <pre><code>operation ::= `vm.ext.i16.i32.u` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_44","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_30","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmexti16i64s-vmexti16i64sop","title":"<code>vm.ext.i16.i64.s</code> (VM::ExtI16I64SOp)","text":"<p>Integer sign extend 16 bits to 64 bits</p> <p>Syntax:</p> <pre><code>operation ::= `vm.ext.i16.i64.s` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_45","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_31","title":"Results:","text":"Result Description <code>result</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmexti16i64u-vmexti16i64uop","title":"<code>vm.ext.i16.i64.u</code> (VM::ExtI16I64UOp)","text":"<p>Integer zero extend 16 bits to 64 bits</p> <p>Syntax:</p> <pre><code>operation ::= `vm.ext.i16.i64.u` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_46","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_32","title":"Results:","text":"Result Description <code>result</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmexti32i64s-vmexti32i64sop","title":"<code>vm.ext.i32.i64.s</code> (VM::ExtI32I64SOp)","text":"<p>Integer sign extend 32 bits to 64 bits</p> <p>Syntax:</p> <pre><code>operation ::= `vm.ext.i32.i64.s` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_47","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_33","title":"Results:","text":"Result Description <code>result</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmexti32i64u-vmexti32i64uop","title":"<code>vm.ext.i32.i64.u</code> (VM::ExtI32I64UOp)","text":"<p>Integer zero extend 32 bits to 64 bits</p> <p>Syntax:</p> <pre><code>operation ::= `vm.ext.i32.i64.u` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_48","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_34","title":"Results:","text":"Result Description <code>result</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmexti8i32s-vmexti8i32sop","title":"<code>vm.ext.i8.i32.s</code> (VM::ExtI8I32SOp)","text":"<p>Integer sign extend 8 bits to 32 bits</p> <p>Syntax:</p> <pre><code>operation ::= `vm.ext.i8.i32.s` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_49","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_35","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmexti8i32u-vmexti8i32uop","title":"<code>vm.ext.i8.i32.u</code> (VM::ExtI8I32UOp)","text":"<p>Integer zero extend 8 bits to 32 bits</p> <p>Syntax:</p> <pre><code>operation ::= `vm.ext.i8.i32.u` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_50","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_36","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmexti8i64s-vmexti8i64sop","title":"<code>vm.ext.i8.i64.s</code> (VM::ExtI8I64SOp)","text":"<p>Integer sign extend 8 bits to 64 bits</p> <p>Syntax:</p> <pre><code>operation ::= `vm.ext.i8.i64.s` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_51","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_37","title":"Results:","text":"Result Description <code>result</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmexti8i64u-vmexti8i64uop","title":"<code>vm.ext.i8.i64.u</code> (VM::ExtI8I64UOp)","text":"<p>Integer zero extend 8 bits to 64 bits</p> <p>Syntax:</p> <pre><code>operation ::= `vm.ext.i8.i64.u` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_52","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_38","title":"Results:","text":"Result Description <code>result</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmtruncf64f32-vmtruncf64f32op","title":"<code>vm.trunc.f64.f32</code> (VM::TruncF64F32Op)","text":"<p>Floating-point truncate to 32 bits</p> <p>Syntax:</p> <pre><code>operation ::= `vm.trunc.f64.f32` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait, VM_ExtF64</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_53","title":"Operands:","text":"Operand Description <code>operand</code> 64-bit float"},{"location":"reference/mlir-dialects/VM/#results_39","title":"Results:","text":"Result Description <code>result</code> 32-bit float"},{"location":"reference/mlir-dialects/VM/#vmtrunci16i8-vmtrunci16i8op","title":"<code>vm.trunc.i16.i8</code> (VM::TruncI16I8Op)","text":"<p>Integer truncate to 8 bits</p> <p>Syntax:</p> <pre><code>operation ::= `vm.trunc.i16.i8` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_54","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_40","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmtrunci32i16-vmtrunci32i16op","title":"<code>vm.trunc.i32.i16</code> (VM::TruncI32I16Op)","text":"<p>Integer truncate to 16 bits</p> <p>Syntax:</p> <pre><code>operation ::= `vm.trunc.i32.i16` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_55","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_41","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmtrunci32i8-vmtrunci32i8op","title":"<code>vm.trunc.i32.i8</code> (VM::TruncI32I8Op)","text":"<p>Integer truncate to 8 bits</p> <p>Syntax:</p> <pre><code>operation ::= `vm.trunc.i32.i8` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_56","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_42","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmtrunci64i16-vmtrunci64i16op","title":"<code>vm.trunc.i64.i16</code> (VM::TruncI64I16Op)","text":"<p>Integer truncate to 16 bits</p> <p>Syntax:</p> <pre><code>operation ::= `vm.trunc.i64.i16` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_57","title":"Operands:","text":"Operand Description <code>operand</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_43","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmtrunci64i32-vmtrunci64i32op","title":"<code>vm.trunc.i64.i32</code> (VM::TruncI64I32Op)","text":"<p>Integer truncate to 32 bits</p> <p>Syntax:</p> <pre><code>operation ::= `vm.trunc.i64.i32` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_58","title":"Operands:","text":"Operand Description <code>operand</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_44","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmtrunci64i8-vmtrunci64i8op","title":"<code>vm.trunc.i64.i8</code> (VM::TruncI64I8Op)","text":"<p>Integer truncate to 8 bits</p> <p>Syntax:</p> <pre><code>operation ::= `vm.trunc.i64.i8` $operand attr-dict `:` type($operand) `-&gt;` type($result)\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_59","title":"Operands:","text":"Operand Description <code>operand</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_45","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#comparison-ops","title":"Comparison ops","text":""},{"location":"reference/mlir-dialects/VM/#vmcmpeqi32-vmcmpeqi32op","title":"<code>vm.cmp.eq.i32</code> (VM::CmpEQI32Op)","text":"<p>Integer equality comparison operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cmp.eq.i32` operands attr-dict `:` type($lhs)\n</code></pre> <p>Compares two operands with the specified predicate.</p> <p>Traits: AlwaysSpeculatableImplTrait, Commutative</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_60","title":"Operands:","text":"Operand Description <code>lhs</code> 32-bit signless integer <code>rhs</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_46","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcmpeqi64-vmcmpeqi64op","title":"<code>vm.cmp.eq.i64</code> (VM::CmpEQI64Op)","text":"<p>Integer equality comparison operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cmp.eq.i64` operands attr-dict `:` type($lhs)\n</code></pre> <p>Compares two operands with the specified predicate.</p> <p>Traits: AlwaysSpeculatableImplTrait, Commutative</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_61","title":"Operands:","text":"Operand Description <code>lhs</code> 64-bit signless integer <code>rhs</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_47","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcmpgtei32s-vmcmpgtei32sop","title":"<code>vm.cmp.gte.i32.s</code> (VM::CmpGTEI32SOp)","text":"<p>Signed integer greater-than-or-equal comparison operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cmp.gte.i32.s` operands attr-dict `:` type($lhs)\n</code></pre> <p>Compares two operands with the specified predicate.</p> <p>Traits: AlwaysSpeculatableImplTrait, VM_PseudoOp</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_62","title":"Operands:","text":"Operand Description <code>lhs</code> 32-bit signless integer <code>rhs</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_48","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcmpgtei32u-vmcmpgtei32uop","title":"<code>vm.cmp.gte.i32.u</code> (VM::CmpGTEI32UOp)","text":"<p>Unsigned integer greater-than-or-equal comparison operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cmp.gte.i32.u` operands attr-dict `:` type($lhs)\n</code></pre> <p>Compares two operands with the specified predicate.</p> <p>Traits: AlwaysSpeculatableImplTrait, VM_PseudoOp</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_63","title":"Operands:","text":"Operand Description <code>lhs</code> 32-bit signless integer <code>rhs</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_49","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcmpgtei64s-vmcmpgtei64sop","title":"<code>vm.cmp.gte.i64.s</code> (VM::CmpGTEI64SOp)","text":"<p>Signed integer greater-than-or-equal comparison operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cmp.gte.i64.s` operands attr-dict `:` type($lhs)\n</code></pre> <p>Compares two operands with the specified predicate.</p> <p>Traits: AlwaysSpeculatableImplTrait, VM_PseudoOp</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_64","title":"Operands:","text":"Operand Description <code>lhs</code> 64-bit signless integer <code>rhs</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_50","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcmpgtei64u-vmcmpgtei64uop","title":"<code>vm.cmp.gte.i64.u</code> (VM::CmpGTEI64UOp)","text":"<p>Unsigned integer greater-than-or-equal comparison operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cmp.gte.i64.u` operands attr-dict `:` type($lhs)\n</code></pre> <p>Compares two operands with the specified predicate.</p> <p>Traits: AlwaysSpeculatableImplTrait, VM_PseudoOp</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_65","title":"Operands:","text":"Operand Description <code>lhs</code> 64-bit signless integer <code>rhs</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_51","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcmpgti32s-vmcmpgti32sop","title":"<code>vm.cmp.gt.i32.s</code> (VM::CmpGTI32SOp)","text":"<p>Signed integer greater-than comparison operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cmp.gt.i32.s` operands attr-dict `:` type($lhs)\n</code></pre> <p>Compares two operands with the specified predicate.</p> <p>Traits: AlwaysSpeculatableImplTrait, VM_PseudoOp</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_66","title":"Operands:","text":"Operand Description <code>lhs</code> 32-bit signless integer <code>rhs</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_52","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcmpgti32u-vmcmpgti32uop","title":"<code>vm.cmp.gt.i32.u</code> (VM::CmpGTI32UOp)","text":"<p>Unsigned integer greater-than comparison operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cmp.gt.i32.u` operands attr-dict `:` type($lhs)\n</code></pre> <p>Compares two operands with the specified predicate.</p> <p>Traits: AlwaysSpeculatableImplTrait, VM_PseudoOp</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_67","title":"Operands:","text":"Operand Description <code>lhs</code> 32-bit signless integer <code>rhs</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_53","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcmpgti64s-vmcmpgti64sop","title":"<code>vm.cmp.gt.i64.s</code> (VM::CmpGTI64SOp)","text":"<p>Signed integer greater-than comparison operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cmp.gt.i64.s` operands attr-dict `:` type($lhs)\n</code></pre> <p>Compares two operands with the specified predicate.</p> <p>Traits: AlwaysSpeculatableImplTrait, VM_PseudoOp</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_68","title":"Operands:","text":"Operand Description <code>lhs</code> 64-bit signless integer <code>rhs</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_54","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcmpgti64u-vmcmpgti64uop","title":"<code>vm.cmp.gt.i64.u</code> (VM::CmpGTI64UOp)","text":"<p>Unsigned integer greater-than comparison operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cmp.gt.i64.u` operands attr-dict `:` type($lhs)\n</code></pre> <p>Compares two operands with the specified predicate.</p> <p>Traits: AlwaysSpeculatableImplTrait, VM_PseudoOp</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_69","title":"Operands:","text":"Operand Description <code>lhs</code> 64-bit signless integer <code>rhs</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_55","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcmpltei32s-vmcmpltei32sop","title":"<code>vm.cmp.lte.i32.s</code> (VM::CmpLTEI32SOp)","text":"<p>Signed integer less-than-or-equal comparison operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cmp.lte.i32.s` operands attr-dict `:` type($lhs)\n</code></pre> <p>Compares two operands with the specified predicate.</p> <p>Traits: AlwaysSpeculatableImplTrait, VM_PseudoOp</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_70","title":"Operands:","text":"Operand Description <code>lhs</code> 32-bit signless integer <code>rhs</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_56","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcmpltei32u-vmcmpltei32uop","title":"<code>vm.cmp.lte.i32.u</code> (VM::CmpLTEI32UOp)","text":"<p>Unsigned integer less-than-or-equal comparison operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cmp.lte.i32.u` operands attr-dict `:` type($lhs)\n</code></pre> <p>Compares two operands with the specified predicate.</p> <p>Traits: AlwaysSpeculatableImplTrait, VM_PseudoOp</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_71","title":"Operands:","text":"Operand Description <code>lhs</code> 32-bit signless integer <code>rhs</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_57","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcmpltei64s-vmcmpltei64sop","title":"<code>vm.cmp.lte.i64.s</code> (VM::CmpLTEI64SOp)","text":"<p>Signed integer less-than-or-equal comparison operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cmp.lte.i64.s` operands attr-dict `:` type($lhs)\n</code></pre> <p>Compares two operands with the specified predicate.</p> <p>Traits: AlwaysSpeculatableImplTrait, VM_PseudoOp</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_72","title":"Operands:","text":"Operand Description <code>lhs</code> 64-bit signless integer <code>rhs</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_58","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcmpltei64u-vmcmpltei64uop","title":"<code>vm.cmp.lte.i64.u</code> (VM::CmpLTEI64UOp)","text":"<p>Unsigned integer less-than-or-equal comparison operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cmp.lte.i64.u` operands attr-dict `:` type($lhs)\n</code></pre> <p>Compares two operands with the specified predicate.</p> <p>Traits: AlwaysSpeculatableImplTrait, VM_PseudoOp</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_73","title":"Operands:","text":"Operand Description <code>lhs</code> 64-bit signless integer <code>rhs</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_59","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcmplti32s-vmcmplti32sop","title":"<code>vm.cmp.lt.i32.s</code> (VM::CmpLTI32SOp)","text":"<p>Signed integer less-than comparison operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cmp.lt.i32.s` operands attr-dict `:` type($lhs)\n</code></pre> <p>Compares two operands with the specified predicate.</p> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_74","title":"Operands:","text":"Operand Description <code>lhs</code> 32-bit signless integer <code>rhs</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_60","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcmplti32u-vmcmplti32uop","title":"<code>vm.cmp.lt.i32.u</code> (VM::CmpLTI32UOp)","text":"<p>Unsigned integer less-than comparison operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cmp.lt.i32.u` operands attr-dict `:` type($lhs)\n</code></pre> <p>Compares two operands with the specified predicate.</p> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_75","title":"Operands:","text":"Operand Description <code>lhs</code> 32-bit signless integer <code>rhs</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_61","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcmplti64s-vmcmplti64sop","title":"<code>vm.cmp.lt.i64.s</code> (VM::CmpLTI64SOp)","text":"<p>Signed integer less-than comparison operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cmp.lt.i64.s` operands attr-dict `:` type($lhs)\n</code></pre> <p>Compares two operands with the specified predicate.</p> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_76","title":"Operands:","text":"Operand Description <code>lhs</code> 64-bit signless integer <code>rhs</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_62","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcmplti64u-vmcmplti64uop","title":"<code>vm.cmp.lt.i64.u</code> (VM::CmpLTI64UOp)","text":"<p>Unsigned integer less-than comparison operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cmp.lt.i64.u` operands attr-dict `:` type($lhs)\n</code></pre> <p>Compares two operands with the specified predicate.</p> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_77","title":"Operands:","text":"Operand Description <code>lhs</code> 64-bit signless integer <code>rhs</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_63","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcmpnei32-vmcmpnei32op","title":"<code>vm.cmp.ne.i32</code> (VM::CmpNEI32Op)","text":"<p>Integer inequality comparison operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cmp.ne.i32` operands attr-dict `:` type($lhs)\n</code></pre> <p>Compares two operands with the specified predicate.</p> <p>Traits: AlwaysSpeculatableImplTrait, Commutative</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_78","title":"Operands:","text":"Operand Description <code>lhs</code> 32-bit signless integer <code>rhs</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_64","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcmpnei64-vmcmpnei64op","title":"<code>vm.cmp.ne.i64</code> (VM::CmpNEI64Op)","text":"<p>Integer inequality comparison operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cmp.ne.i64` operands attr-dict `:` type($lhs)\n</code></pre> <p>Compares two operands with the specified predicate.</p> <p>Traits: AlwaysSpeculatableImplTrait, Commutative</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_79","title":"Operands:","text":"Operand Description <code>lhs</code> 64-bit signless integer <code>rhs</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_65","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcmpnzi32-vmcmpnzi32op","title":"<code>vm.cmp.nz.i32</code> (VM::CmpNZI32Op)","text":"<p>Integer non-zero comparison operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cmp.nz.i32` $operand attr-dict `:` type($operand)\n</code></pre> <p>Compares the given integer operand for a non-zero value.</p> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_80","title":"Operands:","text":"Operand Description <code>operand</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_66","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmcmpnzi64-vmcmpnzi64op","title":"<code>vm.cmp.nz.i64</code> (VM::CmpNZI64Op)","text":"<p>Integer non-zero comparison operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.cmp.nz.i64` $operand attr-dict `:` type($operand)\n</code></pre> <p>Compares the given integer operand for a non-zero value.</p> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_81","title":"Operands:","text":"Operand Description <code>operand</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_67","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#conditional-assignment-ops","title":"Conditional assignment ops","text":""},{"location":"reference/mlir-dialects/VM/#vmselectf32-vmselectf32op","title":"<code>vm.select.f32</code> (VM::SelectF32Op)","text":"<p>Floating-point select operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.select.f32` operands attr-dict `:` type($result)\n</code></pre> <p>Chooses one value based on a binary condition supplied as its first operand. If the value of the condition is true the <code>true_value</code> operand is chosen, otherwise the <code>false_value</code> operand is chosen. The true and false values must have the same types. For example, the maximum operation is obtained by combining \"select\" with \"cmpi\" as follows:</p> <pre><code>%2 = vm.cmp.gt.i32.s %0, %1 : i32\n%3 = vm.select.i32 %2, %0, %1 : i32\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait, VM_ExtF32</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_82","title":"Operands:","text":"Operand Description <code>condition</code> 32-bit signless integer <code>true_value</code> 32-bit float <code>false_value</code> 32-bit float"},{"location":"reference/mlir-dialects/VM/#results_68","title":"Results:","text":"Result Description <code>result</code> 32-bit float"},{"location":"reference/mlir-dialects/VM/#vmselectf64-vmselectf64op","title":"<code>vm.select.f64</code> (VM::SelectF64Op)","text":"<p>Floating-point select operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.select.f64` operands attr-dict `:` type($result)\n</code></pre> <p>Chooses one value based on a binary condition supplied as its first operand. If the value of the condition is true the <code>true_value</code> operand is chosen, otherwise the <code>false_value</code> operand is chosen. The true and false values must have the same types. For example, the maximum operation is obtained by combining \"select\" with \"cmpi\" as follows:</p> <pre><code>%2 = vm.cmp.gt.i32.s %0, %1 : i32\n%3 = vm.select.i32 %2, %0, %1 : i32\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait, VM_ExtF64</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_83","title":"Operands:","text":"Operand Description <code>condition</code> 32-bit signless integer <code>true_value</code> 64-bit float <code>false_value</code> 64-bit float"},{"location":"reference/mlir-dialects/VM/#results_69","title":"Results:","text":"Result Description <code>result</code> 64-bit float"},{"location":"reference/mlir-dialects/VM/#vmselecti32-vmselecti32op","title":"<code>vm.select.i32</code> (VM::SelectI32Op)","text":"<p>Integer select operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.select.i32` operands attr-dict `:` type($result)\n</code></pre> <p>Chooses one value based on a binary condition supplied as its first operand. If the value of the condition is true the <code>true_value</code> operand is chosen, otherwise the <code>false_value</code> operand is chosen. The true and false values must have the same types. For example, the maximum operation is obtained by combining \"select\" with \"cmpi\" as follows:</p> <pre><code>%2 = vm.cmp.gt.i32.s %0, %1 : i32\n%3 = vm.select.i32 %2, %0, %1 : i32\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_84","title":"Operands:","text":"Operand Description <code>condition</code> 32-bit signless integer <code>true_value</code> 32-bit signless integer <code>false_value</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_70","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmselecti64-vmselecti64op","title":"<code>vm.select.i64</code> (VM::SelectI64Op)","text":"<p>Integer select operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.select.i64` operands attr-dict `:` type($result)\n</code></pre> <p>Chooses one value based on a binary condition supplied as its first operand. If the value of the condition is true the <code>true_value</code> operand is chosen, otherwise the <code>false_value</code> operand is chosen. The true and false values must have the same types. For example, the maximum operation is obtained by combining \"select\" with \"cmpi\" as follows:</p> <pre><code>%2 = vm.cmp.gt.i32.s %0, %1 : i32\n%3 = vm.select.i32 %2, %0, %1 : i32\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_85","title":"Operands:","text":"Operand Description <code>condition</code> 32-bit signless integer <code>true_value</code> 64-bit signless integer <code>false_value</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_71","title":"Results:","text":"Result Description <code>result</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmselectref-vmselectrefop","title":"<code>vm.select.ref</code> (VM::SelectRefOp)","text":"<p>Ref select operation <p>Syntax:</p> <pre><code>operation ::= `vm.select.ref` operands attr-dict `:` type($result)\n</code></pre> <p>Chooses one value based on a binary condition supplied as its first operand. If the value of the condition is true the <code>true_value</code> operand is chosen, otherwise the <code>false_value</code> operand is chosen.</p> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_86","title":"Operands:","text":"Operand Description <code>condition</code> 32-bit signless integer <code>true_value</code> ref <code>false_value</code> ref"},{"location":"reference/mlir-dialects/VM/#results_72","title":"Results:","text":"Result Description <code>result</code> ref"},{"location":"reference/mlir-dialects/VM/#vmswitchf32-vmswitchf32op","title":"<code>vm.switch.f32</code> (VM::SwitchF32Op)","text":"<p>Floating-point switch operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.switch.f32` $index `[` $values `]` `else` $default_value attr-dict `:` type($result)\n</code></pre> <p>Returns the value with the given <code>index</code> in <code>values</code> or <code>default_value</code> if the index is out of bounds.</p> <pre><code>// Switch %index to cases of %c100/%c200/%c300 if index==0, ==1, ==2.\n// If %index is out of range (&lt;0 or &gt;2) then default to %c5.\n%0 = vm.switch.f32 %index[%c100, %c200, %c300] else %c5 : f32\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait, VM_ExtF32</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_87","title":"Operands:","text":"Operand Description <code>index</code> 32-bit signless integer <code>default_value</code> 32-bit float <code>values</code> 32-bit float"},{"location":"reference/mlir-dialects/VM/#results_73","title":"Results:","text":"Result Description <code>result</code> 32-bit float"},{"location":"reference/mlir-dialects/VM/#vmswitchf64-vmswitchf64op","title":"<code>vm.switch.f64</code> (VM::SwitchF64Op)","text":"<p>Floating-point switch operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.switch.f64` $index `[` $values `]` `else` $default_value attr-dict `:` type($result)\n</code></pre> <p>Returns the value with the given <code>index</code> in <code>values</code> or <code>default_value</code> if the index is out of bounds.</p> <pre><code>// Switch %index to cases of %c100/%c200/%c300 if index==0, ==1, ==2.\n// If %index is out of range (&lt;0 or &gt;2) then default to %c5.\n%0 = vm.switch.f32 %index[%c100, %c200, %c300] else %c5 : f32\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait, VM_ExtF64</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_88","title":"Operands:","text":"Operand Description <code>index</code> 32-bit signless integer <code>default_value</code> 64-bit float <code>values</code> 64-bit float"},{"location":"reference/mlir-dialects/VM/#results_74","title":"Results:","text":"Result Description <code>result</code> 64-bit float"},{"location":"reference/mlir-dialects/VM/#vmswitchi32-vmswitchi32op","title":"<code>vm.switch.i32</code> (VM::SwitchI32Op)","text":"<p>Integer switch operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.switch.i32` $index `[` $values `]` `else` $default_value attr-dict `:` type($result)\n</code></pre> <p>Returns the value with the given <code>index</code> in <code>values</code> or <code>default_value</code> if the index is out of bounds.</p> <pre><code>// Switch %index to cases of %c100/%c200/%c300 if index==0, ==1, ==2.\n// If %index is out of range (&lt;0 or &gt;2) then default to %c5.\n%0 = vm.switch.i32 %index[%c100, %c200, %c300] else %c5 : i32\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_89","title":"Operands:","text":"Operand Description <code>index</code> 32-bit signless integer <code>default_value</code> 32-bit signless integer <code>values</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_75","title":"Results:","text":"Result Description <code>result</code> 32-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmswitchi64-vmswitchi64op","title":"<code>vm.switch.i64</code> (VM::SwitchI64Op)","text":"<p>Integer switch operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.switch.i64` $index `[` $values `]` `else` $default_value attr-dict `:` type($result)\n</code></pre> <p>Returns the value with the given <code>index</code> in <code>values</code> or <code>default_value</code> if the index is out of bounds.</p> <pre><code>// Switch %index to cases of %c100/%c200/%c300 if index==0, ==1, ==2.\n// If %index is out of range (&lt;0 or &gt;2) then default to %c5.\n%0 = vm.switch.i32 %index[%c100, %c200, %c300] else %c5 : i32\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_90","title":"Operands:","text":"Operand Description <code>index</code> 32-bit signless integer <code>default_value</code> 64-bit signless integer <code>values</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#results_76","title":"Results:","text":"Result Description <code>result</code> 64-bit signless integer"},{"location":"reference/mlir-dialects/VM/#vmswitchref-vmswitchrefop","title":"<code>vm.switch.ref</code> (VM::SwitchRefOp)","text":"<p>Ref switch operation <p>Returns the value with the given <code>index</code> in <code>values</code> or <code>default_value</code> if the index is out of bounds.</p> <pre><code>// Switch %arg0 to cases of %r0/%r1/%r2 if arg0==0, ==1, ==2.\n// If %arg0 is out of range (&lt;0 or &gt;2) then default to %null.\n%0 = vm.switch.ref %index[%r0, %r1, %r2] else %null : vm.ref&lt;!foo&gt;\n</code></pre> <p>Traits: AlwaysSpeculatableImplTrait</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#operands_91","title":"Operands:","text":"Operand Description <code>index</code> 32-bit signless integer <code>default_value</code> ref <code>values</code> ref"},{"location":"reference/mlir-dialects/VM/#results_77","title":"Results:","text":"Result Description <code>result</code> ref"},{"location":"reference/mlir-dialects/VM/#constant-ops","title":"Constant ops","text":""},{"location":"reference/mlir-dialects/VM/#vmconstf32-vmconstf32op","title":"<code>vm.const.f32</code> (VM::ConstF32Op)","text":"<p>32-bit floating-point constant operation</p> <p>Syntax:</p> <pre><code>operation ::= `vm.const.f32` $value attr-dict\n</code></pre> <p>Defines a constant value that is treated as a scalar literal at runtime.</p> <p>Traits: AlwaysSpeculatableImplTrait, ConstantLike, VM_ExtF32</p> <p>Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface</p> <p>Effects: MemoryEffects::Effect{}</p>"},{"location":"reference/mlir-dialects/VM/#attributes","title":"Attributes:","text":"AttributeMLIR TypeDescription <code>value</code>FloatAttr32-bit floating-point value   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.const.f32.zero` (VM::ConstF32ZeroOp)  _32-bit floating-point constant zero operation_   Syntax:  <pre><code>operation ::= `vm.const.f32.zero` attr-dict\n</code></pre>  Defines a constant zero primitive.  Traits: AlwaysSpeculatableImplTrait, ConstantLike, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.const.f64` (VM::ConstF64Op)  _64-bit floating-point constant operation_   Syntax:  <pre><code>operation ::= `vm.const.f64` $value attr-dict\n</code></pre>  Defines a constant value that is treated as a scalar literal at runtime.  Traits: AlwaysSpeculatableImplTrait, ConstantLike, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>value</code>FloatAttr64-bit floating-point value   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.const.f64.zero` (VM::ConstF64ZeroOp)  _64-bit floating-point constant zero operation_   Syntax:  <pre><code>operation ::= `vm.const.f64.zero` attr-dict\n</code></pre>  Defines a constant zero primitive.  Traits: AlwaysSpeculatableImplTrait, ConstantLike, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.const.i32` (VM::ConstI32Op)  _32-bit integer constant operation_   Syntax:  <pre><code>operation ::= `vm.const.i32` $value attr-dict\n</code></pre>  Defines a constant value that is treated as a scalar literal at runtime.  Traits: AlwaysSpeculatableImplTrait, ConstantLike  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>value</code>IntegerAttr32-bit integer value   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.const.i32.zero` (VM::ConstI32ZeroOp)  _32-bit integer constant zero operation_   Syntax:  <pre><code>operation ::= `vm.const.i32.zero` attr-dict\n</code></pre>  Defines a constant zero primitive.  Traits: AlwaysSpeculatableImplTrait, ConstantLike  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.const.i64` (VM::ConstI64Op)  _64-bit integer constant operation_   Syntax:  <pre><code>operation ::= `vm.const.i64` $value attr-dict\n</code></pre>  Defines a constant value that is treated as a scalar literal at runtime.  Traits: AlwaysSpeculatableImplTrait, ConstantLike  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>value</code>IntegerAttr64-bit integer value   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit signless integer  #### `vm.const.i64.zero` (VM::ConstI64ZeroOp)  _64-bit integer constant zero operation_   Syntax:  <pre><code>operation ::= `vm.const.i64.zero` attr-dict\n</code></pre>  Defines a constant zero primitive.  Traits: AlwaysSpeculatableImplTrait, ConstantLike  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit signless integer  #### `vm.const.ref.rodata` (VM::ConstRefRodataOp)  _Constant rodata access operation_   Syntax:  <pre><code>operation ::= `vm.const.ref.rodata` $rodata attr-dict `:` type($value)\n</code></pre>  Returns a reference to a read-only buffer.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>rodata</code>::mlir::FlatSymbolRefAttrflat symbol reference attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `value` | ref  #### `vm.const.ref.zero` (VM::ConstRefZeroOp)  _Null ref constant operation_   Syntax:  <pre><code>operation ::= `vm.const.ref.zero` `:` type($result) attr-dict\n</code></pre>  Defines a constant null ref that can be used in comparisons and initialization.  Traits: AlwaysSpeculatableImplTrait, ConstantLike  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ref  #### `vm.rodata.inline` (VM::RodataInlineOp)  _Inlined constant rodata_   Syntax:  <pre><code>operation ::= `vm.rodata.inline` ($name^)? attr-dict `:` type($result) `=` $value\n</code></pre>  vm.rodata that can be embedded inline in functions. See vm.rodata for more information.  Traits: AlwaysSpeculatableImplTrait, VM_PseudoOp  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>name</code>::mlir::StringAttrstring attribute <code>value</code>::mlir::Attributebuffer-like constant attribute values <code>alignment</code>::mlir::IntegerAttr64-bit signless integer attribute <code>mime_type</code>::mlir::StringAttrstring attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ref  #### `vm.rodata` (VM::RodataOp)  _Read-only data definition operation_   Syntax:  <pre><code>operation ::= `vm.rodata` custom&lt;SymbolVisibility&gt;($sym_visibility) $sym_name attr-dict $value\n</code></pre>  Defines a blob of read-only constant data that can be represented as a ref. This can be used to store arbitrary data within modules such as large constant buffers and other file contents.  Note that the data is reference counted as a way to track its usage once the value leaves the module. For example, returning rodata from an exported function must keep the data (possibly backed by mmap) valid for its entire lifetime.  By default all rodata will be aligned in the final module output at a 16-byte granularity. An optional alignment can be specified to override the default for cases where larger or smaller alignments are needed.  Traits: HasParent, IsolatedFromAbove  Interfaces: Symbol, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>sym_name</code>::mlir::StringAttrstring attribute <code>value</code>::mlir::Attributebuffer-like constant attribute values <code>alignment</code>::mlir::IntegerAttr64-bit signless integer attribute <code>ordinal</code>::mlir::IntegerAttrordinal value <code>mime_type</code>::mlir::StringAttrstring attribute    ### Control flow ops    #### `vm.br` (VM::BranchOp)  _Unconditional branch operation_   Syntax:  <pre><code>operation ::= `vm.br` $dest (`(` $destOperands^ `:` type($destOperands) `)`)? attr-dict\n</code></pre>   Represents an unconditional branch operation that branches to a target block  with the given set of arguments.   ```  ^bb0(...):    vm.br ^bb1(%a)  ^bb1(%blockArg1):    ... ```  Traits: Terminator  Interfaces: BranchOpInterface, VMSerializableOp, VM_OpInterface  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `destOperands` | 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float or 32-bit signless integer or ref  ##### Successors:  | Successor | Description | | :-------: | ----------- | | `dest` | any successor  #### `vm.call` (VM::CallOp)  _Call operation_   Syntax:  <pre><code>operation ::= `vm.call` $callee `(` operands `)` attr-dict `:` functional-type(operands, results)\n</code></pre>  Calls an internal VM function with the given arguments.  Interfaces: CallOpInterface, MemoryEffectOpInterface, VMSerializableOp, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>callee</code>FlatSymbolRefAttrsymbol reference attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operands` | 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float or 32-bit signless integer or ref  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float or 32-bit signless integer or ref  #### `vm.call.variadic` (VM::CallVariadicOp)  _Call operation with variadic arguments_  Calls an internal VM function with the given arguments. One or more of the arguments may be variadic, encoded as segmented sized operand lists.  Variadic arguments must be specified with a total count in the segment_sizes attribute.  Interfaces: CallOpInterface, MemoryEffectOpInterface, VMSerializableOp, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>callee</code>FlatSymbolRefAttrsymbol reference attribute <code>segment_sizes</code>::mlir::DenseIntElementsAttr16-bit signless integer elements attribute <code>segment_types</code>::mlir::ArrayAttrtype array attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operands` | 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float or 32-bit signless integer or ref  ##### Results:  | Result | Description | | :----: | ----------- | | `results` | 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float or 32-bit signless integer or ref  #### `vm.check.eq` (VM::CheckEQOp)  _Raises a global failure if the condition is true_   Syntax:  <pre><code>operation ::= `vm.check.eq` $lhs `,` $rhs (`,` $message^)? attr-dict `:` type($lhs)\n</code></pre>  When the condition is true this signals a runtime failure that causes the entire active invocation - and possibly *all* in-flight and pending invocations - to fail. The status will be propagated back via the available runtime error handling mechanisms such as semaphores or synchronous invocation results.  This is implemented as a pseudo-op that transforms into a vm.cond_fail operation.  <pre><code>vm.check.eq %a, %b, \"a == b\" : i32\nvm.check.nz %ref, \"!null\" : !vm.ref&lt;?&gt;\n</code></pre>  Traits: Commutative, VM_PseudoOp  Interfaces: VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>message</code>::mlir::StringAttrstring attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float or 32-bit signless integer or ref | `rhs` | 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float or 32-bit signless integer or ref  #### `vm.check.ne` (VM::CheckNEOp)  _Raises a global failure if the condition is true_   Syntax:  <pre><code>operation ::= `vm.check.ne` $lhs `,` $rhs (`,` $message^)? attr-dict `:` type($lhs)\n</code></pre>  When the condition is true this signals a runtime failure that causes the entire active invocation - and possibly *all* in-flight and pending invocations - to fail. The status will be propagated back via the available runtime error handling mechanisms such as semaphores or synchronous invocation results.  This is implemented as a pseudo-op that transforms into a vm.cond_fail operation.  <pre><code>vm.check.eq %a, %b, \"a == b\" : i32\nvm.check.nz %ref, \"!null\" : !vm.ref&lt;?&gt;\n</code></pre>  Traits: Commutative, VM_PseudoOp  Interfaces: VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>message</code>::mlir::StringAttrstring attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float or 32-bit signless integer or ref | `rhs` | 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float or 32-bit signless integer or ref  #### `vm.check.nz` (VM::CheckNZOp)  _Raises a global failure if the condition is true_   Syntax:  <pre><code>operation ::= `vm.check.nz` $value (`,` $message^)? attr-dict `:` type($value)\n</code></pre>  When the condition is true this signals a runtime failure that causes the entire active invocation - and possibly *all* in-flight and pending invocations - to fail. The status will be propagated back via the available runtime error handling mechanisms such as semaphores or synchronous invocation results.  This is implemented as a pseudo-op that transforms into a vm.cond_fail operation.  <pre><code>vm.check.eq %a, %b, \"a == b\" : i32\nvm.check.nz %ref, \"!null\" : !vm.ref&lt;?&gt;\n</code></pre>  Traits: VM_PseudoOp  Interfaces: VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>message</code>::mlir::StringAttrstring attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float or 32-bit signless integer or ref  #### `vm.check.nearly_eq` (VM::CheckNearlyEQOp)  _Raises a global failure if the condition is true_   Syntax:  <pre><code>operation ::= `vm.check.nearly_eq` $lhs `,` $rhs (`,` $message^)? attr-dict `:` type($lhs)\n</code></pre>  When the condition is true this signals a runtime failure that causes the entire active invocation - and possibly *all* in-flight and pending invocations - to fail. The status will be propagated back via the available runtime error handling mechanisms such as semaphores or synchronous invocation results.  This is implemented as a pseudo-op that transforms into a vm.cond_fail operation.  <pre><code>vm.check.eq %a, %b, \"a == b\" : i32\nvm.check.nz %ref, \"!null\" : !vm.ref&lt;?&gt;\n</code></pre>  Traits: Commutative, VM_PseudoOp  Interfaces: VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>message</code>::mlir::StringAttrstring attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float or 32-bit signless integer or ref | `rhs` | 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float or 32-bit signless integer or ref  #### `vm.cond_br` (VM::CondBranchOp)  _Conditional branch operation_   Syntax:  <pre><code>operation ::= `vm.cond_br` $condition `,`\n              $trueDest (`(` $trueDestOperands^ `:` type($trueDestOperands) `)`)? `,`\n              $falseDest (`(` $falseDestOperands^ `:` type($falseDestOperands) `)`)?\n              attr-dict\n</code></pre>   Represents a conditional branch operation that branches to one of the two  target blocks with the given set of arguments.   ```  ^bb0(...):    vm.cond_br %condition, ^bb1(%a), ^bb2(%b)  ^bb1(%blockArg1):    ...  ^bb2(%blockArg2):    ... ```  Traits: AttrSizedOperandSegments, Terminator  Interfaces: BranchOpInterface, VMSerializableOp, VM_OpInterface  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `condition` | 32-bit signless integer | `trueDestOperands` | 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float or 32-bit signless integer or ref | `falseDestOperands` | 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float or 32-bit signless integer or ref  ##### Successors:  | Successor | Description | | :-------: | ----------- | | `trueDest` | any successor | `falseDest` | any successor  #### `vm.cond_fail` (VM::CondFailOp)  _Raises a global failure if the condition is true_  When the condition is true this signals a runtime failure that causes the entire active invocation - and possibly *all* in-flight and pending invocations - to fail with the given status. The status will be propagated back via the available runtime error handling mechanisms such as semaphores or synchronous invocation results.  As the IREE execution model is deeply pipelined it's possible that failures have a latency between when they are emitted and when the application can observe the failure. It's also possible that other work that is in-flight or pending when the failure occurs will complete.  This is implemented as a pseudo-op that transforms into a vm.fail operation guarded by the condition.  <pre><code>%nz = vm.cmp.nz.i32 %value : i32\n%statusCode = vm.const.i32 9\nvm.cond_fail %nz, %statusCode, \"expected non-zero\"\n</code></pre>  Traits: VM_PseudoOp  Interfaces: VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>message</code>::mlir::StringAttrstring attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `condition` | 32-bit signless integer | `status` | 32-bit signless integer  #### `vm.fail` (VM::FailOp)  _Raises a global failure_   Syntax:  <pre><code>operation ::= `vm.fail` $status (`,` $message^)? attr-dict\n</code></pre>  Signals a runtime failure that causes the entire active invocation - and possibly *all* in-flight and pending invocations - to fail with the given status. The status will be propagated back via the available runtime error handling mechanisms such as semaphores or synchronous invocation results.  As the IREE execution model is deeply pipelined it's possible that failures have a latency between when they are emitted and when the application can observe the failure. It's also possible that other work that is in-flight or pending when the failure occurs will complete.  <pre><code>%statusCode = vm.const.i32 9\nvm.fail %statusCode, \"oh no!\"\n</code></pre>  Traits: Terminator  Interfaces: VMSerializableOp, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>message</code>::mlir::StringAttrstring attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `status` | 32-bit signless integer  #### `vm.import.resolved` (VM::ImportResolvedOp)  _Returns true if an optional import was resolved at runtime_   Syntax:  <pre><code>operation ::= `vm.import.resolved` $import attr-dict `:` type($result)\n</code></pre>  Allows for checking whether a optional import was resolved at runtime. If this returns false then attempting to call the imported function will result in a failure at runtime.  Interfaces: NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>import</code>FlatSymbolRefAttrsymbol reference attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.return` (VM::ReturnOp)  _Return operation_   Syntax:  <pre><code>operation ::= `vm.return` attr-dict ($operands^ `:` type($operands))?\n</code></pre>  Represents a return operation within a function.  <pre><code>vm.func @foo(%0: i32, %1: f8) -&gt; (i32, f8) {\n  vm.return %0, %1 : i32, f8\n}\n</code></pre>  Traits: AlwaysSpeculatableImplTrait, ReturnLike, Terminator  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), RegionBranchTerminatorOpInterface, VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operands` | 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float or 32-bit signless integer or ref   ### Debugging ops    #### `vm.break` (VM::BreakOp)  _Unconditional debug break operation_   Syntax:  <pre><code>operation ::= `vm.break` $dest (`(` $destOperands^ `:` type($destOperands) `)`)? attr-dict\n</code></pre>  Breaks into the attached debugger or asks for attaching a debugger. After resuming (or if a debugger is not attached) execution will continue at the target block.  Traits: Terminator, Util_YieldPoint, VM_DebugOnly, VM_FullBarrier  Interfaces: BranchOpInterface, VMSerializableOp, VM_OpInterface  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `destOperands` | 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float or 32-bit signless integer or ref  ##### Successors:  | Successor | Description | | :-------: | ----------- | | `dest` | any successor  #### `vm.cond_break` (VM::CondBreakOp)  _Conditional debug break operation_   Syntax:  <pre><code>operation ::= `vm.cond_break` $condition `,` $dest (`(` $destOperands^ `:` type($destOperands) `)`)?\n              attr-dict\n</code></pre>  Breaks into the attached debugger or asks for attaching a debugger if the provided condition is true. After resuming (or if a debugger is not attached) execution will continue at the target block.  Traits: Terminator, Util_YieldPoint, VM_DebugOnly, VM_FullBarrier  Interfaces: BranchOpInterface, VMSerializableOp, VM_OpInterface  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `condition` | 32-bit signless integer | `destOperands` | 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float or 32-bit signless integer or ref  ##### Successors:  | Successor | Description | | :-------: | ----------- | | `dest` | any successor  #### `vm.print` (VM::PrintOp)  _Message printing operation_   Syntax:  <pre><code>operation ::= `vm.print` $message `(` operands `)` attr-dict `:` type(operands)\n</code></pre>  Prints the given string message and zero or more values.  Traits: VM_DebugOnly, VM_FullBarrier  Interfaces: VMSerializableOp, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>message</code>::mlir::StringAttrstring attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operands` | 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float or 32-bit signless integer or ref  #### `vm.trace` (VM::TraceOp)  _Trace value(s) operation_   Syntax:  <pre><code>operation ::= `vm.trace` $event_name `(` operands `)` attr-dict `:` type(operands)\n</code></pre>  Traces one or more values at the time the operation is executed. These values will be encoded into the active trace depending on the active trace verbosity setting.  Traits: VM_DebugOnly, VM_FullBarrier  Interfaces: VMSerializableOp, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>event_name</code>::mlir::StringAttrstring attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operands` | 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float or 32-bit signless integer or ref   ### Floating-point arithmetic ops    #### `vm.abs.f32` (VM::AbsF32Op)  _Floating point absolute-value operation_   Syntax:  <pre><code>operation ::= `vm.abs.f32` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.abs.f64` (VM::AbsF64Op)  _Floating point absolute-value operation_   Syntax:  <pre><code>operation ::= `vm.abs.f64` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.add.f32` (VM::AddF32Op)  _Floating-point add operation_   Syntax:  <pre><code>operation ::= `vm.add.f32` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, Commutative, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit float | `rhs` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.add.f64` (VM::AddF64Op)  _Floating-point add operation_   Syntax:  <pre><code>operation ::= `vm.add.f64` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, Commutative, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit float | `rhs` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.ceil.f32` (VM::CeilF32Op)  _Floating point ceiling operation_   Syntax:  <pre><code>operation ::= `vm.ceil.f32` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.ceil.f64` (VM::CeilF64Op)  _Floating point ceiling operation_   Syntax:  <pre><code>operation ::= `vm.ceil.f64` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.div.f32` (VM::DivF32Op)  _Floating point division operation_   Syntax:  <pre><code>operation ::= `vm.div.f32` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit float | `rhs` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.div.f64` (VM::DivF64Op)  _Floating point division operation_   Syntax:  <pre><code>operation ::= `vm.div.f64` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit float | `rhs` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.fma.f32` (VM::FMAF32Op)  _Floating point fused multiply-add operation (a*b+c)_   Syntax:  <pre><code>operation ::= `vm.fma.f32` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `a` | 32-bit float | `b` | 32-bit float | `c` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.fma.f64` (VM::FMAF64Op)  _Floating point fused multiply-add operation (a*b+c)_   Syntax:  <pre><code>operation ::= `vm.fma.f64` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `a` | 64-bit float | `b` | 64-bit float | `c` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.floor.f32` (VM::FloorF32Op)  _Floating point floor operation_   Syntax:  <pre><code>operation ::= `vm.floor.f32` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.floor.f64` (VM::FloorF64Op)  _Floating point floor operation_   Syntax:  <pre><code>operation ::= `vm.floor.f64` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.max.f32` (VM::MaxF32Op)  _Floating point maximum operation_   Syntax:  <pre><code>operation ::= `vm.max.f32` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, Commutative, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit float | `rhs` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.max.f64` (VM::MaxF64Op)  _Floating point maximum operation_   Syntax:  <pre><code>operation ::= `vm.max.f64` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, Commutative, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit float | `rhs` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.min.f32` (VM::MinF32Op)  _Floating point minimum operation_   Syntax:  <pre><code>operation ::= `vm.min.f32` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, Commutative, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit float | `rhs` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.min.f64` (VM::MinF64Op)  _Floating point minimum operation_   Syntax:  <pre><code>operation ::= `vm.min.f64` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, Commutative, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit float | `rhs` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.mul.f32` (VM::MulF32Op)  _Floating point multiplication operation_   Syntax:  <pre><code>operation ::= `vm.mul.f32` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, Commutative, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit float | `rhs` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.mul.f64` (VM::MulF64Op)  _Floating point multiplication operation_   Syntax:  <pre><code>operation ::= `vm.mul.f64` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, Commutative, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit float | `rhs` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.neg.f32` (VM::NegF32Op)  _Floating point negation operation_   Syntax:  <pre><code>operation ::= `vm.neg.f32` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.neg.f64` (VM::NegF64Op)  _Floating point negation operation_   Syntax:  <pre><code>operation ::= `vm.neg.f64` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.rem.f32` (VM::RemF32Op)  _Floating point remainder operation_   Syntax:  <pre><code>operation ::= `vm.rem.f32` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit float | `rhs` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.rem.f64` (VM::RemF64Op)  _Floating point remainder operation_   Syntax:  <pre><code>operation ::= `vm.rem.f64` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit float | `rhs` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.round.f32.even` (VM::RoundF32EvenOp)  _Rounds the value to the nearest even integer_   Syntax:  <pre><code>operation ::= `vm.round.f32.even` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.round.f32` (VM::RoundF32Op)  _Rounds the value to the nearest integer away from zero_   Syntax:  <pre><code>operation ::= `vm.round.f32` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.round.f64.even` (VM::RoundF64EvenOp)  _Rounds the value to the nearest even integer_   Syntax:  <pre><code>operation ::= `vm.round.f64.even` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.round.f64` (VM::RoundF64Op)  _Rounds the value to the nearest integer away from zero_   Syntax:  <pre><code>operation ::= `vm.round.f64` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.sub.f32` (VM::SubF32Op)  _Floating point subtraction operation_   Syntax:  <pre><code>operation ::= `vm.sub.f32` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit float | `rhs` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.sub.f64` (VM::SubF64Op)  _Floating point subtraction operation_   Syntax:  <pre><code>operation ::= `vm.sub.f64` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit float | `rhs` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float   ### Floating-point comparison ops    #### `vm.cmp.eq.f32.near` (VM::CmpEQF32NearOp)  _Near floating-point equality comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.eq.f32.near` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, Commutative, VM_ExtF32, VM_PseudoOp  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit float | `rhs` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.eq.f32.o` (VM::CmpEQF32OOp)  _Ordered floating-point equality comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.eq.f32.o` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, Commutative, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit float | `rhs` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.eq.f32.u` (VM::CmpEQF32UOp)  _Unordered floating-point equality comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.eq.f32.u` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, Commutative, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit float | `rhs` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.eq.f64.near` (VM::CmpEQF64NearOp)  _Near floating-point equality comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.eq.f64.near` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, Commutative, VM_ExtF64, VM_PseudoOp  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit float | `rhs` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.eq.f64.o` (VM::CmpEQF64OOp)  _Ordered floating-point equality comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.eq.f64.o` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, Commutative, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit float | `rhs` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.eq.f64.u` (VM::CmpEQF64UOp)  _Unordered floating-point equality comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.eq.f64.u` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, Commutative, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit float | `rhs` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.gte.f32.o` (VM::CmpGTEF32OOp)  _Ordered floating-point greater-than-or-equal comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.gte.f32.o` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, VM_ExtF32, VM_PseudoOp  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit float | `rhs` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.gte.f32.u` (VM::CmpGTEF32UOp)  _Unordered floating-point greater-than-or-equal comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.gte.f32.u` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, VM_ExtF32, VM_PseudoOp  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit float | `rhs` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.gte.f64.o` (VM::CmpGTEF64OOp)  _Ordered floating-point greater-than-or-equal comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.gte.f64.o` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, VM_ExtF64, VM_PseudoOp  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit float | `rhs` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.gte.f64.u` (VM::CmpGTEF64UOp)  _Unordered floating-point greater-than-or-equal comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.gte.f64.u` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, VM_ExtF64, VM_PseudoOp  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit float | `rhs` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.gt.f32.o` (VM::CmpGTF32OOp)  _Ordered floating-point greater-than comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.gt.f32.o` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, VM_ExtF32, VM_PseudoOp  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit float | `rhs` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.gt.f32.u` (VM::CmpGTF32UOp)  _Unordered floating-point greater-than comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.gt.f32.u` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, VM_ExtF32, VM_PseudoOp  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit float | `rhs` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.gt.f64.o` (VM::CmpGTF64OOp)  _Ordered floating-point greater-than comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.gt.f64.o` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, VM_ExtF64, VM_PseudoOp  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit float | `rhs` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.gt.f64.u` (VM::CmpGTF64UOp)  _Unordered floating-point greater-than comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.gt.f64.u` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, VM_ExtF64, VM_PseudoOp  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit float | `rhs` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.lte.f32.o` (VM::CmpLTEF32OOp)  _Ordered floating-point less-than-or-equal comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.lte.f32.o` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit float | `rhs` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.lte.f32.u` (VM::CmpLTEF32UOp)  _Unordered floating-point less-than-or-equal comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.lte.f32.u` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit float | `rhs` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.lte.f64.o` (VM::CmpLTEF64OOp)  _Ordered floating-point less-than-or-equal comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.lte.f64.o` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit float | `rhs` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.lte.f64.u` (VM::CmpLTEF64UOp)  _Unordered floating-point less-than-or-equal comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.lte.f64.u` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit float | `rhs` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.lt.f32.o` (VM::CmpLTF32OOp)  _Ordered floating-point less-than comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.lt.f32.o` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit float | `rhs` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.lt.f32.u` (VM::CmpLTF32UOp)  _Unordered floating-point less-than comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.lt.f32.u` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit float | `rhs` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.lt.f64.o` (VM::CmpLTF64OOp)  _Ordered floating-point less-than comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.lt.f64.o` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit float | `rhs` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.lt.f64.u` (VM::CmpLTF64UOp)  _Unordered floating-point less-than comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.lt.f64.u` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit float | `rhs` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.ne.f32.o` (VM::CmpNEF32OOp)  _Ordered floating-point inequality comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.ne.f32.o` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, Commutative, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit float | `rhs` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.ne.f32.u` (VM::CmpNEF32UOp)  _Unordered floating-point inequality comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.ne.f32.u` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, Commutative, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit float | `rhs` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.ne.f64.o` (VM::CmpNEF64OOp)  _Ordered floating-point inequality comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.ne.f64.o` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, Commutative, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit float | `rhs` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.ne.f64.u` (VM::CmpNEF64UOp)  _Unordered floating-point inequality comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.ne.f64.u` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, Commutative, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit float | `rhs` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.nz.f32.o` (VM::CmpNZF32OOp)  _Ordered floating-point non-zero comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.nz.f32.o` operands attr-dict `:` type($operand)\n</code></pre>  Compares the given floating-point operand for a non-zero value.  Traits: AlwaysSpeculatableImplTrait, VM_ExtF32, VM_PseudoOp  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.nz.f32.u` (VM::CmpNZF32UOp)  _Unordered floating-point non-zero comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.nz.f32.u` operands attr-dict `:` type($operand)\n</code></pre>  Compares the given floating-point operand for a non-zero value.  Traits: AlwaysSpeculatableImplTrait, VM_ExtF32, VM_PseudoOp  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.nz.f64.o` (VM::CmpNZF64OOp)  _Ordered floating-point non-zero comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.nz.f64.o` operands attr-dict `:` type($operand)\n</code></pre>  Compares the given floating-point operand for a non-zero value.  Traits: AlwaysSpeculatableImplTrait, VM_ExtF64, VM_PseudoOp  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.nz.f64.u` (VM::CmpNZF64UOp)  _Unordered floating-point non-zero comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.nz.f64.u` operands attr-dict `:` type($operand)\n</code></pre>  Compares the given floating-point operand for a non-zero value.  Traits: AlwaysSpeculatableImplTrait, VM_ExtF64, VM_PseudoOp  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.nan.f32` (VM::CmpNaNF32Op)  _Floating-point NaN comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.nan.f32` $operand attr-dict `:` type($operand)\n</code></pre>  Returns 1 if the value is NaN.  Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.nan.f64` (VM::CmpNaNF64Op)  _Floating-point NaN comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.nan.f64` $operand attr-dict `:` type($operand)\n</code></pre>  Returns 1 if the value is NaN.  Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer   ### Floating-point math ops  These map directly to the `math` dialect.  #### `vm.atan2.f32` (VM::Atan2F32Op)  _2-argument arcus tangent of the given values_   Syntax:  <pre><code>operation ::= `vm.atan2.f32` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit float | `rhs` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.atan2.f64` (VM::Atan2F64Op)  _2-argument arcus tangent of the given values_   Syntax:  <pre><code>operation ::= `vm.atan2.f64` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit float | `rhs` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.atan.f32` (VM::AtanF32Op)  _Arcus tangent of the given value_   Syntax:  <pre><code>operation ::= `vm.atan.f32` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.atan.f64` (VM::AtanF64Op)  _Arcus tangent of the given value_   Syntax:  <pre><code>operation ::= `vm.atan.f64` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.cos.f32` (VM::CosF32Op)  _Cosine of the specified value_   Syntax:  <pre><code>operation ::= `vm.cos.f32` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.cos.f64` (VM::CosF64Op)  _Cosine of the specified value_   Syntax:  <pre><code>operation ::= `vm.cos.f64` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.erf.f32` (VM::ErfF32Op)  _Computes the error function of the specified value_   Syntax:  <pre><code>operation ::= `vm.erf.f32` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.erf.f64` (VM::ErfF64Op)  _Computes the error function of the specified value_   Syntax:  <pre><code>operation ::= `vm.erf.f64` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.exp2.f32` (VM::Exp2F32Op)  _Base-2 exponential of the specified value_   Syntax:  <pre><code>operation ::= `vm.exp2.f32` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.exp2.f64` (VM::Exp2F64Op)  _Base-2 exponential of the specified value_   Syntax:  <pre><code>operation ::= `vm.exp2.f64` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.exp.f32` (VM::ExpF32Op)  _Base-e exponential of the specified value_   Syntax:  <pre><code>operation ::= `vm.exp.f32` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.exp.f64` (VM::ExpF64Op)  _Base-e exponential of the specified value_   Syntax:  <pre><code>operation ::= `vm.exp.f64` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.expm1.f32` (VM::ExpM1F32Op)  _Base-e exponential of the specified value minus 1_   Syntax:  <pre><code>operation ::= `vm.expm1.f32` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.expm1.f64` (VM::ExpM1F64Op)  _Base-e exponential of the specified value minus 1_   Syntax:  <pre><code>operation ::= `vm.expm1.f64` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.log10.f32` (VM::Log10F32Op)  _Base-10 logarithm of the specified value_   Syntax:  <pre><code>operation ::= `vm.log10.f32` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.log10.f64` (VM::Log10F64Op)  _Base-10 logarithm of the specified value_   Syntax:  <pre><code>operation ::= `vm.log10.f64` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.log1p.f32` (VM::Log1pF32Op)  _Natural logarithm of one plus the given value_   Syntax:  <pre><code>operation ::= `vm.log1p.f32` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.log1p.f64` (VM::Log1pF64Op)  _Natural logarithm of one plus the given value_   Syntax:  <pre><code>operation ::= `vm.log1p.f64` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.log2.f32` (VM::Log2F32Op)  _Base-2 logarithm of the specified value_   Syntax:  <pre><code>operation ::= `vm.log2.f32` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.log2.f64` (VM::Log2F64Op)  _Base-2 logarithm of the specified value_   Syntax:  <pre><code>operation ::= `vm.log2.f64` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.log.f32` (VM::LogF32Op)  _Base-e logarithm of the specified value_   Syntax:  <pre><code>operation ::= `vm.log.f32` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.log.f64` (VM::LogF64Op)  _Base-e logarithm of the specified value_   Syntax:  <pre><code>operation ::= `vm.log.f64` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.pow.f32` (VM::PowF32Op)  _Floating point raised to the power of operation_   Syntax:  <pre><code>operation ::= `vm.pow.f32` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit float | `rhs` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.pow.f64` (VM::PowF64Op)  _Floating point raised to the power of operation_   Syntax:  <pre><code>operation ::= `vm.pow.f64` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit float | `rhs` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.rsqrt.f32` (VM::RsqrtF32Op)  _Reciprocal of sqrt (1 / sqrt of the specified value)_   Syntax:  <pre><code>operation ::= `vm.rsqrt.f32` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.rsqrt.f64` (VM::RsqrtF64Op)  _Reciprocal of sqrt (1 / sqrt of the specified value)_   Syntax:  <pre><code>operation ::= `vm.rsqrt.f64` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.sin.f32` (VM::SinF32Op)  _Sine of the specified value_   Syntax:  <pre><code>operation ::= `vm.sin.f32` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.sin.f64` (VM::SinF64Op)  _Sine of the specified value_   Syntax:  <pre><code>operation ::= `vm.sin.f64` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.sqrt.f32` (VM::SqrtF32Op)  _Sqrt of the specified value_   Syntax:  <pre><code>operation ::= `vm.sqrt.f32` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.sqrt.f64` (VM::SqrtF64Op)  _Sqrt of the specified value_   Syntax:  <pre><code>operation ::= `vm.sqrt.f64` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.tanh.f32` (VM::TanhF32Op)  _Hyperbolic tangent of the specified value_   Syntax:  <pre><code>operation ::= `vm.tanh.f32` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF32  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.tanh.f64` (VM::TanhF64Op)  _Hyperbolic tangent of the specified value_   Syntax:  <pre><code>operation ::= `vm.tanh.f64` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, VM_ExtF64  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit float  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float   ### Global ops    #### `vm.global.address` (VM::GlobalAddressOp)  _Returns an address reference to a global_   Syntax:  <pre><code>operation ::= `vm.global.address` $global attr-dict `:` type($result)\n</code></pre>  Returns an indirect address reference to the given global. During export the address will be converted to the natural format of the global table (for example, ordinals for refs and byte offsets for primitive types).  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), SymbolUserOpInterface, Util_GlobalAddressOpInterface, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>global</code>FlatSymbolRefAttrsymbol reference attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer or a pointer-like reference  #### `vm.global.f32` (VM::GlobalF32Op)  _32-bit floating-point global declaration_   Syntax:  <pre><code>operation ::= `vm.global.f32` custom&lt;SymbolVisibility&gt;($sym_visibility)\n              (`mutable` $is_mutable^)?\n              $sym_name\n              attr-dict\n              custom&lt;TypeOrAttr&gt;($type, $initial_value)\n</code></pre>  Defines a global value that is treated as a scalar literal at runtime. Initialized to zero unless an initial value is specified.  Traits: HasParent, IsolatedFromAbove, VM_ExtF32  Interfaces: Symbol, Util_GlobalOpInterface, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>sym_name</code>::mlir::StringAttrstring attribute <code>type</code>::mlir::TypeAttrany type attribute <code>is_mutable</code>::mlir::UnitAttrunit attribute <code>initial_value</code>FloatAttr32-bit floating-point value <code>ordinal</code>::mlir::IntegerAttrordinal value   #### `vm.global.f64` (VM::GlobalF64Op)  _64-bit floating-point global declaration_   Syntax:  <pre><code>operation ::= `vm.global.f64` custom&lt;SymbolVisibility&gt;($sym_visibility)\n              (`mutable` $is_mutable^)?\n              $sym_name\n              attr-dict\n              custom&lt;TypeOrAttr&gt;($type, $initial_value)\n</code></pre>  Defines a global value that is treated as a scalar literal at runtime. Initialized to zero unless an initial value is specified.  Traits: HasParent, IsolatedFromAbove, VM_ExtF64  Interfaces: Symbol, Util_GlobalOpInterface, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>sym_name</code>::mlir::StringAttrstring attribute <code>type</code>::mlir::TypeAttrany type attribute <code>is_mutable</code>::mlir::UnitAttrunit attribute <code>initial_value</code>FloatAttr64-bit floating-point value <code>ordinal</code>::mlir::IntegerAttrordinal value   #### `vm.global.i32` (VM::GlobalI32Op)  _32-bit integer global declaration_   Syntax:  <pre><code>operation ::= `vm.global.i32` custom&lt;SymbolVisibility&gt;($sym_visibility)\n              (`mutable` $is_mutable^)?\n              $sym_name\n              attr-dict\n              custom&lt;TypeOrAttr&gt;($type, $initial_value)\n</code></pre>  Defines a global value that is treated as a scalar literal at runtime. Initialized to zero unless an initial value is specified.  Traits: HasParent, IsolatedFromAbove  Interfaces: Symbol, Util_GlobalOpInterface, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>sym_name</code>::mlir::StringAttrstring attribute <code>type</code>::mlir::TypeAttrany type attribute <code>is_mutable</code>::mlir::UnitAttrunit attribute <code>initial_value</code>IntegerAttr32-bit integer value <code>ordinal</code>::mlir::IntegerAttrordinal value   #### `vm.global.i64` (VM::GlobalI64Op)  _64-bit integer global declaration_   Syntax:  <pre><code>operation ::= `vm.global.i64` custom&lt;SymbolVisibility&gt;($sym_visibility)\n              (`mutable` $is_mutable^)?\n              $sym_name\n              attr-dict\n              custom&lt;TypeOrAttr&gt;($type, $initial_value)\n</code></pre>  Defines a global value that is treated as a scalar literal at runtime. Initialized to zero unless an initial value is specified.  Traits: HasParent, IsolatedFromAbove  Interfaces: Symbol, Util_GlobalOpInterface, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>sym_name</code>::mlir::StringAttrstring attribute <code>type</code>::mlir::TypeAttrany type attribute <code>is_mutable</code>::mlir::UnitAttrunit attribute <code>initial_value</code>IntegerAttr64-bit integer value <code>ordinal</code>::mlir::IntegerAttrordinal value   #### `vm.global.load.f32` (VM::GlobalLoadF32Op)  _Global 32-bit floating-point load operation_   Syntax:  <pre><code>operation ::= `vm.global.load.f32` $global attr-dict `:` type($value)\n</code></pre>  Loads the value of a global containing an primitive value.  Traits: VM_ExtF32  Interfaces: MemoryEffectOpInterface, OpAsmOpInterface, SymbolUserOpInterface, Util_GlobalLoadOpInterface, VMSerializableOp, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>global</code>FlatSymbolRefAttrsymbol reference attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `value` | 32-bit float  #### `vm.global.load.f64` (VM::GlobalLoadF64Op)  _Global 64-bit floating-point load operation_   Syntax:  <pre><code>operation ::= `vm.global.load.f64` $global attr-dict `:` type($value)\n</code></pre>  Loads the value of a global containing an primitive value.  Traits: VM_ExtF64  Interfaces: MemoryEffectOpInterface, OpAsmOpInterface, SymbolUserOpInterface, Util_GlobalLoadOpInterface, VMSerializableOp, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>global</code>FlatSymbolRefAttrsymbol reference attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `value` | 64-bit float  #### `vm.global.load.i32` (VM::GlobalLoadI32Op)  _Global 32-bit integer load operation_   Syntax:  <pre><code>operation ::= `vm.global.load.i32` $global attr-dict `:` type($value)\n</code></pre>  Loads the value of a global containing an primitive value.  Interfaces: MemoryEffectOpInterface, OpAsmOpInterface, SymbolUserOpInterface, Util_GlobalLoadOpInterface, VMSerializableOp, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>global</code>FlatSymbolRefAttrsymbol reference attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `value` | 32-bit signless integer  #### `vm.global.load.i64` (VM::GlobalLoadI64Op)  _Global 64-bit integer load operation_   Syntax:  <pre><code>operation ::= `vm.global.load.i64` $global attr-dict `:` type($value)\n</code></pre>  Loads the value of a global containing an primitive value.  Interfaces: MemoryEffectOpInterface, OpAsmOpInterface, SymbolUserOpInterface, Util_GlobalLoadOpInterface, VMSerializableOp, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>global</code>FlatSymbolRefAttrsymbol reference attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `value` | 64-bit signless integer  #### `vm.global.load.indirect.f32` (VM::GlobalLoadIndirectF32Op)  _Global 32-bit floating-point load operation_   Syntax:  <pre><code>operation ::= `vm.global.load.indirect.f32` $global attr-dict `:` type($global) `-&gt;` type($value)\n</code></pre>  Loads the value of a global containing a primitive value.  Traits: VM_ExtF64  Interfaces: Util_GlobalLoadIndirectOpInterface, VMSerializableOp, VM_OpInterface  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `global` | 32-bit signless integer or ptr&lt;32-bit float&gt;  ##### Results:  | Result | Description | | :----: | ----------- | | `value` | 32-bit float  #### `vm.global.load.indirect.f64` (VM::GlobalLoadIndirectF64Op)  _Global 64-bit floating-point load operation_   Syntax:  <pre><code>operation ::= `vm.global.load.indirect.f64` $global attr-dict `:` type($global) `-&gt;` type($value)\n</code></pre>  Loads the value of a global containing a primitive value.  Traits: VM_ExtF64  Interfaces: Util_GlobalLoadIndirectOpInterface, VMSerializableOp, VM_OpInterface  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `global` | 32-bit signless integer or ptr&lt;64-bit float&gt;  ##### Results:  | Result | Description | | :----: | ----------- | | `value` | 64-bit float  #### `vm.global.load.indirect.i32` (VM::GlobalLoadIndirectI32Op)  _Global 32-bit integer load operation_   Syntax:  <pre><code>operation ::= `vm.global.load.indirect.i32` $global attr-dict `:` type($global) `-&gt;` type($value)\n</code></pre>  Loads the value of a global containing a primitive value.  Interfaces: Util_GlobalLoadIndirectOpInterface, VMSerializableOp, VM_OpInterface  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `global` | 32-bit signless integer or ptr&lt;32-bit signless integer&gt;  ##### Results:  | Result | Description | | :----: | ----------- | | `value` | 32-bit signless integer  #### `vm.global.load.indirect.i64` (VM::GlobalLoadIndirectI64Op)  _Global 64-bit integer load operation_   Syntax:  <pre><code>operation ::= `vm.global.load.indirect.i64` $global attr-dict `:` type($global) `-&gt;` type($value)\n</code></pre>  Loads the value of a global containing a primitive value.  Interfaces: Util_GlobalLoadIndirectOpInterface, VMSerializableOp, VM_OpInterface  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `global` | 32-bit signless integer or ptr&lt;64-bit signless integer&gt;  ##### Results:  | Result | Description | | :----: | ----------- | | `value` | 64-bit signless integer  #### `vm.global.load.indirect.ref` (VM::GlobalLoadIndirectRefOp)  _Global ref load operation_   Syntax:  <pre><code>operation ::= `vm.global.load.indirect.ref` $global attr-dict `:` type($global) `-&gt;` type($value)\n</code></pre>  Loads the value of a global containing a ref of the given type.  Interfaces: Util_GlobalLoadIndirectOpInterface, VMSerializableOp, VM_OpInterface  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `global` | 32-bit signless integer or ptr  ##### Results:  | Result | Description | | :----: | ----------- | | `value` | ref  #### `vm.global.load.ref` (VM::GlobalLoadRefOp)  _Global ref load operation_   Syntax:  <pre><code>operation ::= `vm.global.load.ref` $global attr-dict `:` type($value)\n</code></pre>  Loads the value of a global containing a ref of the given type.  Interfaces: MemoryEffectOpInterface, OpAsmOpInterface, SymbolUserOpInterface, Util_GlobalLoadOpInterface, VMSerializableOp, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>global</code>FlatSymbolRefAttrsymbol reference attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `value` | ref  #### `vm.global.ref` (VM::GlobalRefOp)  _Ref global declaration_   Syntax:  <pre><code>operation ::= `vm.global.ref` custom&lt;SymbolVisibility&gt;($sym_visibility)\n              (`mutable` $is_mutable^)?\n              $sym_name\n              attr-dict\n              `:` $type\n</code></pre>  Defines a global value that is a ref of a specific type. The global will retain the ref object for the lifetime of the context or until the value is replaced with a store or reset. Initialized to null unless an initial value is specified.  Traits: HasParent, IsolatedFromAbove  Interfaces: GlobalOpInterface, Symbol, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>sym_name</code>::mlir::StringAttrstring attribute <code>type</code>::mlir::TypeAttrany type attribute <code>is_mutable</code>::mlir::UnitAttrunit attribute <code>ordinal</code>::mlir::IntegerAttrordinal value   #### `vm.global.store.f32` (VM::GlobalStoreF32Op)  _Global 32-bit floating-point store operation_   Syntax:  <pre><code>operation ::= `vm.global.store.f32` $value `,` $global attr-dict `:` type($value)\n</code></pre>  Stores a primitive value value to a global.  Traits: VM_ExtF32  Interfaces: SymbolUserOpInterface, Util_GlobalStoreOpInterface, VMSerializableOp, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>global</code>FlatSymbolRefAttrsymbol reference attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | 32-bit float  #### `vm.global.store.f64` (VM::GlobalStoreF64Op)  _Global 64-bit floating-point store operation_   Syntax:  <pre><code>operation ::= `vm.global.store.f64` $value `,` $global attr-dict `:` type($value)\n</code></pre>  Stores a primitive value value to a global.  Traits: VM_ExtF64  Interfaces: SymbolUserOpInterface, Util_GlobalStoreOpInterface, VMSerializableOp, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>global</code>FlatSymbolRefAttrsymbol reference attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | 64-bit float  #### `vm.global.store.i32` (VM::GlobalStoreI32Op)  _Global 32-bit integer store operation_   Syntax:  <pre><code>operation ::= `vm.global.store.i32` $value `,` $global attr-dict `:` type($value)\n</code></pre>  Stores a primitive value value to a global.  Interfaces: SymbolUserOpInterface, Util_GlobalStoreOpInterface, VMSerializableOp, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>global</code>FlatSymbolRefAttrsymbol reference attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | 32-bit signless integer  #### `vm.global.store.i64` (VM::GlobalStoreI64Op)  _Global 64-bit integer store operation_   Syntax:  <pre><code>operation ::= `vm.global.store.i64` $value `,` $global attr-dict `:` type($value)\n</code></pre>  Stores a primitive value value to a global.  Interfaces: SymbolUserOpInterface, Util_GlobalStoreOpInterface, VMSerializableOp, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>global</code>FlatSymbolRefAttrsymbol reference attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | 64-bit signless integer  #### `vm.global.store.indirect.f32` (VM::GlobalStoreIndirectF32Op)  _Global 32-bit floating-point store operation_   Syntax:  <pre><code>operation ::= `vm.global.store.indirect.f32` $value `,` $global attr-dict `:` type($value) `-&gt;` type($global)\n</code></pre>  Stores a primitive value to a global.  Traits: VM_ExtF32  Interfaces: Util_GlobalStoreIndirectOpInterface, VMSerializableOp, VM_OpInterface  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | 32-bit float | `global` | 32-bit signless integer or ptr&lt;32-bit float&gt;  #### `vm.global.store.indirect.f64` (VM::GlobalStoreIndirectF64Op)  _Global 64-bit floating-point store operation_   Syntax:  <pre><code>operation ::= `vm.global.store.indirect.f64` $value `,` $global attr-dict `:` type($value) `-&gt;` type($global)\n</code></pre>  Stores a primitive value to a global.  Traits: VM_ExtF64  Interfaces: Util_GlobalStoreIndirectOpInterface, VMSerializableOp, VM_OpInterface  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | 64-bit float | `global` | 32-bit signless integer or ptr&lt;64-bit float&gt;  #### `vm.global.store.indirect.i32` (VM::GlobalStoreIndirectI32Op)  _Global 32-bit integer store operation_   Syntax:  <pre><code>operation ::= `vm.global.store.indirect.i32` $value `,` $global attr-dict `:` type($value) `-&gt;` type($global)\n</code></pre>  Stores a primitive value to a global.  Interfaces: Util_GlobalStoreIndirectOpInterface, VMSerializableOp, VM_OpInterface  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | 32-bit signless integer | `global` | 32-bit signless integer or ptr&lt;32-bit signless integer&gt;  #### `vm.global.store.indirect.i64` (VM::GlobalStoreIndirectI64Op)  _Global 64-bit integer store operation_   Syntax:  <pre><code>operation ::= `vm.global.store.indirect.i64` $value `,` $global attr-dict `:` type($value) `-&gt;` type($global)\n</code></pre>  Stores a primitive value to a global.  Interfaces: Util_GlobalStoreIndirectOpInterface, VMSerializableOp, VM_OpInterface  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | 64-bit signless integer | `global` | 32-bit signless integer or ptr&lt;64-bit signless integer&gt;  #### `vm.global.store.indirect.ref` (VM::GlobalStoreIndirectRefOp)  _Global ref stores operation_   Syntax:  <pre><code>operation ::= `vm.global.store.indirect.ref` $value `,` $global attr-dict `:` type($value) `-&gt;` type($global)\n</code></pre>  Stores a ref to a global, retaining it until the global is reset.  Interfaces: Util_GlobalStoreIndirectOpInterface, VMSerializableOp, VM_OpInterface  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | ref | `global` | 32-bit signless integer or ptr  #### `vm.global.store.ref` (VM::GlobalStoreRefOp)  _Global ref stores operation_   Syntax:  <pre><code>operation ::= `vm.global.store.ref` $value `,` $global attr-dict `:` type($value)\n</code></pre>  Stores a ref to a global, retaining it until the global is reset.  Interfaces: SymbolUserOpInterface, Util_GlobalStoreOpInterface, VMSerializableOp, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>global</code>FlatSymbolRefAttrsymbol reference attribute   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `value` | ref   ### Integer arithmetic ops    #### `vm.abs.i32` (VM::AbsI32Op)  _Integer absolute-value operation_   Syntax:  <pre><code>operation ::= `vm.abs.i32` $operand attr-dict `:` type($result)\n</code></pre>   Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.abs.i64` (VM::AbsI64Op)  _Integer absolute-value operation_   Syntax:  <pre><code>operation ::= `vm.abs.i64` $operand attr-dict `:` type($result)\n</code></pre>   Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit signless integer  #### `vm.add.i32` (VM::AddI32Op)  _Integer add operation_   Syntax:  <pre><code>operation ::= `vm.add.i32` operands attr-dict `:` type($result)\n</code></pre>   Traits: Commutative  Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit signless integer | `rhs` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.add.i64` (VM::AddI64Op)  _Integer add operation_   Syntax:  <pre><code>operation ::= `vm.add.i64` operands attr-dict `:` type($result)\n</code></pre>   Traits: Commutative  Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit signless integer | `rhs` | 64-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit signless integer  #### `vm.div.i32.s` (VM::DivI32SOp)  _Signed integer division operation_   Syntax:  <pre><code>operation ::= `vm.div.i32.s` operands attr-dict `:` type($result)\n</code></pre>   Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit signless integer | `rhs` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.div.i32.u` (VM::DivI32UOp)  _Unsigned integer division operation_   Syntax:  <pre><code>operation ::= `vm.div.i32.u` operands attr-dict `:` type($result)\n</code></pre>   Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit signless integer | `rhs` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.div.i64.s` (VM::DivI64SOp)  _Signed integer division operation_   Syntax:  <pre><code>operation ::= `vm.div.i64.s` operands attr-dict `:` type($result)\n</code></pre>   Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit signless integer | `rhs` | 64-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit signless integer  #### `vm.div.i64.u` (VM::DivI64UOp)  _Unsigned integer division operation_   Syntax:  <pre><code>operation ::= `vm.div.i64.u` operands attr-dict `:` type($result)\n</code></pre>   Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit signless integer | `rhs` | 64-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit signless integer  #### `vm.fma.i32` (VM::FMAI32Op)  _Integer fused-multiply add operation (a*b+c)_   Syntax:  <pre><code>operation ::= `vm.fma.i32` operands attr-dict `:` type($result)\n</code></pre>   Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `a` | 32-bit signless integer | `b` | 32-bit signless integer | `c` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.fma.i64` (VM::FMAI64Op)  _Integer fused-multiply add operation (a*b+c)_   Syntax:  <pre><code>operation ::= `vm.fma.i64` operands attr-dict `:` type($result)\n</code></pre>   Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `a` | 64-bit signless integer | `b` | 64-bit signless integer | `c` | 64-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit signless integer  #### `vm.max.i32.s` (VM::MaxI32SOp)  _Signed integer maximum operation_   Syntax:  <pre><code>operation ::= `vm.max.i32.s` operands attr-dict `:` type($result)\n</code></pre>   Traits: Commutative  Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit signless integer | `rhs` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.max.i32.u` (VM::MaxI32UOp)  _Unsigned integer maximum operation_   Syntax:  <pre><code>operation ::= `vm.max.i32.u` operands attr-dict `:` type($result)\n</code></pre>   Traits: Commutative  Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit signless integer | `rhs` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.max.i64.s` (VM::MaxI64SOp)  _Signed integer maximum operation_   Syntax:  <pre><code>operation ::= `vm.max.i64.s` operands attr-dict `:` type($result)\n</code></pre>   Traits: Commutative  Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit signless integer | `rhs` | 64-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit signless integer  #### `vm.max.i64.u` (VM::MaxI64UOp)  _Unsigned integer maximum operation_   Syntax:  <pre><code>operation ::= `vm.max.i64.u` operands attr-dict `:` type($result)\n</code></pre>   Traits: Commutative  Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit signless integer | `rhs` | 64-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit signless integer  #### `vm.min.i32.s` (VM::MinI32SOp)  _Signed integer minimum operation_   Syntax:  <pre><code>operation ::= `vm.min.i32.s` operands attr-dict `:` type($result)\n</code></pre>   Traits: Commutative  Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit signless integer | `rhs` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.min.i32.u` (VM::MinI32UOp)  _Unsigned integer minimum operation_   Syntax:  <pre><code>operation ::= `vm.min.i32.u` operands attr-dict `:` type($result)\n</code></pre>   Traits: Commutative  Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit signless integer | `rhs` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.min.i64.s` (VM::MinI64SOp)  _Signed integer minimum operation_   Syntax:  <pre><code>operation ::= `vm.min.i64.s` operands attr-dict `:` type($result)\n</code></pre>   Traits: Commutative  Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit signless integer | `rhs` | 64-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit signless integer  #### `vm.min.i64.u` (VM::MinI64UOp)  _Unsigned integer minimum operation_   Syntax:  <pre><code>operation ::= `vm.min.i64.u` operands attr-dict `:` type($result)\n</code></pre>   Traits: Commutative  Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit signless integer | `rhs` | 64-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit signless integer  #### `vm.mul.i32` (VM::MulI32Op)  _Integer multiplication operation_   Syntax:  <pre><code>operation ::= `vm.mul.i32` operands attr-dict `:` type($result)\n</code></pre>   Traits: Commutative  Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit signless integer | `rhs` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.mul.i64` (VM::MulI64Op)  _Integer multiplication operation_   Syntax:  <pre><code>operation ::= `vm.mul.i64` operands attr-dict `:` type($result)\n</code></pre>   Traits: Commutative  Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit signless integer | `rhs` | 64-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit signless integer  #### `vm.rem.i32.s` (VM::RemI32SOp)  _Signed integer division remainder operation_   Syntax:  <pre><code>operation ::= `vm.rem.i32.s` operands attr-dict `:` type($result)\n</code></pre>   Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit signless integer | `rhs` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.rem.i32.u` (VM::RemI32UOp)  _Unsigned integer division remainder operation_   Syntax:  <pre><code>operation ::= `vm.rem.i32.u` operands attr-dict `:` type($result)\n</code></pre>   Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit signless integer | `rhs` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.rem.i64.s` (VM::RemI64SOp)  _Signed integer division remainder operation_   Syntax:  <pre><code>operation ::= `vm.rem.i64.s` operands attr-dict `:` type($result)\n</code></pre>   Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit signless integer | `rhs` | 64-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit signless integer  #### `vm.rem.i64.u` (VM::RemI64UOp)  _Unsigned integer division remainder operation_   Syntax:  <pre><code>operation ::= `vm.rem.i64.u` operands attr-dict `:` type($result)\n</code></pre>   Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit signless integer | `rhs` | 64-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit signless integer  #### `vm.sub.i32` (VM::SubI32Op)  _Integer subtract operation_   Syntax:  <pre><code>operation ::= `vm.sub.i32` operands attr-dict `:` type($result)\n</code></pre>   Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit signless integer | `rhs` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.sub.i64` (VM::SubI64Op)  _Integer subtract operation_   Syntax:  <pre><code>operation ::= `vm.sub.i64` operands attr-dict `:` type($result)\n</code></pre>   Interfaces: NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit signless integer | `rhs` | 64-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit signless integer   ### Integer bit manipulation ops    #### `vm.and.i32` (VM::AndI32Op)  _Integer binary and operation_   Syntax:  <pre><code>operation ::= `vm.and.i32` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, Commutative  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit signless integer | `rhs` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.and.i64` (VM::AndI64Op)  _Integer binary and operation_   Syntax:  <pre><code>operation ::= `vm.and.i64` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, Commutative  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit signless integer | `rhs` | 64-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit signless integer  #### `vm.ctlz.i32` (VM::CtlzI32Op)  _Counts the leading zeros in an integer value_   Syntax:  <pre><code>operation ::= `vm.ctlz.i32` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.ctlz.i64` (VM::CtlzI64Op)  _Counts the leading zeros in an integer value_   Syntax:  <pre><code>operation ::= `vm.ctlz.i64` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit signless integer  #### `vm.not.i32` (VM::NotI32Op)  _Integer binary not operation_   Syntax:  <pre><code>operation ::= `vm.not.i32` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.not.i64` (VM::NotI64Op)  _Integer binary not operation_   Syntax:  <pre><code>operation ::= `vm.not.i64` $operand attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | 64-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit signless integer  #### `vm.or.i32` (VM::OrI32Op)  _Integer binary or operation_   Syntax:  <pre><code>operation ::= `vm.or.i32` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, Commutative  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit signless integer | `rhs` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.or.i64` (VM::OrI64Op)  _Integer binary or operation_   Syntax:  <pre><code>operation ::= `vm.or.i64` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, Commutative  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit signless integer | `rhs` | 64-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit signless integer  #### `vm.xor.i32` (VM::XorI32Op)  _Integer binary exclusive-or operation_   Syntax:  <pre><code>operation ::= `vm.xor.i32` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, Commutative  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 32-bit signless integer | `rhs` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.xor.i64` (VM::XorI64Op)  _Integer binary exclusive-or operation_   Syntax:  <pre><code>operation ::= `vm.xor.i64` operands attr-dict `:` type($result)\n</code></pre>   Traits: AlwaysSpeculatableImplTrait, Commutative  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | 64-bit signless integer | `rhs` | 64-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit signless integer   ### List ops    #### `vm.list.alloc` (VM::ListAllocOp)  _Allocates a new empty list_   Syntax:  <pre><code>operation ::= `vm.list.alloc` operands attr-dict `:` `(` type($initial_capacity) `)` `-&gt;` type($result)\n</code></pre>  Allocates a new typed list with a minimum initial_capacity.  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{MemoryEffects::Allocate on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `initial_capacity` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | list  #### `vm.list.get.f32` (VM::ListGetF32Op)  _Primitive type element accessor_   Syntax:  <pre><code>operation ::= `vm.list.get.f32` operands attr-dict `:` `(` type($list) `,` type($index) `)` `-&gt;` type($result)\n</code></pre>  Returns the value of the element at the given index.  Traits: VM_ExtF32  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `list` | list&lt;8/16/32/64-bit integer or 16/32/64-bit float&gt; | `index` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit float  #### `vm.list.get.f64` (VM::ListGetF64Op)  _Primitive type element accessor_   Syntax:  <pre><code>operation ::= `vm.list.get.f64` operands attr-dict `:` `(` type($list) `,` type($index) `)` `-&gt;` type($result)\n</code></pre>  Returns the value of the element at the given index.  Traits: VM_ExtF64  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `list` | list&lt;8/16/32/64-bit integer or 16/32/64-bit float&gt; | `index` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit float  #### `vm.list.get.i32` (VM::ListGetI32Op)  _Primitive type element accessor_   Syntax:  <pre><code>operation ::= `vm.list.get.i32` operands attr-dict `:` `(` type($list) `,` type($index) `)` `-&gt;` type($result)\n</code></pre>  Returns the value of the element at the given index.  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `list` | list&lt;8/16/32/64-bit integer or 16/32/64-bit float&gt; | `index` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.list.get.i64` (VM::ListGetI64Op)  _Primitive type element accessor_   Syntax:  <pre><code>operation ::= `vm.list.get.i64` operands attr-dict `:` `(` type($list) `,` type($index) `)` `-&gt;` type($result)\n</code></pre>  Returns the value of the element at the given index.  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `list` | list&lt;8/16/32/64-bit integer or 16/32/64-bit float&gt; | `index` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 64-bit signless integer  #### `vm.list.get.ref` (VM::ListGetRefOp)  _Ref type element accessor_   Syntax:  <pre><code>operation ::= `vm.list.get.ref` operands attr-dict `:` `(` type($list) `,` type($index) `)` `-&gt;` type($result)\n</code></pre>  Returns the ref value of the element at the given index. Note that the value may be null if the element is null or the type does not match.  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `list` | list | `index` | 32-bit signless integer  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | ref  #### `vm.list.reserve` (VM::ListReserveOp)  _Reserves capacity for list growth_   Syntax:  <pre><code>operation ::= `vm.list.reserve` operands attr-dict `:` `(` type($list) `,` type($minimum_capacity) `)`\n</code></pre>  Reserves storage for at least minimum_capacity elements. If the list already has at least the specified capacity the operation is ignored.  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{MemoryEffects::Allocate on ::mlir::SideEffects::DefaultResource, MemoryEffects::Read on ::mlir::SideEffects::DefaultResource, MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `list` | list | `minimum_capacity` | 32-bit signless integer  #### `vm.list.resize` (VM::ListResizeOp)  _Resizes the list to a new count in elements_   Syntax:  <pre><code>operation ::= `vm.list.resize` operands attr-dict `:` `(` type($list) `,` type($new_size) `)`\n</code></pre>  Resizes the list to contain new_size elements. This will either truncate the list if the existing size is greater than new_size or extend the list with the default list value of 0 if storing primitives and null if refs.  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `list` | list | `new_size` | 32-bit signless integer  #### `vm.list.set.f32` (VM::ListSetF32Op)  _Primitive type element mutator_   Syntax:  <pre><code>operation ::= `vm.list.set.f32` operands attr-dict `:` `(` type($list) `,` type($index) `,` type($value) `)`\n</code></pre>  Sets the element at the given index to the new value.  Traits: VM_ExtF32  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `list` | list&lt;8/16/32/64-bit integer or 16/32/64-bit float&gt; | `index` | 32-bit signless integer | `value` | 32-bit float  #### `vm.list.set.f64` (VM::ListSetF64Op)  _Primitive type element mutator_   Syntax:  <pre><code>operation ::= `vm.list.set.f64` operands attr-dict `:` `(` type($list) `,` type($index) `,` type($value) `)`\n</code></pre>  Sets the element at the given index to the new value.  Traits: VM_ExtF64  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `list` | list&lt;8/16/32/64-bit integer or 16/32/64-bit float&gt; | `index` | 32-bit signless integer | `value` | 64-bit float  #### `vm.list.set.i32` (VM::ListSetI32Op)  _Primitive type element mutator_   Syntax:  <pre><code>operation ::= `vm.list.set.i32` operands attr-dict `:` `(` type($list) `,` type($index) `,` type($value) `)`\n</code></pre>  Sets the element at the given index to the new value.  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `list` | list&lt;8/16/32/64-bit integer or 16/32/64-bit float&gt; | `index` | 32-bit signless integer | `value` | 32-bit signless integer  #### `vm.list.set.i64` (VM::ListSetI64Op)  _Primitive type element mutator_   Syntax:  <pre><code>operation ::= `vm.list.set.i64` operands attr-dict `:` `(` type($list) `,` type($index) `,` type($value) `)`\n</code></pre>  Sets the element at the given index to the new value.  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `list` | list&lt;8/16/32/64-bit integer or 16/32/64-bit float&gt; | `index` | 32-bit signless integer | `value` | 64-bit signless integer  #### `vm.list.set.ref` (VM::ListSetRefOp)  _Ref type element mutator_   Syntax:  <pre><code>operation ::= `vm.list.set.ref` operands attr-dict `:` `(` type($list) `,` type($index) `,` type($value) `)`\n</code></pre>  Sets the element at the given index to the new ref value (possibly null).  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{MemoryEffects::Write on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `list` | list | `index` | 32-bit signless integer | `value` | ref  #### `vm.list.size` (VM::ListSizeOp)  _The size of the list in elements_   Syntax:  <pre><code>operation ::= `vm.list.size` operands attr-dict `:` `(` type($list) `)` `-&gt;` type($result)\n</code></pre>  Returns the current size of the list in elements.  Interfaces: MemoryEffectOpInterface (MemoryEffectOpInterface), VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `list` | list  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer   ### Ref comparison ops  Comparison ops for `vm.ref`.  #### `vm.cmp.eq.ref` (VM::CmpEQRefOp)  _Ref equality comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.eq.ref` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, Commutative  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | ref | `rhs` | ref  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.ne.ref` (VM::CmpNERefOp)  _Ref inequality comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.ne.ref` operands attr-dict `:` type($lhs)\n</code></pre>  Compares two operands with the specified predicate.  Traits: AlwaysSpeculatableImplTrait, Commutative  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs` | ref | `rhs` | ref  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer  #### `vm.cmp.nz.ref` (VM::CmpNZRefOp)  _Ref non-zero comparison operation_   Syntax:  <pre><code>operation ::= `vm.cmp.nz.ref` $operand attr-dict `:` type($operand)\n</code></pre>  Compares the given ref operand for a non-zero/null value.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface), OpAsmOpInterface, VMSerializableOp, VM_OpInterface  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `operand` | ref  ##### Results:  | Result | Description | | :----: | ----------- | | `result` | 32-bit signless integer   ### Structural ops    #### `vm.export` (VM::ExportOp)  _Exports a function from the module_  Specifies an exported function with an externally-visible alias. Multiple exports can reference the same internal functions.  Interfaces: SymbolUserOpInterface, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>function_ref</code>::mlir::FlatSymbolRefAttrflat symbol reference attribute <code>export_name</code>::mlir::StringAttrstring attribute <code>ordinal</code>::mlir::IntegerAttrordinal value   #### `vm.func` (VM::FuncOp)  _Function defined with VM control flow ops_  Represents a function containing VM ops and those of compatible dialects. All flow control is performed by VM ops.  Traits: HasParent, IsolatedFromAbove  Interfaces: CallableOpInterface, FunctionOpInterface, Symbol, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>function_type</code>::mlir::TypeAttrtype attribute of function type <code>ordinal</code>::mlir::IntegerAttrordinal value <code>noinline</code>::mlir::UnitAttrunit attribute <code>arg_attrs</code>::mlir::ArrayAttrArray of dictionary attributes <code>res_attrs</code>::mlir::ArrayAttrArray of dictionary attributes   #### `vm.import` (VM::ImportOp)  _Imports a function from an external module_  Specifies a function that should be imported from either the runtime or an external VM module.  Required imports can be declared with a minimum version of the module that contains the import. The maximum declared minimum version of all required imports from the module will become the required minimum version at runtime.  Optional imports not present at runtime will be invalid to call and whether they were resolved can be queried with `vm.import.resolved`.  Interfaces: CallableOpInterface, FunctionOpInterface, Symbol, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_name</code>::mlir::StringAttrstring attribute <code>function_type</code>::mlir::TypeAttrtype attribute of function type <code>arg_attrs</code>::mlir::ArrayAttrArray of dictionary attributes <code>res_attrs</code>::mlir::ArrayAttrArray of dictionary attributes <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>ordinal</code>::mlir::IntegerAttrordinal value <code>is_optional</code>::mlir::UnitAttrunit attribute <code>minimum_version</code>::mlir::IntegerAttr32-bit signless integer attribute   #### `vm.initializer` (VM::InitializerOp)  _Global initialization function_  A function that is called in definition order upon module initialization. Must not load any globals that are defined or initialized after it in the module.  Traits: HasParent, IsolatedFromAbove  Interfaces: CallableOpInterface, FunctionOpInterface, Symbol, Util_InitializerOpInterface, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>function_type</code>::mlir::TypeAttrtype attribute of function type <code>arg_attrs</code>::mlir::ArrayAttrArray of dictionary attributes <code>res_attrs</code>::mlir::ArrayAttrArray of dictionary attributes   #### `vm.module` (VM::ModuleOp)  _Module containing VM functions and variables_   Syntax:  <pre><code>operation ::= `vm.module` custom&lt;SymbolVisibility&gt;($sym_visibility)\n              $sym_name\n              attr-dict-with-keyword\n              regions\n</code></pre>  Top-level container for VM functions.  Traits: IsolatedFromAbove, SingleBlock, SingleBlockImplicitTerminator, SymbolTable  Interfaces: Symbol, VM_OpInterface  ##### Attributes:   AttributeMLIR TypeDescription <code>sym_visibility</code>::mlir::StringAttrstring attribute <code>sym_name</code>::mlir::StringAttrstring attribute <code>ordinal_counts</code>::mlir::iree_compiler::IREE::VM::OrdinalCountsAttr <code>version</code>::mlir::IntegerAttr32-bit signless integer attribute   #### `vm.module_terminator` (VM::ModuleTerminatorOp)  _Terminator pseudo-op for the module op_   Syntax:  <pre><code>operation ::= `vm.module_terminator` attr-dict\n</code></pre>   Traits: HasParent, Terminator  Interfaces: VM_OpInterface   ## Attribute definition  ### OrdinalCountsAttr    Syntax:  <pre><code>#vm.ordinal_counts&lt;\n  int32_t,   # import_funcs\n  int32_t,   # export_funcs\n  int32_t,   # internal_funcs\n  int32_t,   # global_bytes\n  int32_t,   # global_refs\n  int32_t,   # rodatas\n  int32_t   # rwdatas\n&gt;\n</code></pre>   ##### Parameters:  | Parameter | C++ type | Description | | :-------: | :-------: | ----------- | | import_funcs | `int32_t` |  | | export_funcs | `int32_t` |  | | internal_funcs | `int32_t` |  | | global_bytes | `int32_t` |  | | global_refs | `int32_t` |  | | rodatas | `int32_t` |  | | rwdatas | `int32_t` |  |"},{"location":"reference/mlir-dialects/VMVX/","title":"VMVX","text":""},{"location":"reference/mlir-dialects/VMVX/#vmvx-dialect","title":"'vmvx' Dialect","text":"<p>Vector extensions to the IREE VM.</p> <p>This is a reference dialect representing a simple IREE VM-based linear algebra module that is used as a library at runtime. The ops in this dialect map (roughly) 1:1 with the exported functions in the runtime module.</p> <p>See <code>vmvx.imports.mlir</code> for the full list of exported functions.</p> <ul> <li>'vmvx' Dialect<ul> <li>Operation definition<ul> <li>ABI ops<ul> <li>vmvx.binary (VMVX::BinaryOp)</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/mlir-dialects/VMVX/#operation-definition","title":"Operation definition","text":""},{"location":"reference/mlir-dialects/VMVX/#abi-ops","title":"ABI ops","text":""},{"location":"reference/mlir-dialects/VMVX/#vmvxbinary-vmvxbinaryop","title":"<code>vmvx.binary</code> (VMVX::BinaryOp)","text":"<p>Performs a strided elementwise operation on two same-rank buffers</p> <p>Syntax:</p> <pre><code>operation ::= `vmvx.binary` `op` `` `(` $opcode `:` $element_type `)`\n              `lhs` `` `(` $lhs_buffer `offset` $lhs_offset `strides` `[` $lhs_strides `]` `:` type($lhs_buffer) `)`\n              `rhs` `` `(` $rhs_buffer `offset` $rhs_offset `strides` `[` $rhs_strides `]` `:` type($rhs_buffer) `)`\n              `out` `` `(` $out_buffer `offset` $out_offset `strides` `[` $out_strides `]` `:` type($out_buffer) `)`\n              `sizes` `` `(` $sizes `)`\n              attr-dict\n</code></pre> <p>Performs the operation in-place as if: <pre><code>  OUT = OP(LHS, RHS)\n</code></pre></p> <p>Where <code>OP</code> is a concrete operation name as defined in ukernel/elementwise.h</p> <p>Traits: SameVariadicOperandSize</p>"},{"location":"reference/mlir-dialects/VMVX/#attributes","title":"Attributes:","text":"AttributeMLIR TypeDescription <code>opcode</code>::mlir::StringAttrstring attribute <code>element_type</code>::mlir::TypeAttrtype attribute of 8-bit signless integer or 16-bit signless integer or 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `lhs_buffer` | a reference counted byte buffer | `lhs_offset` | index | `lhs_strides` | index | `rhs_buffer` | a reference counted byte buffer | `rhs_offset` | index | `rhs_strides` | index | `out_buffer` | a reference counted byte buffer | `out_offset` | index | `out_strides` | index | `sizes` | index  #### `vmvx.copy` (VMVX::CopyOp)  _Copy from one buffer to another_   Syntax:  <pre><code>operation ::= `vmvx.copy` `in` `` `(` $in_buffer `offset` $in_offset `strides` `[` $in_strides `]` `:` type($in_buffer) `)`\n              `out` `` `(` $out_buffer `offset` $out_offset `strides` `[` $out_strides `]` `:` type($out_buffer) `)`\n              `sizes` `` `(` $sizes `)`\n              `:` $element_type\n              attr-dict\n</code></pre>   Traits: SameVariadicOperandSize  ##### Attributes:   AttributeMLIR TypeDescription <code>element_type</code>::mlir::TypeAttrtype attribute of 8-bit signless integer or 16-bit signless integer or 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `in_buffer` | a reference counted byte buffer | `in_offset` | index | `in_strides` | index | `out_buffer` | a reference counted byte buffer | `out_offset` | index | `out_strides` | index | `sizes` | index  #### `vmvx.fill2d` (VMVX::Fill2DOp)  _Fill a tile with a scalar_   Syntax:  <pre><code>operation ::= `vmvx.fill2d` `scalar` `` `(` $scalar `:` type($scalar) `)`\n              `out` `` `(` $out_buffer `offset` $out_offset `row_stride` $out_row_stride `:` type($out_buffer) `)`\n              `sizes` `` `(` $m `,` $n `)`\n              attr-dict\n</code></pre>  Fills a tile with dimensions [m, n] with a scalar.  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `scalar` | 8-bit signless integer or 16-bit signless integer or 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float | `out_buffer` | a reference counted byte buffer | `out_offset` | index | `out_row_stride` | index | `m` | index | `n` | index  #### `vmvx.unary` (VMVX::UnaryOp)  _Performs a strided elementwise unary operation_   Syntax:  <pre><code>operation ::= `vmvx.unary` `op` `` `(` $opcode `:` $element_type `)`\n              `in` `` `(` $in_buffer `offset` $in_offset `strides` `[` $in_strides `]` `:` type($in_buffer) `)`\n              `out` `` `(` $out_buffer `offset` $out_offset `strides` `[` $out_strides `]` `:` type($out_buffer) `)`\n              `sizes` `` `(` $sizes `)`\n              attr-dict\n</code></pre>  Performs the operation in-place as if: <pre><code>  OUT = OP(IN)\n</code></pre>  Where `OP` is a concrete operation name as defined in ukernel/elementwise.h  Traits: SameVariadicOperandSize  ##### Attributes:   AttributeMLIR TypeDescription <code>opcode</code>::mlir::StringAttrstring attribute <code>element_type</code>::mlir::TypeAttrtype attribute of 8-bit signless integer or 16-bit signless integer or 32-bit signless integer or 64-bit signless integer or 32-bit float or 64-bit float   ##### Operands:  | Operand | Description | | :-----: | ----------- | | `in_buffer` | a reference counted byte buffer | `in_offset` | index | `in_strides` | index | `out_buffer` | a reference counted byte buffer | `out_offset` | index | `out_strides` | index | `sizes` | index   ### Utility ops    #### `vmvx.get_buffer_descriptor` (VMVX::GetBufferDescriptorOp)  _Late binds a base buffer/offset/strides_   Syntax:  <pre><code>operation ::= `vmvx.get_buffer_descriptor` $source `:` type($source) `-&gt;` type(results) attr-dict\n</code></pre>  Queries a base buffer, offset and strides. This op is late bound to its source (alloca, binding, etc), allowing additional layers of transformations to be added as lowering progresses (or for buffers to be combined).  This op has canonicalization rules which will bubble it up through the view stack. A final reconciliation pass is used explicitly to bind it to concrete sources.  Traits: AlwaysSpeculatableImplTrait, SameVariadicResultSize  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Operands:  | Operand | Description | | :-----: | ----------- | | `source` | memref of any type values  ##### Results:  | Result | Description | | :----: | ----------- | | `base_buffer` | a reference counted byte buffer | `offset` | index | `sizes` | index | `strides` | index  #### `vmvx.get_raw_interface_binding_buffer` (VMVX::GetRawInterfaceBindingBufferOp)  _Gets the raw buffer associated with a binding_   Syntax:  <pre><code>operation ::= `vmvx.get_raw_interface_binding_buffer` `set` `(` $set `)` `binding` `(` $binding `)` attr-dict\n</code></pre>  Normally, a slice of a binding buffer is returned via hal.interface.binding.subspan. However, the normal VMVX lowering flow for this presumes that the result is a memref, and upon final conversion, it will offset the memref automatically to make it consistent.  This op is used in situations where earlier in a lowering, we have fully resolved the binding to a buffer and would just like the raw backing buffer as passed to the interface.  Traits: AlwaysSpeculatableImplTrait  Interfaces: ConditionallySpeculatable, NoMemoryEffect (MemoryEffectOpInterface)  Effects: MemoryEffects::Effect{}  ##### Attributes:   AttributeMLIR TypeDescription <code>set</code>::mlir::IntegerAttrindex attribute <code>binding</code>::mlir::IntegerAttrindex attribute   ##### Results:  | Result | Description | | :----: | ----------- | | `buffer` | a reference counted byte buffer"},{"location":"community/tags/","title":"Tags","text":"<p>Website pages sorted by tag:</p>"},{"location":"community/tags/#android","title":"Android","text":"<ul> <li>Android cross-compilation</li> </ul>"},{"location":"community/tags/#cpu","title":"CPU","text":"<ul> <li>RISC-V cross-compilation</li> <li>Matrix Multiplication with MMT4D</li> <li>CPU - Bare-Metal</li> <li>CPU</li> </ul>"},{"location":"community/tags/#cuda","title":"CUDA","text":"<ul> <li>CUDA backend</li> <li>GPU - CUDA/ROCm</li> </ul>"},{"location":"community/tags/#gpu","title":"GPU","text":"<ul> <li>CUDA backend</li> <li>GPU - CUDA/ROCm</li> <li>GPU - Metal</li> <li>GPU - Vulkan</li> </ul>"},{"location":"community/tags/#jax","title":"JAX","text":"<ul> <li>JAX</li> <li>Extensions</li> <li>Glossary</li> </ul>"},{"location":"community/tags/#pytorch","title":"PyTorch","text":"<ul> <li>PyTorch</li> <li>Extensions</li> <li>Glossary</li> </ul>"},{"location":"community/tags/#python","title":"Python","text":"<ul> <li>JAX</li> <li>PyTorch</li> <li>TensorFlow</li> <li>TensorFlow Lite</li> <li>Python</li> </ul>"},{"location":"community/tags/#tensorflow","title":"TensorFlow","text":"<ul> <li>TFLite support via TOSA</li> <li>TensorFlow</li> <li>TensorFlow Lite</li> <li>Extensions</li> <li>Glossary</li> </ul>"},{"location":"community/tags/#vulkan","title":"Vulkan","text":"<ul> <li>GPU - Vulkan</li> </ul>"},{"location":"community/tags/#ios","title":"iOS","text":"<ul> <li>iOS cross-compilation</li> <li>GPU - Metal</li> </ul>"},{"location":"community/blog/archive/2021/","title":"2021","text":""},{"location":"community/blog/category/platforms/","title":"Platforms","text":""},{"location":"community/blog/category/performance/","title":"Performance","text":""},{"location":"community/blog/category/frontends/","title":"Frontends","text":""}]}